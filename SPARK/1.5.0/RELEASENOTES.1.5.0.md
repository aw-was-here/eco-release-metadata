
<!---
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
-->
# Apache Spark  1.5.0 Release Notes

These release notes cover new developer and user-facing incompatibilities, features, and major improvements.


---

* [SPARK-10467](https://issues.apache.org/jira/browse/SPARK-10467) | *Minor* | **Vector is converted to tuple when extracted from Row using \_\_getitem\_\_**

If we take a row from a data frame and try to extract vector element by index it is converted to tuple:

{code}
from pyspark.ml.feature import HashingTF

df = sqlContext.createDataFrame([(["foo", "bar"], )], ("keys", ))
transformer = HashingTF(inputCol="keys", outputCol="vec", numFeatures=5)
transformed = transformer.transform(df)
row = transformed.first()

row.vec # As expected
## SparseVector(5, {4: 2.0})

row[1]  # Returns tuple
## (0, 5, [4], [2.0]) 
{code}

Problem cannot be reproduced if we create and access Row directly:

{code}
from pyspark.mllib.linalg import Vectors
from pyspark.sql.types import Row

row = Row(vec=Vectors.sparse(3, [(0, 1)]))

row.vec
## SparseVector(3, {0: 1.0})

row[0]
## SparseVector(3, {0: 1.0})
{code}

but if we use above to create a data frame and extract:

{code}
df = sqlContext.createDataFrame([row], ("vec", ))

df.first()[0]
## (0, 3, [0], [1.0])  
{code}


---

* [SPARK-10391](https://issues.apache.org/jira/browse/SPARK-10391) | *Minor* | **Spark 1.4.1 released news under news/spark-1-3-1-released.html**

The link to the news "Spark 1.4.1 released" is under http://spark.apache.org/news/spark-1-3-1-released.html. It's certainly inconsistent with the other news.


---

* [SPARK-10354](https://issues.apache.org/jira/browse/SPARK-10354) | *Minor* | **First cost RDD shouldn't be cached in k-means\|\| and the following cost RDD should use MEMORY\_AND\_DISK**

The first RDD doesn't need to be cached, other cost RDDs should use MEMORY\_AND\_DISK to avoid recomputing.


---

* [SPARK-10353](https://issues.apache.org/jira/browse/SPARK-10353) | *Major* | **MLlib BLAS gemm outputs wrong result when beta = 0.0 for transpose transpose matrix multiplication**

Basically 
{code}
if (beta != 0.0) {
  f2jBLAS.dscal(C.values.length, beta, C.values, 1)
}
{code}
should be
{code}
if (beta != 1.0) {
  f2jBLAS.dscal(C.values.length, beta, C.values, 1)
}
{code}


---

* [SPARK-10350](https://issues.apache.org/jira/browse/SPARK-10350) | *Minor* | **Fix SQL Programming Guide**

[b93d99a\|https://github.com/apache/spark/commit/b93d99ae21b8b3af1dd55775f77e5a9ddea48f95#diff-d8aa7a37d17a1227cba38c99f9f22511R1383] contains duplicate content:  {{spark.sql.parquet.mergeSchema}}


---

* [SPARK-10348](https://issues.apache.org/jira/browse/SPARK-10348) | *Major* | **Improve Spark ML user guide**

improve ml-guide:

\* replace `ML Dataset` by `DataFrame` to simplify the abstraction
\* remove links to Scala API doc in the main guide
\* change ML algorithms to pipeline components


---

* [SPARK-10341](https://issues.apache.org/jira/browse/SPARK-10341) | *Critical* | **SMJ fail with unable to acquire memory**

In SMJ, the first ExternalSorter could consume all the memory before spilling, then the second can not even acquire the first page.

{code}
ava.io.IOException: Unable to acquire 16777216 bytes of memory
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.acquireNewPage(UnsafeExternalSorter.java:368)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.\<init\>(UnsafeExternalSorter.java:138)
	at org.apache.spark.util.collection.unsafe.sort.UnsafeExternalSorter.create(UnsafeExternalSorter.java:106)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.\<init\>(UnsafeExternalRowSorter.java:68)
	at org.apache.spark.sql.execution.TungstenSort.org$apache$spark$sql$execution$TungstenSort$$preparePartition$1(sort.scala:146)
	at org.apache.spark.sql.execution.TungstenSort$$anonfun$doExecute$3.apply(sort.scala:169)
	at org.apache.spark.sql.execution.TungstenSort$$anonfun$doExecute$3.apply(sort.scala:169)
	at org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:45)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.ZippedPartitionsRDD2.compute(ZippedPartitionsRDD.scala:88)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
{code}


---

* [SPARK-10339](https://issues.apache.org/jira/browse/SPARK-10339) | *Blocker* | **When scanning a partitioned table having thousands of partitions, Driver has a very high memory pressure because of SQL metrics**

I have a local dataset having 5000 partitions stored in {{/tmp/partitioned}}. When I run the following code, the free memory space in driver's old gen gradually decreases and eventually there is pretty much no free space in driver's old gen. Finally, all kinds of timeouts happen and the cluster is died.
{code}
val df = sqlContext.read.format("parquet").load("/tmp/partitioned")
df.filter("a \> -100").selectExpr("hash(a, b)").queryExecution.toRdd.foreach(\_ =\> Unit)
{code}

I did a quick test by deleting SQL metrics from project and filter operator, my job works fine.

The reason is that for a partitioned table, when we scan it, the actual plan is like
{code}
       other operators
           \|
           \|
        /--\|------\
       /   \|       \
      /    \|        \
     /     \|         \
project  project ... project
  \|        \|           \|
filter   filter  ... filter
  \|        \|           \|
part1    part2   ... part n
{code}

We create SQL metrics for every filter and project, which causing the extremely high memory pressure to the driver.


---

* [SPARK-10336](https://issues.apache.org/jira/browse/SPARK-10336) | *Major* | **fitIntercept is a command line option but not set in the LR example program.**

the parsed parameter is not set.


---

* [SPARK-10334](https://issues.apache.org/jira/browse/SPARK-10334) | *Critical* | **Partitioned table scan's query plan does not show Filter and Project on top of the table scan**

{code}
Seq(Tuple2(1, 1), Tuple2(2, 2)).toDF("i", "j").write.format("parquet").partitionBy("i").save("/tmp/testFilter\_partitioned")
val df1 = sqlContext.read.format("parquet").load("/tmp/testFilter\_partitioned")
df1.selectExpr("hash(i)", "hash(j)").show
df1.filter("hash(j) = 1").explain
== Physical Plan ==
Scan ParquetRelation[file:/tmp/testFilter\_partitioned][j#20,i#21]
{code}

Looks like the reason is that we correctly apply the project and filter. Then, we create an RDD for the result and then manually create a PhysicalRDD. So, the Project and Filter on top of the original table scan disappears from the physical  plan.

See https://github.com/apache/spark/blob/master/sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/DataSourceStrategy.scala#L138-L175

We will not generate wrong result. But, the query plan is confusing.


---

* [SPARK-10331](https://issues.apache.org/jira/browse/SPARK-10331) | *Major* | **Update user guide to address minor comments during code review**

Clean-up user guides to address some minor comments in:

https://github.com/apache/spark/pull/8304
https://github.com/apache/spark/pull/8487

Some code examples were introduced in 1.2 before `createDataFrame`. We should switch to that.


---

* [SPARK-10328](https://issues.apache.org/jira/browse/SPARK-10328) | *Major* | **na.omit has too restrictive generic in SparkR**

It should match the S3 function definition


---

* [SPARK-10326](https://issues.apache.org/jira/browse/SPARK-10326) | *Major* | **Cannot launch YARN job on Windows**

The fix is already in master, and it's one line out of the patch for SPARK-5754; the bug is that a Windows file path cannot be used to create a URI, to {{File.toURI()}} needs to be called.


---

* [SPARK-10325](https://issues.apache.org/jira/browse/SPARK-10325) | *Critical* | **Public Row no longer overrides hashCode / violates hashCode + equals contract**

It appears that the public {{Row}}'s hashCode is no longer overridden as of Spark 1.5.0:

{code}
val x = Row("Hello")
val y = Row("Hello")
println(x == y)
println(x.hashCode)
println(y.hashCode)
{code}

outputs

{code}
true
1032103993
1346393532
{code}

This violates the hashCode/equals contract.

I discovered this because it broke tests in the {{spark-avro}} library.


---

* [SPARK-10323](https://issues.apache.org/jira/browse/SPARK-10323) | *Critical* | **NPE in code-gened In expression**

To reproduce the problem, you can run {{null in ('str')}}. Let's also take a look InSet and other similar expressions.


---

* [SPARK-10321](https://issues.apache.org/jira/browse/SPARK-10321) | *Critical* | **OrcRelation doesn't override sizeInBytes**

This hurts performance badly because broadcast join can never be enabled.


---

* [SPARK-10315](https://issues.apache.org/jira/browse/SPARK-10315) | *Minor* | **remove document on spark.akka.failure-detector.threshold**

this parameter is not used any longer and there is some mistake in the current document , should be 'akka.remote.watch-failure-detector.threshold'


---

* [SPARK-10308](https://issues.apache.org/jira/browse/SPARK-10308) | *Major* | **%in% is not exported in SparkR**

While the operator is defined in Column.R it is not exported in our NAMESPACE file.


---

* [SPARK-10305](https://issues.apache.org/jira/browse/SPARK-10305) | *Critical* | **PySpark createDataFrame on list of LabeledPoints fails (regression)**

The following code works in 1.4 but fails in 1.5:
{code}
import numpy as np
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.linalg import Vectors

lp1 = LabeledPoint(1.0, Vectors.sparse(5, np.array([0, 1]), np.array([2.0, 21.0])))
lp2 = LabeledPoint(0.0, Vectors.sparse(5, np.array([2, 3]), np.array([2.0, 21.0])))
tmp = [lp1, lp2]
sqlContext.createDataFrame(tmp).show()
{code}

The failure is:
{code}
ValueError: Unexpected tuple LabeledPoint(1.0, (5,[0,1],[2.0,21.0])) with StructType
---------------------------------------------------------------------------
ValueError                                Traceback (most recent call last)
\<ipython-input-1-0e7cb8772e10\> in \<module\>()
      6 lp2 = LabeledPoint(0.0, Vectors.sparse(5, np.array([2, 3]), np.array([2.0, 21.0])))
      7 tmp = [lp1, lp2]
----\> 8 sqlContext.createDataFrame(tmp).show()

/home/ubuntu/databricks/spark/python/pyspark/sql/context.pyc in createDataFrame(self, data, schema, samplingRatio)
    404             rdd, schema = self.\_createFromRDD(data, schema, samplingRatio)
    405         else:
--\> 406             rdd, schema = self.\_createFromLocal(data, schema)
    407         jrdd = self.\_jvm.SerDeUtil.toJavaArray(rdd.\_to\_java\_object\_rdd())
    408         jdf = self.\_ssql\_ctx.applySchemaToPythonRDD(jrdd.rdd(), schema.json())

/home/ubuntu/databricks/spark/python/pyspark/sql/context.pyc in \_createFromLocal(self, data, schema)
    335 
    336         # convert python objects to sql data
--\> 337         data = [schema.toInternal(row) for row in data]
    338         return self.\_sc.parallelize(data), schema
    339 

/home/ubuntu/databricks/spark/python/pyspark/sql/types.pyc in toInternal(self, obj)
    539                 return tuple(f.toInternal(v) for f, v in zip(self.fields, obj))
    540             else:
--\> 541                 raise ValueError("Unexpected tuple %r with StructType" % obj)
    542         else:
    543             if isinstance(obj, dict):

ValueError: Unexpected tuple LabeledPoint(1.0, (5,[0,1],[2.0,21.0])) with StructType
{code}


---

* [SPARK-10298](https://issues.apache.org/jira/browse/SPARK-10298) | *Major* | **PySpark can't JSON serialize a DataFrame with DecimalType columns.**

{code}
In [8]: sc.sql.createDataFrame([[Decimal(123)]], types.StructType([types.StructField("a", types.DecimalType())]))
Out[8]: DataFrame[a: decimal(10,0)]

In [9]: \_.write.json("foo")
15/08/26 14:26:21 ERROR DefaultWriterContainer: Aborting task.
scala.MatchError: (DecimalType(10,0),123) (of class scala.Tuple2)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:126)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$.apply(JacksonGenerator.scala:133)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.writeInternal(JSONRelation.scala:191)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:224)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
15/08/26 14:26:21 ERROR DefaultWriterContainer: Task attempt attempt\_201508261426\_0000\_m\_000000\_0 aborted.
15/08/26 14:26:21 ERROR Executor: Exception in task 0.0 in stage 0.0 (TID 0)
org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:232)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: scala.MatchError: (DecimalType(10,0),123) (of class scala.Tuple2)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:126)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$.apply(JacksonGenerator.scala:133)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.writeInternal(JSONRelation.scala:191)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:224)
	... 8 more
15/08/26 14:26:21 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:232)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: scala.MatchError: (DecimalType(10,0),123) (of class scala.Tuple2)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:126)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$.apply(JacksonGenerator.scala:133)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.writeInternal(JSONRelation.scala:191)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:224)
	... 8 more

15/08/26 14:26:21 ERROR TaskSetManager: Task 0 in stage 0.0 failed 1 times; aborting job
15/08/26 14:26:21 ERROR InsertIntoHadoopFsRelation: Aborting job.
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:232)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: scala.MatchError: (DecimalType(10,0),123) (of class scala.Tuple2)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:126)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$.apply(JacksonGenerator.scala:133)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.writeInternal(JSONRelation.scala:191)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:224)
	... 8 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1254)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1253)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:684)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1476)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1427)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:554)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1795)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1808)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1885)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:1000)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:1000)
	at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:197)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:137)
	at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:287)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:232)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more
Caused by: scala.MatchError: (DecimalType(10,0),123) (of class scala.Tuple2)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:126)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$.apply(JacksonGenerator.scala:133)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.writeInternal(JSONRelation.scala:191)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:224)
	... 8 more
15/08/26 14:26:21 ERROR DefaultWriterContainer: Job job\_201508261426\_0000 aborted.
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
/Users/kevincox/starscream/bin/starscream in \<module\>()
----\> 1 \_.write.json("foo")

/Users/kevincox/starscream/.cache/spark/current/python/pyspark/sql/readwriter.pyc in json(self, path, mode)
    382         \>\>\> df.write.json(os.path.join(tempfile.mkdtemp(), 'data'))
    383         """
--\> 384         self.mode(mode).\_jwrite.json(path)
    385 
    386     @since(1.4)

/Users/kevincox/starscream/.cache/spark/current/python/lib/py4j-0.8.2.1-src.zip/py4j/java\_gateway.py in \_\_call\_\_(self, \*args)
    536         answer = self.gateway\_client.send\_command(command)
    537         return\_value = get\_return\_value(answer, self.gateway\_client,
--\> 538                 self.target\_id, self.name)
    539 
    540         for temp\_arg in temp\_args:

/Users/kevincox/starscream/.cache/spark/current/python/pyspark/sql/utils.pyc in deco(\*a, \*\*kw)
     34     def deco(\*a, \*\*kw):
     35         try:
---\> 36             return f(\*a, \*\*kw)
     37         except py4j.protocol.Py4JJavaError as e:
     38             s = e.java\_exception.toString()

/Users/kevincox/starscream/.cache/spark/current/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get\_return\_value(answer, gateway\_client, target\_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--\> 300                     format(target\_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o60.json.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:156)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:1000)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:1000)
	at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:197)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:137)
	at org.apache.spark.sql.DataFrameWriter.json(DataFrameWriter.scala:287)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:232)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:745)
Caused by: scala.MatchError: (DecimalType(10,0),123) (of class scala.Tuple2)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:126)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$.apply(JacksonGenerator.scala:133)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.writeInternal(JSONRelation.scala:191)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:224)
	... 8 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1254)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1253)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1253)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:684)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1476)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1438)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1427)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:554)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1795)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1808)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1885)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:150)
	... 28 more
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:232)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	... 1 more
Caused by: scala.MatchError: (DecimalType(10,0),123) (of class scala.Tuple2)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:126)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$.apply(JacksonGenerator.scala:133)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.writeInternal(JSONRelation.scala:191)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:224)
	... 8 more
{code}


---

* [SPARK-10295](https://issues.apache.org/jira/browse/SPARK-10295) | *Minor* | **Dynamic allocation in Mesos does not release when RDDs are cached**

When running spark in coarse grained mode with shuffle service and dynamic allocation, the driver does not release executors if a dataset is cached.
The console output OTOH shows:
\> 15/08/26 17:29:58 WARN SparkContext: Dynamic allocation currently does not support cached RDDs. Cached data for RDD 9 will be lost when executors are removed.
However after the default of 1m, executors are not released. When I perform the same initial setup, loading data, etc, but without caching, the executors are released.

Is this intended behaviour?
If this is intended behaviour, the console warning is misleading.


---

* [SPARK-10287](https://issues.apache.org/jira/browse/SPARK-10287) | *Critical* | **After processing a query using JSON data, Spark SQL continuously refreshes metadata of the table**

I have a partitioned json table with 1824 partitions.
{code}
val df = sqlContext.read.format("json").load("aPartitionedJsonData")
val columnStr = df.schema.map(\_.name).mkString(",")
println(s"columns: $columnStr")
val hash = df
  .selectExpr(s"hash($columnStr) as hashValue")
  .groupBy()
  .sum("hashValue")
  .head()
  .getLong(0)
{code}
Looks like for JSON, we refresh metadata when we call buildScan. For a partitioned table, we call buildScan for every partition. So, looks like we will refresh this table 1824 times.


---

* [SPARK-10245](https://issues.apache.org/jira/browse/SPARK-10245) | *Blocker* | **SQLContext can't parse literal less than 0.1 (e.g. 0.01)**

{code}

scala\> sqlCtx.sql("select 0.01")
org.apache.spark.sql.AnalysisException: Decimal scale (2) cannot be greater than precision (1).;
	at org.apache.spark.sql.types.PrecisionInfo.\<init\>(DecimalType.scala:32)
	at org.apache.spark.sql.types.DecimalType.\<init\>(DecimalType.scala:68)
	at org.apache.spark.sql.catalyst.expressions.Literal$.apply(literals.scala:40)
	at org.apache.spark.sql.catalyst.SqlParser$$anonfun$numericLiteral$2$$anonfun$apply$216.apply(SqlParser.scala:335)
	at org.apache.spark.sql.catalyst.SqlParser$$anonfun$numericLiteral$2$$anonfun$apply$216.apply(SqlParser.scala:334)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:136)
	at scala.util.parsing.combinator.Parsers$Success.map(Parsers.scala:135)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$map$1.apply(Parsers.scala:242)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1$$anonfun$apply$2.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Failure.append(Parsers.scala:202)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$$anon$3.apply(Parsers.scala:222)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)
	at scala.util.parsing.combinator.Parsers$Parser$$anonfun$append$1.apply(Parsers.scala:254)

{code}


---

* [SPARK-10231](https://issues.apache.org/jira/browse/SPARK-10231) | *Minor* | **Update @Since annotation for mllib.classification**

Some public methods are missing @Since tags, and some versions are not correct.


---

* [SPARK-10230](https://issues.apache.org/jira/browse/SPARK-10230) | *Minor* | **LDA public API should use docConcentration**

{{alpha}} is provided as an alias to {{docConcentration}} because it is commonly used in literature. However, we should prefer {{docConcentration}} since it is unambiguous what we mean.

The public API currently uses {{ {get,set}OptimizeAlpha}} but should instead use {{ {get,set}OptimizeDocConcentration}}. We should also probably deprecate any public API's using {{alpha}} directly and refer users to the corresponding {{docConcentration}} methods.


---

* [SPARK-10226](https://issues.apache.org/jira/browse/SPARK-10226) | *Major* | **Error occured in SparkSQL when using  !=**

DataSource:  
                    src/main/resources/kv1.txt

SQL: 
          1. create table src(id string, name string);
          2. load data local inpath '${SparkHome}/examples/src/main/resources/kv1.txt' into table src;
          3. select count( \* ) from src where id != '0';

[ERROR] Could not expand event
java.lang.IllegalArgumentException: != 0;: event not found
	at jline.console.ConsoleReader.expandEvents(ConsoleReader.java:779)
	at jline.console.ConsoleReader.finishBuffer(ConsoleReader.java:631)
	at jline.console.ConsoleReader.accept(ConsoleReader.java:2019)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2666)
	at jline.console.ConsoleReader.readLine(ConsoleReader.java:2269)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver$.main(SparkSQLCLIDriver.scala:231)
	at org.apache.spark.sql.hive.thriftserver.SparkSQLCLIDriver.main(SparkSQLCLIDriver.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:666)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:178)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:118)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)


---

* [SPARK-10219](https://issues.apache.org/jira/browse/SPARK-10219) | *Major* | **Error when additional options provided as variable in write.df**

Opened a SparkR shell

Created a df using 
\> df \<- jsonFile(sqlContext, "examples/src/main/resources/people.json")

Assigned a variable like below
\> mode \<- "append"

When write.df called using below statement got the mentioned error
\> write.df(df, source="org.apache.spark.sql.parquet", path=par\_path, option=mode)

Error in writeType(con, type) : Unsupported type for serialization name

Whereas mode is passed as "append" itself, i.e. not via mode variable as below everything works fine

\> write.df(df, source="org.apache.spark.sql.parquet", path=par\_path, option="append")

Note: For parquet it is not needed to hanve option. But we are using Spark Salesforce package (http://spark-packages.org/package/springml/spark-salesforce) which require additional options to be passed.


---

* [SPARK-10215](https://issues.apache.org/jira/browse/SPARK-10215) | *Blocker* | **Div of Decimal returns null**

{code}
val d = Decimal(1.12321)
val df = Seq((d, 1)).toDF("a", "b")
df.selectExpr("b \* a / b").collect() =\> Array(Row(null))
{code}


---

* [SPARK-10214](https://issues.apache.org/jira/browse/SPARK-10214) | *Major* | **Improve SparkR Column, DataFrame API docs**

Right now the docs for functions like `agg` and `filter` have duplicate entries like `agg-method` and `filter-method` etc. We should use the `name` Rd tag and remove these duplicates.


---

* [SPARK-10210](https://issues.apache.org/jira/browse/SPARK-10210) | *Critical* | **Exception "Could not compute split, block input-XXX not found" after streaming app driver recovers from checkpoint**

When write ahead log is not enabled, a recovered streaming driver still tries to run jobs using pre-failure block ids, and fails as the block do not exists in-memory any more (and cannot be recovered as receiver WAL is not enabled).

This occurs because the driver-side WAL of ReceivedBlockTracker is recovers that past block information, and ReceiveInputDStream creates BlockRDDs even if those blocks do not exist.

The solution is to filter out block ids that do not exist before creating the BlockRDD.


---

* [SPARK-10198](https://issues.apache.org/jira/browse/SPARK-10198) | *Blocker* | **Turn off Hive verifyPartitionPath by default**

I've seen several cases in production where this option either causes us to fail reading valid tables, or incorrectly returns no results.  It also invalidates our new metastore partition pruning feature.  Since there is not much time to dig into the root cause, I propose we turn it off by default for Spark 1.5.


---

* [SPARK-10197](https://issues.apache.org/jira/browse/SPARK-10197) | *Blocker* | **Add null check in wrapperFor (inside HiveInspectors).**

I tried to save a table to ORC, but seems we need to add null check in wrapperFor method of HiveInspectors. 
{code}
Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1267)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1255)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1254)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1254)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:684)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1442)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1431)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:554)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1805)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1818)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1895)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:927)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:927)
	at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:197)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:137)
	at com.databricks.spark.sql.perf.tpcds.Tables$Table.genData(Tables.scala:125)
	at com.databricks.spark.sql.perf.tpcds.Tables$$anonfun$genData$2.apply(Tables.scala:169)
	at com.databricks.spark.sql.perf.tpcds.Tables$$anonfun$genData$2.apply(Tables.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at com.databricks.spark.sql.perf.tpcds.Tables.genData(Tables.scala:167)
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:391)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$wrapperFor$2.apply(HiveInspectors.scala:377)
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$wrapperFor$2.apply(HiveInspectors.scala:377)
	at org.apache.spark.sql.hive.orc.OrcOutputWriter.writeInternal(OrcRelation.scala:130)
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:346)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-10196](https://issues.apache.org/jira/browse/SPARK-10196) | *Blocker* | **Failed to save json data with a decimal type in the schema**

I try to save a dataset with a decimal type in it and I got
{code}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 19.0 failed 4 times, most recent failure: Lost task 7.3 in stage 19.0 (TID 932, 10.0.243.5): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:391)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: scala.MatchError: (DecimalType(7,2),40.74) (of class scala.Tuple2)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:126)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$.apply(JacksonGenerator.scala:133)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.writeInternal(JSONRelation.scala:191)
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:334)
	... 8 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1267)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1255)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1254)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1254)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:684)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:684)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1480)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1442)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1431)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:554)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1805)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1818)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1895)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply$mcV$sp(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1.apply(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation.run(InsertIntoHadoopFsRelation.scala:108)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:69)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:140)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:138)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:138)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:927)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:927)
	at org.apache.spark.sql.execution.datasources.ResolvedDataSource$.apply(ResolvedDataSource.scala:197)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:146)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:137)
	at com.databricks.spark.sql.perf.tpcds.Tables$Table.genData(Tables.scala:125)
	at com.databricks.spark.sql.perf.tpcds.Tables$$anonfun$genData$2.apply(Tables.scala:169)
	at com.databricks.spark.sql.perf.tpcds.Tables$$anonfun$genData$2.apply(Tables.scala:167)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at com.databricks.spark.sql.perf.tpcds.Tables.genData(Tables.scala:167)
Caused by: org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:391)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: scala.MatchError: (DecimalType(7,2),40.74) (of class scala.Tuple2)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:126)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$.apply(JacksonGenerator.scala:133)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.writeInternal(JSONRelation.scala:191)
	at org.apache.spark.sql.execution.datasources.DynamicPartitionWriterContainer.writeRows(WriterContainer.scala:334)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-10195](https://issues.apache.org/jira/browse/SPARK-10195) | *Critical* | **Spark SQL's internal types should not be exposed via data sources Filter interfaces**

Spark SQL's data sources API exposes Catalyst's internal types through its Filter interfaces. This is a problem because types like UTF8String are not stable developer APIs and should not be exposed to third-parties.

This issue caused incompatibilities when upgrading our {{spark-redshift}} library to work against Spark 1.5.0.  To avoid these issues in the future, the Filter interfaces should only expose public types.


---

* [SPARK-10190](https://issues.apache.org/jira/browse/SPARK-10190) | *Blocker* | **NPE in CatalystTypeConverters when converting null decimal value back to Scala**

CatalystTypeConverters can throw an NPE when converting a null decimal value back to Scala:

{code}
18:42:40.130 ERROR org.apache.spark.sql.execution.datasources.DefaultWriterContainer: Aborting task.
java.lang.NullPointerException
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$DecimalConverter.toScala(CatalystTypeConverters.scala:332)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$DecimalConverter.toScala(CatalystTypeConverters.scala:318)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$ArrayConverter$$anonfun$toScala$1.apply(CatalystTypeConverters.scala:178)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$ArrayConverter$$anonfun$toScala$1.apply(CatalystTypeConverters.scala:177)
    at org.apache.spark.sql.types.ArrayData.foreach(ArrayData.scala:127)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$ArrayConverter.toScala(CatalystTypeConverters.scala:177)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$ArrayConverter.toScalaImpl(CatalystTypeConverters.scala:185)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$ArrayConverter.toScalaImpl(CatalystTypeConverters.scala:148)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toScala(CatalystTypeConverters.scala:110)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toScala(CatalystTypeConverters.scala:278)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toScala(CatalystTypeConverters.scala:245)
    at org.apache.spark.sql.catalyst.CatalystTypeConverters$$anonfun$createToScalaConverter$2.apply(CatalystTypeConverters.scala:406)
    at org.apache.spark.sql.sources.OutputWriter.writeInternal(interfaces.scala:380)
    at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:240)
    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
    at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
    at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
    at org.apache.spark.scheduler.Task.run(Task.scala:88)
    at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-10188](https://issues.apache.org/jira/browse/SPARK-10188) | *Critical* | **Pyspark CrossValidator with RMSE selects incorrect model**

Pyspark {{CrossValidator}} is giving incorrect results when selecting estimators using RMSE as an evaluation metric.

In the example below, it should be selecting the {{LogisticRegression}} estimator with zero regularization as that gives the most accurate result, but instead it selects the one with the largest.

Probably related to: SPARK-10097

{code}
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml.regression import LinearRegression
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator, CrossValidatorModel
from pyspark.ml.feature import Binarizer
from pyspark.mllib.linalg import Vectors
from pyspark.sql import SQLContext

sqlContext = SQLContext(sc)

# Label = 2 \* feature
train = sqlContext.createDataFrame([
    (Vectors.dense([10.0]), 20.0), 
    (Vectors.dense([100.0]), 200.0), 
    (Vectors.dense([1000.0]), 2000.0)] \* 10,
    ["features", "label"])

test = sqlContext.createDataFrame([
    (Vectors.dense([1000.0]),)],  
    ["features"])

# Expected prediction 2000.0
print LinearRegression(regParam=0.0).fit(train).transform(test).collect() # Predicts 2000.0 (perfect)
print LinearRegression(regParam=100.0).fit(train).transform(test).collect() # Predicts 1869.31
print LinearRegression(regParam=1000000.0).fit(train).transform(test).collect() # 741.08 (worst)

# Cross-validation
lr = LinearRegression()
rmse\_eval = RegressionEvaluator(metricName="rmse")
grid = (ParamGridBuilder()
    .addGrid( lr.regParam, [0.0, 100.0, 1000000.0] )
    .build())
cv = CrossValidator(estimator=lr, estimatorParamMaps=grid, evaluator=rmse\_eval)
cv\_model = cv.fit(train)

cv\_model.bestModel.transform(test).collect() # Predicts 741.08 (i.e. worst model selected)
{code}

Once workaround for users would be to add a wrapper around the selected evaluator to invert the metric:

{code}
class InvertedEvaluator(Evaluator):

    def \_\_init\_\_(self, evaluator):
        super(Evaluator, self).\_\_init\_\_()
        self.evaluator = evaluator
    
    def \_evaluate(self, dataset):
        return -self.evaluator.evaluate(dataset)

 invertedEvaluator = InvertedEvaluator(RegressionEvaluator(metricName="rmse"))
{code}


---

* [SPARK-10177](https://issues.apache.org/jira/browse/SPARK-10177) | *Blocker* | **Parquet support interprets timestamp values differently from Hive 0.14.0+**

Running the following SQL under Hive 0.14.0+ (tested against 0.14.0 and 1.2.1):
{code:sql}
CREATE TABLE ts\_test STORED AS PARQUET
AS SELECT CAST("2015-01-01 00:00:00" AS TIMESTAMP);
{code}
Then read the Parquet file generated by Hive with Spark SQL:
{noformat}
scala\> sqlContext.read.parquet("hdfs://localhost:9000/user/hive/warehouse\_hive14/ts\_test").collect()
res1: Array[org.apache.spark.sql.Row] = Array([2015-01-01 12:00:00.0])
{noformat}

This issue can be easily reproduced with [this test case in PR #8392\|https://github.com/apache/spark/pull/8392/files#diff-1e55698cc579cbae676f827a89c2dc2eR116].

Spark 1.4.1 works as expected in this case.

----

Update:

Seems that the problem is that we do Julian day conversion wrong in {{DateTimeUtils}}.  The following {{spark-shell}} session illustrates it:
{code}
import java.sql.\_
import java.util.\_
import org.apache.hadoop.hive.ql.io.parquet.timestamp.\_
import org.apache.spark.sql.catalyst.util.\_

TimeZone.setDefault(TimeZone.getTimeZone("GMT"))
val ts = Timestamp.valueOf("1970-01-01 00:00:00")
val nt = NanoTimeUtils.getNanoTime(ts, false)
val jts = DateTimeUtils.fromJulianDay(nt.getJulianDay, nt.getTimeOfDayNanos)
DateTimeUtils.toJavaTimestamp(jts)

// ==\> java.sql.Timestamp = 1970-01-01 12:00:00.0
{code}


---

* [SPARK-10175](https://issues.apache.org/jira/browse/SPARK-10175) | *Minor* | **Enhance spark doap file**

The Spark doap has broken links and is also missing entries related to issue tracker and mailing lists. This affects the list in projects.apache.org and also in the main apache website.


---

* [SPARK-10168](https://issues.apache.org/jira/browse/SPARK-10168) | *Blocker* | **Streaming assembly jars doesn't publish correctly**

E.g., https://repository.apache.org/content/repositories/orgapachespark-1137/org/apache/spark/spark-streaming-kafka-assembly\_2.10/1.5.0-rc1/

{code}
spark-streaming-kafka-assembly\_2.10-1.5.0-rc1.jar	Fri Aug 21 01:39:01 UTC 2015	8176	
{code}

spark-streaming-kafka-assembly\_2.10-1.5.0-rc1.jar doesn't contain the correct content.

This is because https://github.com/apache/spark/pull/5632 changes the output path of the shaded jar but maven publish plugin doesn't know it.


---

* [SPARK-10166](https://issues.apache.org/jira/browse/SPARK-10166) | *Critical* | **Python Streaming checkpoint recovery fails if an active Python SparkContext already exists**

The current code in Python StreamingContext.getOrCreate() will try to create a new Python SparkContext, even if another active context exists. The change use an active context if it exists or create a new one


---

* [SPARK-10165](https://issues.apache.org/jira/browse/SPARK-10165) | *Blocker* | **Nested Hive UDF resolution fails in Analyzer**

When running a query with hive udfs nested in hive udfs the analyzer fails since we don't check children resolution first.


---

* [SPARK-10164](https://issues.apache.org/jira/browse/SPARK-10164) | *Critical* | **GMM bug: match error**

GaussianMixture now distributes matrix decompositions for certain problem sizes.  Distributed computation actually fails, but this was not tested in unit tests.  This is a regression.

Here is an example failure:
{code}
Exception in thread "main" scala.MatchError: ArrayBuffer(0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000
001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.050
00000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001, 0.0500000000000
0001, 0.05000000000000001, 0.05000000000000001, 0.05000000000000001) (of class scala.collection.mutable.ArrayBuffer)
        at scala.runtime.ScalaRunTime$.array\_apply(ScalaRunTime.scala:71)
        at scala.Array$.slowcopy(Array.scala:81)
        at scala.Array$.copy(Array.scala:107)
        at org.apache.spark.mllib.clustering.GaussianMixture.run(GaussianMixture.scala:215)
        at mllib.perf.clustering.GaussianMixtureTest.run(GaussianMixtureTest.scala:60)
        at mllib.perf.TestRunner$$anonfun$2.apply(TestRunner.scala:66)
        at mllib.perf.TestRunner$$anonfun$2.apply(TestRunner.scala:64)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
        at scala.collection.immutable.Range.foreach(Range.scala:141)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
        at scala.collection.AbstractTraversable.map(Traversable.scala:105)
        at mllib.perf.TestRunner$.main(TestRunner.scala:64)
        at mllib.perf.TestRunner.main(TestRunner.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
15/08/21 21:25:33 INFO spark.SparkContext: Invoking stop() from shutdown hook
{code}


---

* [SPARK-10163](https://issues.apache.org/jira/browse/SPARK-10163) | *Major* | **Allow single-category features for GBT models**

There is a remaining issue with using single-category features for GBTRegressor and GBTClassifier.  They technically already work, but they include a validity check which is too strict.  This is to remove that check.


---

* [SPARK-10159](https://issues.apache.org/jira/browse/SPARK-10159) | *Major* | **Hive 1.3.x GenericUDFDate NPE issue**

When run sql query with HiveContext, Hive 1.3.x GenericUDFDate NPE issue.

The following is the query and log
{code}
SELECT a.stationid AS stationid,
a.month AS month,
a.year AS year,
AVG(a.mean) AS mean,
MIN(a.min) AS min,
MAX(a.max) AS max
FROM 
  (SELECT \*,
     YEAR(date) AS year,
     MONTH(date) AS month,
     FROM\_UNIXTIME(UNIX\_TIMESTAMP(TO\_DATE(date), 'yyyy-MM-dd'), 'E') AS weekday
   FROM weathercql.daily) a
WHERE ((a.weekday = 'Mon'))
  AND (a.metric = 'temperature')
GROUP BY a.stationid, a.month, a.year
ORDER BY stationid, year, month
LIMIT 100
{code}

log {code}
    Filter ((HiveSimpleUdf#org.apache.hadoop.hive.ql.udf.UDFFromUnixTime(HiveGenericUdf#org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp(HiveGenericUdf#org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate(date#81),yyyy-MM-dd),E) = Mon) && (metric#80 = temperature))

ERROR 2015-08-20 15:39:06 org.apache.spark.sql.hive.thriftserver.SparkExecuteStatementOperation: Error executing query:
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 2.0 failed 4 times, most recent failure: Lost task 1.3 in stage 2.0 (TID 208, 127.0.0.1): java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFDate.evaluate(GenericUDFDate.java:119)
	at org.apache.spark.sql.hive.HiveGenericUdf.eval(hiveUdfs.scala:188)
	at org.apache.spark.sql.hive.HiveGenericUdf$$anonfun$eval$2.apply(hiveUdfs.scala:184)
	at org.apache.spark.sql.hive.DeferredObjectAdapter.get(hiveUdfs.scala:138)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFToUnixTimeStamp.evaluate(GenericUDFToUnixTimeStamp.java:121)
	at org.apache.hadoop.hive.ql.udf.generic.GenericUDFUnixTimeStamp.evaluate(GenericUDFUnixTimeStamp.java:52)
	at org.apache.spark.sql.hive.HiveGenericUdf.eval(hiveUdfs.scala:188)
	at org.apache.spark.sql.hive.HiveSimpleUdf$$anonfun$eval$1.apply(hiveUdfs.scala:121)
	at org.apache.spark.sql.hive.HiveSimpleUdf$$anonfun$eval$1.apply(hiveUdfs.scala:121)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.hive.HiveSimpleUdf.eval(hiveUdfs.scala:121)
	at org.apache.spark.sql.catalyst.expressions.EqualTo.eval(predicates.scala:191)
	at org.apache.spark.sql.catalyst.expressions.And.eval(predicates.scala:130)
	at org.apache.spark.sql.catalyst.expressions.InterpretedPredicate$$anonfun$create$1.apply(predicates.scala:30)
	at org.apache.spark.sql.catalyst.expressions.InterpretedPredicate$$anonfun$create$1.apply(predicates.scala:30)
	at scala.collection.Iterator$$anon$14.hasNext(Iterator.scala:390)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$7.apply(Aggregate.scala:154)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$7.apply(Aggregate.scala:149)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273) ~[spark-core\_2.10-1.4.1.1.jar:1.4.1.1]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264) ~[spark-core\_2.10-1.4.1.1.jar:1.4.1.1]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263) ~[spark-core\_2.10-1.4.1.1.jar:1.4.1.1]
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59) ~[scala-library-2.10.5.jar:na]
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47) ~[scala-library-2.10.5.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263) ~[spark-core\_2.10-1.4.1.1.jar:1.4.1.1]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730) ~[spark-core\_2.10-1.4.1.1.jar:1.4.1.1]
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730) ~[spark-core\_2.10-1.4.1.1.jar:1.4.1.1]
	at scala.Option.foreach(Option.scala:236) ~[scala-library-2.10.5.jar:na]
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730) ~[spark-core\_2.10-1.4.1.1.jar:1.4.1.1]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457) ~[spark-core\_2.10-1.4.1.1.jar:1.4.1.1]
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418) ~[spark-core\_2.10-1.4.1.1.jar:1.4.1.1]
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48) ~[spark-core\_2.10-1.4.1.1.jar:1.4.1.1]
{code}

https://github.com/apache/hive/blob/branch-0.13/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFDate.java#L61 needs to be transient. 

Should a custom Hive build for Spark? or open a ticket on OSS Hive on it?


---

* [SPARK-10148](https://issues.apache.org/jira/browse/SPARK-10148) | *Minor* | **Display active and inactive receiver numbers in Streaming page**

Displaying active and inactive receiver numbers in Streaming page is helpful to  understand whether receivers have started or not.


---

* [SPARK-10144](https://issues.apache.org/jira/browse/SPARK-10144) | *Major* | **Actually show peak execution memory on UI by default**

The peak execution memory metric was introduced in SPARK-8735. That was before Tungsten was enabled by default, so it assumed that `spark.sql.unsafe.enabled` must be explicitly set to true. This is no longer the case...


---

* [SPARK-10143](https://issues.apache.org/jira/browse/SPARK-10143) | *Critical* | **Parquet changed the behavior of calculating splits**

When Parquet's task side metadata is enabled (by default it is enabled and it needs to be enabled to deal with tables with many files), Parquet delegates the work of calculating initial splits to FileInputFormat (see https://github.com/apache/parquet-mr/blob/master/parquet-hadoop/src/main/java/org/apache/parquet/hadoop/ParquetInputFormat.java#L301-L311). If filesystem's block size is smaller than the row group size and users do not set min split size, splits in the initial split list will have lots of dummy splits and they contribute to empty tasks (because the starting point and ending point of a split does not cover the starting point of a row group).


---

* [SPARK-10142](https://issues.apache.org/jira/browse/SPARK-10142) | *Critical* | **Python Streaming checkpoint recovery does not work with non-local file path**

The Python code in StreamingContext.getOrCreate() check whether the give checkpointPath exists on local file system. The solution is to use the same code path as Java to verify whether the valid checkpoint is present or not


---

* [SPARK-10140](https://issues.apache.org/jira/browse/SPARK-10140) | *Major* | **Add target fields to @Since annotation**

Add target fields to @Since so constructor params and fields also get annotated.


---

* [SPARK-10138](https://issues.apache.org/jira/browse/SPARK-10138) | *Blocker* | **Setters do not return self type in Java MultilayerPerceptronClassifier**

We need to move setters to the final class instead.


---

* [SPARK-10137](https://issues.apache.org/jira/browse/SPARK-10137) | *Blocker* | **Avoid to restart receivers if scheduleReceivers returns balanced results**

In some cases, even if scheduleReceivers returns balanced results, ReceiverTracker still may reject some receivers and force them to restart. See my PR for more details.


---

* [SPARK-10136](https://issues.apache.org/jira/browse/SPARK-10136) | *Blocker* | **Parquet support fail to decode Avro/Thrift arrays of primitive array (e.g. array\<array\<int\>\>)**

The following Avro schema
{noformat}
record AvroNonNullableArrays {
  array\<array\<int\>\> int\_arrays\_column;
}
{noformat}
is translated into the following Parquet schema by parquet-avro 1.7.0:
{noformat}
message root {
  required group int\_arrays\_column (LIST) {
    repeated group array (LIST) {
      repeated int32 array;
    }
  }
}
{noformat}
When making converters, the inner most {{array}} field is covered by {{RepeatedPrimitiveConverter}}, which is introduced in SPARK-9340 and [PR #8070\|https://github.com/apache/spark/pull/8070]. However, PR #8070 assumes that repeated converters can only be created for struct fields, and doesn't take array elements into account.

Verified that 1.4 doesn't suffer this issue. This is a 1.5 regression.


---

* [SPARK-10135](https://issues.apache.org/jira/browse/SPARK-10135) | *Trivial* | **Percent of pruned partitions is shown wrong**

When reading partitioned Parquet in SparkSQL, an info message about the number of pruned partitions is displayed.

Actual:
"Selected 15 partitions out of 181, pruned -1106.6666666666667% partitions."

Expected:
"Selected 15 partitions out of 181, pruned 91.71270718232044% partitions."

Fix: (i'm newbie here so please help make patch, thanks!)
in DataSourceStrategy.scala in method apply()

insted of:
val percentPruned = (1 - total.toDouble / selected.toDouble) \* 100
should be:
val percentPruned = (1 - selected.toDouble / total.toDouble) \* 100


---

* [SPARK-10130](https://issues.apache.org/jira/browse/SPARK-10130) | *Blocker* | **type coercion for IF should have children resolved first**

SELECT IF(a \> 0, a, 0) FROM (SELECT key a FROM src) temp;


---

* [SPARK-10128](https://issues.apache.org/jira/browse/SPARK-10128) | *Blocker* | **Kinesis sequence numbers cannot be recovered from WAL because WAL does not use the correct classloader**

Recovering Kinesis sequence numbers from WAL leads to classnotfoundexception. The solution is to not use the default classloader of the ObjectInputStream, but `Thread.currentThread().getContextClassLoader`


---

* [SPARK-10126](https://issues.apache.org/jira/browse/SPARK-10126) | *Blocker* | **Fix typo in release-build.sh which broke snapshot publishing for Scala 2.11**

Obvious typo in release-build.sh broke the 2.11 Maven snapshot publisher:

{code}
./dev/change-scala-version.sh 2.10
$MVN -DzincPort=$ZINC\_PORT -Dscala-2.11 --settings $tmp\_settings \
-DskipTests $PUBLISH\_PROFILES deploy
{code}

Should change to 2.11, not 2.10.  Should probably clean in-between as well.


---

* [SPARK-10125](https://issues.apache.org/jira/browse/SPARK-10125) | *Major* | **Fix a potential deadlock in JobGenerator.stop**

Because `lazy val` uses `this` lock, if JobGenerator.stop and JobGenerator.doCheckpoint (JobGenerator.shouldCheckpoint has not yet been initialized) run at the same time, it may hang.

Here are the stack traces for the deadlock:
{code}
"pool-1-thread-1-ScalaTest-running-StreamingListenerSuite" #11 prio=5 os\_prio=31 tid=0x00007fd35d094800 nid=0x5703 in Object.wait() [0x000000012ecaf000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Thread.join(Thread.java:1245)
        - locked \<0x00000007b5d8d7f8\> (a org.apache.spark.util.EventLoop$$anon$1)
        at java.lang.Thread.join(Thread.java:1319)
        at org.apache.spark.util.EventLoop.stop(EventLoop.scala:81)
        at org.apache.spark.streaming.scheduler.JobGenerator.stop(JobGenerator.scala:155)
        - locked \<0x00000007b5d8cea0\> (a org.apache.spark.streaming.scheduler.JobGenerator)
        at org.apache.spark.streaming.scheduler.JobScheduler.stop(JobScheduler.scala:95)
        - locked \<0x00000007b5d8ced8\> (a org.apache.spark.streaming.scheduler.JobScheduler)
        at org.apache.spark.streaming.StreamingContext.stop(StreamingContext.scala:687)

"JobGenerator" #67 daemon prio=5 os\_prio=31 tid=0x00007fd35c3b9800 nid=0x9f03 waiting for monitor entry [0x0000000139e4a000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.spark.streaming.scheduler.JobGenerator.shouldCheckpoint$lzycompute(JobGenerator.scala:63)
        - waiting to lock \<0x00000007b5d8cea0\> (a org.apache.spark.streaming.scheduler.JobGenerator)
        at org.apache.spark.streaming.scheduler.JobGenerator.shouldCheckpoint(JobGenerator.scala:63)
        at org.apache.spark.streaming.scheduler.JobGenerator.doCheckpoint(JobGenerator.scala:290)
        at org.apache.spark.streaming.scheduler.JobGenerator.org$apache$spark$streaming$scheduler$JobGenerator$$processEvent(JobGenerator.scala:182)
        at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:83)
        at org.apache.spark.streaming.scheduler.JobGenerator$$anon$1.onReceive(JobGenerator.scala:82)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}

I can use this patch to produce this deadlock: https://github.com/zsxwing/spark/commit/8a88f28d1331003a65fabef48ae3d22a7c21f05f

And a timeout build in Jenkins due to this deadlock: https://amplab.cs.berkeley.edu/jenkins/job/NewSparkPullRequestBuilder/1654/


---

* [SPARK-10124](https://issues.apache.org/jira/browse/SPARK-10124) | *Major* | **Mesos cluster mode causes exception when multiple spark apps are being scheduled**

Currently the spark applications can be queued to the Mesos cluster dispatcher, but when multiple jobs are in queue we don't handle removing jobs from the buffer correctly while iterating and causes null pointer exception.


---

* [SPARK-10122](https://issues.apache.org/jira/browse/SPARK-10122) | *Major* | **AttributeError: 'RDD' object has no attribute 'offsetRanges'**

SPARK-8389 added the offsetRanges interface to Kafka direct streams. This however appears to break when chaining operations after a transform operation. Following is example code that would result in an error (stack trace below). Note that if the 'count()' operation is taken out of the example code then this error does not occur anymore, and the Kafka data is printed.

{code:title=kafka\_test.py\|collapse=true}
from pyspark import SparkContext
from pyspark.streaming import StreamingContext
from pyspark.streaming.kafka import KafkaUtils

def attach\_kafka\_metadata(kafka\_rdd):
    offset\_ranges = kafka\_rdd.offsetRanges()

    return kafka\_rdd


if \_\_name\_\_ == "\_\_main\_\_":
    sc = SparkContext(appName='kafka-test')
    ssc = StreamingContext(sc, 10)

    kafka\_stream = KafkaUtils.createDirectStream(
        ssc,
        [TOPIC],
        kafkaParams={
            'metadata.broker.list': BROKERS,
        },
    )
    kafka\_stream.transform(attach\_kafka\_metadata).count().pprint()

    ssc.start()
    ssc.awaitTermination()
{code}

{code:title=Stack trace\|collapse=true}
Traceback (most recent call last):
  File "/home/spark/spark/python/lib/pyspark.zip/pyspark/streaming/util.py", line 62, in call
    r = self.func(t, \*rdds)
  File "/home/spark/spark/python/lib/pyspark.zip/pyspark/streaming/dstream.py", line 616, in \<lambda\>
    self.func = lambda t, rdd: func(t, prev\_func(t, rdd))
  File "/home/spark/spark/python/lib/pyspark.zip/pyspark/streaming/dstream.py", line 616, in \<lambda\>
    self.func = lambda t, rdd: func(t, prev\_func(t, rdd))
  File "/home/spark/spark/python/lib/pyspark.zip/pyspark/streaming/dstream.py", line 616, in \<lambda\>
    self.func = lambda t, rdd: func(t, prev\_func(t, rdd))
  File "/home/spark/spark/python/lib/pyspark.zip/pyspark/streaming/dstream.py", line 616, in \<lambda\>
    self.func = lambda t, rdd: func(t, prev\_func(t, rdd))
  File "/home/spark/spark/python/lib/pyspark.zip/pyspark/streaming/kafka.py", line 332, in \<lambda\>
    func = lambda t, rdd: oldfunc(rdd)
  File "/home/spark/ad\_realtime/batch/kafka\_test.py", line 7, in attach\_kafka\_metadata
    offset\_ranges = kafka\_rdd.offsetRanges()
AttributeError: 'RDD' object has no attribute 'offsetRanges'
{code}


---

* [SPARK-10121](https://issues.apache.org/jira/browse/SPARK-10121) | *Critical* | **Custom Class added through Spark SQL's add jar command may not be in the class loader used by metadataHive**

One symptom is that people connecting to Spark SQL through thrift server may see class not found exception when accessing their tables using custom serdes.

I feel https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/commands.scala#L103 is not enough. We probably should set the class loader explicitly to metadataHive like what we do at https://github.com/apache/spark/blob/master/sql/hive/src/main/scala/org/apache/spark/sql/hive/execution/commands.scala#L101.


---

* [SPARK-10119](https://issues.apache.org/jira/browse/SPARK-10119) | *Critical* | **Utils.isDynamicAllocationEnabled returns wrong value if dynAlloc is explicitly disabled**

{code}
  def isDynamicAllocationEnabled(conf: SparkConf): Boolean = {
    conf.contains("spark.dynamicAllocation.enabled") &&
      conf.getInt("spark.executor.instances", 0) == 0
  }
{code}

If you have {{spark.dynamicAllocation.enabled=false}} in the config, that will return true.


---

* [SPARK-10118](https://issues.apache.org/jira/browse/SPARK-10118) | *Major* | **Improve SparkR API docs for 1.5 release**

This includes checking if the new DataFrame functions & expression show up appropriately in the roxygen docs


---

* [SPARK-10107](https://issues.apache.org/jira/browse/SPARK-10107) | *Blocker* | **NPE in format\_number**

{code}
sqlContext.range(1\<\<20).selectExpr("format\_number(id, 2)").show()
{code}

{code}
java.lang.NullPointerException
	at org.apache.spark.sql.catalyst.expressions.UnsafeRowWriters$UTF8StringWriter.getSize(UnsafeRowWriters.java:93)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.TungstenProject$$anonfun$2$$anonfun$apply$3.apply(basicOperators.scala:88)
	at org.apache.spark.sql.execution.TungstenProject$$anonfun$2$$anonfun$apply$3.apply(basicOperators.scala:86)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:129)
	at org.apache.spark.sql.columnar.InMemoryRelation$$anonfun$3$$anon$1.next(InMemoryColumnarTableScan.scala:120)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:278)
	at org.apache.spark.CacheManager.putInBlockManager(CacheManager.scala:171)
	at org.apache.spark.CacheManager.getOrCompute(CacheManager.scala:78)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:262)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:46)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-10106](https://issues.apache.org/jira/browse/SPARK-10106) | *Major* | **Add `ifelse` Column function to SparkR**

Add a column function on a DataFrame like `ifelse` in R to SparkR.
I guess we could implement it with a combination with {{when}} and {{otherwise}}.

h3. Example

If {{df$x \> 0}} is TRUE, then return 0, otherwise return 1.
{noformat}
ifelse(df$x \> 0, 0, 1)
{noformat}


---

* [SPARK-10102](https://issues.apache.org/jira/browse/SPARK-10102) | *Major* | **Fix a race condition that startReceiver may happen before setting trackerState to Started**

Test failure: https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/HADOOP\_PROFILE=hadoop-2.4,label=spark-test/3305/testReport/junit/org.apache.spark.streaming/StreamingContextSuite/stop\_gracefully/

There is a race condition that setting `trackerState` to `Started` could happen after calling `startReceiver`. Then `startReceiver` won't start the receivers because it uses `! isTrackerStarted` to check if ReceiverTracker is stopping or stopped. But actually, `trackerState` is `Initialized` and will be changed to `Started` soon.

Therefore, we should use `isTrackerStopping \|\| isTrackerStopped`.


---

* [SPARK-10100](https://issues.apache.org/jira/browse/SPARK-10100) | *Major* | **Eliminate hash table lookup if there is no grouping key in aggregation.**

\*Update: The title of this jira has been changed to "Eliminate hash table lookup if there is no grouping key in aggregation."\*. For the Max and Min expressions, we can revisit them later if we find a better way to improve the performance and open a new jira.

\*The original title of this JIRA is "AggregateFunction2's Max is slower than AggregateExpression1's MaxFunction"\*. Below is the original description:
Looks like Max (probably Min) implemented based on AggregateFunction2 is slower than the old MaxFunction.


---

* [SPARK-10099](https://issues.apache.org/jira/browse/SPARK-10099) | *Trivial* | **Use @deprecated instead of @Deprecated in Scala code**

{code}
$ find . -name "\*.scala" -exec grep -l "@Deprecated" {} \;
./core/src/main/scala/org/apache/spark/api/java/JavaRDDLike.scala
./core/src/main/scala/org/apache/spark/deploy/SparkHadoopUtil.scala
./streaming/src/main/scala/org/apache/spark/streaming/api/java/JavaDStreamLike.scala
{code}


---

* [SPARK-10097](https://issues.apache.org/jira/browse/SPARK-10097) | *Major* | **ML Evaluator should indicate if metric should be maximized or minimized**

ML Evaluator currently requires that metrics be maximized (bigger is better).  That is counterintuitive for some metrics.  Currently, we hackily negate some metrics in RegressionEvaluator, which is weird.  Instead, we should:
\* Return the metric as expected (e.g., "rmse" should return RMSE, not its negation).
\* Provide an indicator of whether the metric should be maximized or minimized.

Model selection algorithms can use the indicator as needed.


---

* [SPARK-10096](https://issues.apache.org/jira/browse/SPARK-10096) | *Blocker* | **CreateStruct/CreateArray/CreateNamedStruct broken with UDFs**

{code}
val f = udf((a: String) =\> a)
val df = sc.parallelize((1,1) :: Nil).toDF("a", "b")
df.select(struct($"a").as("s")).select(f($"s.a")).collect()
{code}

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 9.0 failed 4 times, most recent failure: Lost task 3.3 in stage 9.0 (TID 78, 10.0.243.97): java.lang.UnsupportedOperationException
	at org.apache.spark.sql.catalyst.expressions.CreateStructUnsafe.eval(complexTypeCreator.scala:209)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:247)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:247)
	at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:76)
	at org.apache.spark.sql.catalyst.expressions.ScalaUDF$$anonfun$2.apply(ScalaUDF.scala:74)
	at org.apache.spark.sql.catalyst.expressions.ScalaUDF.eval(ScalaUDF.scala:964)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.TungstenProject$$anonfun$3$$anonfun$apply$3.apply(basicOperators.scala:90)
	at org.apache.spark.sql.execution.TungstenProject$$anonfun$3$$anonfun$apply$3.apply(basicOperators.scala:88)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:905)
	at org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:905)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1826)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1826)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-10095](https://issues.apache.org/jira/browse/SPARK-10095) | *Minor* | **Should not use the private field of BigInteger**

In UnsafeRow, we use the private field of BigInteger for better performance, but it actually didn't contribute much to end-to-end runtime, and make it not portable (may fail on other JVM implementations).

So we should use the public API instead.


---

* [SPARK-10093](https://issues.apache.org/jira/browse/SPARK-10093) | *Blocker* | **Invalid transformations for TungstenProject of struct type**

Code to reproduce:
{code}
val df = Seq((1,1)).toDF("a", "b")
df.where($"a" === 1)
  .select($"a", $"b", struct($"b"))
  .orderBy("a")
  .select(struct($"b"))
  .collect()
{code}

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 37.0 failed 4 times, most recent failure: Lost task 1.3 in stage 37.0 (TID 1118, 10.0.167.218): org.apache.spark.sql.catalyst.errors.package$TreeNodeException: makeCopy, tree:
Exchange rangepartitioning(a#197 ASC)
 ConvertToSafe
  TungstenProject [\_1#195 AS a#197,\_2#196 AS b#198,struct(\_2#196 AS b#198) AS struct(b)#199]
   Filter (\_1#195 = 1)
    LocalTableScan [\_1#195,\_2#196], [[1,1]]

	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:49)
	at org.apache.spark.sql.catalyst.trees.TreeNode.makeCopy(TreeNode.scala:315)
	at org.apache.spark.sql.execution.SparkPlan.makeCopy(SparkPlan.scala:80)
	at org.apache.spark.sql.execution.SparkPlan.makeCopy(SparkPlan.scala:46)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:280)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:232)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:232)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:232)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:232)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:232)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$1.apply(TreeNode.scala:232)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:232)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:234)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformDown$2.apply(TreeNode.scala:234)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:234)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:217)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformAllExpressions(QueryPlan.scala:134)
	at org.apache.spark.sql.execution.TungstenProject$$anonfun$2.apply(basicOperators.scala:81)
	at org.apache.spark.sql.execution.TungstenProject$$anonfun$2.apply(basicOperators.scala:80)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.GeneratedConstructorAccessor48.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1$$anonfun$apply$10.apply(TreeNode.scala:326)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1$$anonfun$apply$10.apply(TreeNode.scala:325)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1.apply(TreeNode.scala:323)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$makeCopy$1.apply(TreeNode.scala:315)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:48)
	... 74 more
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.execution.Exchange.sparkConf$lzycompute(Exchange.scala:130)
	at org.apache.spark.sql.execution.Exchange.sparkConf(Exchange.scala:130)
	at org.apache.spark.sql.execution.Exchange.\<init\>(Exchange.scala:137)
	... 83 more
{code}


---

* [SPARK-10092](https://issues.apache.org/jira/browse/SPARK-10092) | *Blocker* | **Multi-DB support follow up**

Seems we need a follow-up work for our multi-db support. Here are issues we need to address.

1. saveAsTable always save the table in the folder of the current database
2. HiveContext's refrshTable and analyze do not dbName.tableName.
3. It will be good to use TableIdentifier in CreateTableUsing, CreateTableUsingAsSelect, CreateTempTableUsing, CreateTempTableUsingAsSelect, CreateMetastoreDataSource, and CreateMetastoreDataSourceAsSelect, instead of using string representation (actually, in several places we have already parsed the string and get the TableIdentifier).


---

* [SPARK-10090](https://issues.apache.org/jira/browse/SPARK-10090) | *Blocker* | **After division, Decimal may have longer precision than expected**

In TPCDS Q59, the result should be DecimalType(37, 20), but got Decimal('0.69903637110664268591656984574863203607'), should be Decimal('0.69903637110664268592').


---

* [SPARK-10089](https://issues.apache.org/jira/browse/SPARK-10089) | *Trivial* | **SQL tests leave unmanaged files in source directory**

After running SQL unit tests, I end up with an unclean source dir:

{noformat}
$ git status
On branch SPARK-10088
Untracked files:
  (use "git add \<file\>..." to include in what will be committed)

        sql/hive/src/test/resources/golden/Column pruning - non-trivial top project with aliases - query test-0-515e406ffb23f6fd0d8cd34c2b25fbe6
        sql/hive/src/test/resources/golden/Partition pruning - non-partitioned, non-trivial project - query test-0-eabbebd5c1d127b1605bfec52d7b7f3f
{noformat}

If I understand correctly, the fix is just to add those files to the repo.


---

* [SPARK-10088](https://issues.apache.org/jira/browse/SPARK-10088) | *Minor* | **Support "stored as avro" HiveQL construct**

SparkSQL currently does not understand "stored as avro"; that should be pretty trivial to add, to increase compatibility with Hive.


---

* [SPARK-10087](https://issues.apache.org/jira/browse/SPARK-10087) | *Critical* | **Disable reducer locality in 1.5**

In some cases, when spark.shuffle.reduceLocality.enabled is enabled, we are scheduling all reducers to the same executor (the cluster has plenty of resources). Changing spark.shuffle.reduceLocality.enabled to false resolve the problem. 

Comments of https://github.com/apache/spark/pull/8280 provide more details of the symptom of this issue.

The query I was using is
{code:sql}
select
  i\_brand\_id,
  i\_brand,
  i\_manufact\_id,
  i\_manufact,
  sum(ss\_ext\_sales\_price) ext\_price
from
  store\_sales
  join item on (store\_sales.ss\_item\_sk = item.i\_item\_sk)
  join customer on (store\_sales.ss\_customer\_sk = customer.c\_customer\_sk)
  join customer\_address on (customer.c\_current\_addr\_sk = customer\_address.ca\_address\_sk)
  join store on (store\_sales.ss\_store\_sk = store.s\_store\_sk)
  join date\_dim on (store\_sales.ss\_sold\_date\_sk = date\_dim.d\_date\_sk)
where
  --ss\_date between '1999-11-01' and '1999-11-30'
  ss\_sold\_date\_sk between 2451484 and 2451513
  and d\_moy = 11
  and d\_year = 1999
  and i\_manager\_id = 7
  and substr(ca\_zip, 1, 5) \<\> substr(s\_zip, 1, 5)
group by
  i\_brand,
  i\_brand\_id,
  i\_manufact\_id,
  i\_manufact
order by
  ext\_price desc,
  i\_brand,
  i\_brand\_id,
  i\_manufact\_id,
  i\_manufact
limit 100
{code}
The dataset is tpc-ds scale factor 1500. To reproduce the problem, you can just join store\_sales with customer and make sure there is only one mapper reads the data of customer.


---

* [SPARK-10085](https://issues.apache.org/jira/browse/SPARK-10085) | *Minor* | **unnecessary array import in Python MLLib linear models**

Imports `from numpy import array` in the Python version of MLLib Linear Methods https://spark.apache.org/docs/1.4.1/mllib-linear-methods.html are apparently unused (and the code works after removing them). 

Should I remove them or do they actually do anything? (But if they do, it is a bad pattern.)


---

* [SPARK-10084](https://issues.apache.org/jira/browse/SPARK-10084) | *Minor* | **Add Python example for mllib FP-growth user guide**

Add Python example for mllib FP-growth user guide


---

* [SPARK-10083](https://issues.apache.org/jira/browse/SPARK-10083) | *Major* | **CaseWhen should support type coercion of DecimalType and FractionalType**

create t1 (a decimal(7, 2), b long);
select case when 1=1 then a else 1.0 end from t1;
select case when 1=1 then a else b end from t1;


---

* [SPARK-10080](https://issues.apache.org/jira/browse/SPARK-10080) | *Blocker* | **Binary Incompatibility in SQLContext implicits**

{code}
java.lang.NoSuchMethodError: org.apache.spark.sql.SQLContext$implicits$.StringToColumn(Lscala/StringContext;)Lorg/apache/spark/sql/SQLContext$implicits$StringToColumn;
	at com.databricks.spark.sparkprs.SparkPRs.prs(sparkprs.scala:129)
	at com.databricks.spark.sparkprs.SparkPRs.joinedData(sparkprs.scala:142)
	at com.databricks.spark.sparkprs.SparkPRs.sparkSqlWork(sparkprs.scala:145)
{code}


---

* [SPARK-10076](https://issues.apache.org/jira/browse/SPARK-10076) | *Major* | **make MultilayerPerceptronClassifier layers and weights public**

make MultilayerPerceptronClassifier layers and weights public


---

* [SPARK-10075](https://issues.apache.org/jira/browse/SPARK-10075) | *Major* | **Add `when` expressino function in SparkR**

Add {{when}} function into SparkR. Before this issue, we need to implement {{when}}, {{otherwise}} and so on as {{Column}} methods.


---

* [SPARK-10073](https://issues.apache.org/jira/browse/SPARK-10073) | *Blocker* | **Python withColumn for existing column name not consistent with scala**

The same code as below works in Scala (replacing the old column with the new one).

{code}
from pyspark.sql import Row
df = sc.parallelize([Row(a=1)]).toDF()
df.withColumn("a", df.a).select("a")
---------------------------------------------------------------------------
AnalysisException                         Traceback (most recent call last)
\<ipython-input-4-d5a4f4132506\> in \<module\>()
      1 from pyspark.sql import Row
      2 df = sc.parallelize([Row(a=1)]).toDF()
----\> 3 df.withColumn("a", df.a).select("a")

/home/ubuntu/databricks/spark/python/pyspark/sql/dataframe.py in select(self, \*cols)
    764         [Row(name=u'Alice', age=12), Row(name=u'Bob', age=15)]
    765         """
--\> 766         jdf = self.\_jdf.select(self.\_jcols(\*cols))
    767         return DataFrame(jdf, self.sql\_ctx)
    768 

/home/ubuntu/databricks/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java\_gateway.py in \_\_call\_\_(self, \*args)
    536         answer = self.gateway\_client.send\_command(command)
    537         return\_value = get\_return\_value(answer, self.gateway\_client,
--\> 538                 self.target\_id, self.name)
    539 
    540         for temp\_arg in temp\_args:

/home/ubuntu/databricks/spark/python/pyspark/sql/utils.py in deco(\*a, \*\*kw)
     38             s = e.java\_exception.toString()
     39             if s.startswith('org.apache.spark.sql.AnalysisException: '):
---\> 40                 raise AnalysisException(s.split(': ', 1)[1])
     41             if s.startswith('java.lang.IllegalArgumentException: '):
     42                 raise IllegalArgumentException(s.split(': ', 1)[1])

AnalysisException: Reference 'a' is ambiguous, could be: a#894L, a#895L.;
{code}


---

* [SPARK-10072](https://issues.apache.org/jira/browse/SPARK-10072) | *Blocker* | **BlockGenerator can deadlock when the queue block queue of generate blocks fills up to capacity**

Generated blocks are inserted into an ArrayBlockingQueue, and another thread pulls stuff from the ArrayBlockingQueue and pushes it into BlockManager. Now if that queue fills up to capacity (default is 10 blocks), then the inserting into queue (done in the function updateCurrentBuffer) get blocked inside a synchronized block. However, the thread that is pulling blocks from the queue uses the same lock to check the current (active or stopped) while pulling from the queue. Since the block generating threads is blocked (as the queue is full) on the lock, this thread that is supposed to drain the queue gets blocked. Ergo, deadlock.


---

* [SPARK-10070](https://issues.apache.org/jira/browse/SPARK-10070) | *Minor* | **Remove Guava dependencies in user guides**

Many code examples in documentation use {{Lists.newArrayList}} (e.g. [ml-feature\|https://github.com/apache/spark/blob/master/docs/ml-features.md]) which brings in a dependency on {{com.google.common.collect.Lists}}.

We can remove this dependency by using {{Arrays.asList}} instead.


---

* [SPARK-10068](https://issues.apache.org/jira/browse/SPARK-10068) | *Minor* | **Add links to sections in MLlib's user guide**

In {{mllib-guide.md}}, the listing under {{MLlib types, algorithms and utilities
}} is inconsistent with linking to sections referenced. We should provide links to every section mentioned in this listing.


---

* [SPARK-10060](https://issues.apache.org/jira/browse/SPARK-10060) | *Major* | **User guide for ml trees**

Create user guide section for spark.ml DecisionTree\*


---

* [SPARK-10059](https://issues.apache.org/jira/browse/SPARK-10059) | *Critical* | **Broken test: YarnClusterSuite**

This test failed everytime:  https://amplab.cs.berkeley.edu/jenkins/view/Spark-QA-Test/job/Spark-1.5-SBT/AMPLAB\_JENKINS\_BUILD\_PROFILE=hadoop2.3,label=spark-test/116/testReport/junit/org.apache.spark.deploy.yarn/YarnClusterSuite/\_It\_is\_not\_a\_test\_/history/

{code}
Error Message

java.io.IOException: ResourceManager failed to start. Final state is STOPPED
Stacktrace

sbt.ForkMain$ForkError: java.io.IOException: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:302)
	at org.apache.hadoop.yarn.server.MiniYARNCluster.access$500(MiniYARNCluster.java:87)
	at org.apache.hadoop.yarn.server.MiniYARNCluster$ResourceManagerWrapper.serviceStart(MiniYARNCluster.java:422)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.hadoop.service.CompositeService.serviceStart(CompositeService.java:121)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.spark.deploy.yarn.YarnClusterSuite.beforeAll(YarnClusterSuite.scala:104)
	at org.scalatest.BeforeAndAfterAll$class.beforeAll(BeforeAndAfterAll.scala:187)
	at org.apache.spark.deploy.yarn.YarnClusterSuite.beforeAll(YarnClusterSuite.scala:46)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:253)
	at org.apache.spark.deploy.yarn.YarnClusterSuite.run(YarnClusterSuite.scala:46)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: sbt.ForkMain$ForkError: ResourceManager failed to start. Final state is STOPPED
	at org.apache.hadoop.yarn.server.MiniYARNCluster.startResourceManager(MiniYARNCluster.java:297)
	... 18 more
{code}


---

* [SPARK-10038](https://issues.apache.org/jira/browse/SPARK-10038) | *Blocker* | **TungstenProject code generation fails when applied to array\<binary\>**

During fuzz testing, I discovered that TungstenProject can crash when applied to schemas that contain {{array\<binary\>}} columns.  As a minimal example, the following code crashes in spark-shell:

{code}
sc.parallelize(Seq((Array(Array[Byte](1)), 1))).toDF.select("\_1").rdd.count()
{code}

Here's the stacktrace:

{code}
15/08/16 17:11:49 ERROR Executor: Exception in task 3.0 in stage 29.0 (TID 144)
java.util.concurrent.ExecutionException: java.lang.Exception: failed to compile: org.codehaus.commons.compiler.CompileException: Line 53, Column 63: '{' expected instead of '['

public Object generate(org.apache.spark.sql.catalyst.expressions.Expression[] exprs) {
  return new SpecificUnsafeProjection(exprs);
}

class SpecificUnsafeProjection extends org.apache.spark.sql.catalyst.expressions.UnsafeProjection {

  private org.apache.spark.sql.catalyst.expressions.Expression[] expressions;

  private UnsafeRow convertedStruct2;
  private byte[] buffer3;
  private int cursor4;
  private UnsafeArrayData convertedArray6;
  private byte[] buffer7;


  public SpecificUnsafeProjection(org.apache.spark.sql.catalyst.expressions.Expression[] expressions) {
    this.expressions = expressions;
    this.convertedStruct2 = new UnsafeRow();
    this.buffer3 = new byte[16];
    this.cursor4 = 0;
    convertedArray6 = new UnsafeArrayData();
    buffer7 = new byte[64];
  }

  // Scala.Function1 need this
  public Object apply(Object row) {
    return apply((InternalRow) row);
  }

  public UnsafeRow apply(InternalRow i) {

    cursor4 = 16;
    convertedStruct2.pointTo(buffer3, Platform.BYTE\_ARRAY\_OFFSET, 1, cursor4);


    /\* input[0, ArrayType(BinaryType,true)] \*/

    boolean isNull0 = i.isNullAt(0);
    ArrayData primitive1 = isNull0 ? null : (i.getArray(0));

    final boolean isNull8 = isNull0;
    if (!isNull8) {
      final ArrayData tmp9 = primitive1;
      if (tmp9 instanceof UnsafeArrayData) {
        convertedArray6 = (UnsafeArrayData) tmp9;
      } else {
        final int numElements10 = tmp9.numElements();
        final int fixedSize11 = 4 \* numElements10;
        int numBytes12 = fixedSize11;


        final byte[][] elements13 = new byte[][numElements10];
        for (int index15 = 0; index15 \< numElements10; index15++) {

          if (!tmp9.isNullAt(index15)) {
            elements13[index15] = tmp9.getBinary(index15);
            numBytes12 += org.apache.spark.sql.catalyst.expressions.UnsafeWriters$BinaryWriter.getSize(elements13[index15]);
          }
        }


        if (numBytes12 \> buffer7.length) {
          buffer7 = new byte[numBytes12];
        }

        int cursor14 = fixedSize11;
        for (int index15 = 0; index15 \< numElements10; index15++) {
          if (elements13[index15] == null) {
            // If element is null, write the negative value address into offset region.
            Platform.putInt(buffer7, Platform.BYTE\_ARRAY\_OFFSET + 4 \* index15, -cursor14);
          } else {
            Platform.putInt(buffer7, Platform.BYTE\_ARRAY\_OFFSET + 4 \* index15, cursor14);

            cursor14 += org.apache.spark.sql.catalyst.expressions.UnsafeWriters$BinaryWriter.write(
              buffer7,
              Platform.BYTE\_ARRAY\_OFFSET + cursor14,
              elements13[index15]);

          }
        }

        convertedArray6.pointTo(
          buffer7,
          Platform.BYTE\_ARRAY\_OFFSET,
          numElements10,
          numBytes12);
      }
    }


    int numBytes16 = cursor4 + (isNull8 ? 0 : org.apache.spark.sql.catalyst.expressions.UnsafeRowWriters$ArrayWriter.getSize(convertedArray6));
    if (buffer3.length \< numBytes16) {
      // This will not happen frequently, because the buffer is re-used.
      byte[] tmpBuffer5 = new byte[numBytes16 \* 2];
      Platform.copyMemory(buffer3, Platform.BYTE\_ARRAY\_OFFSET,
        tmpBuffer5, Platform.BYTE\_ARRAY\_OFFSET, buffer3.length);
      buffer3 = tmpBuffer5;
    }
    convertedStruct2.pointTo(buffer3, Platform.BYTE\_ARRAY\_OFFSET, 1, numBytes16);


    if (isNull8) {
      convertedStruct2.setNullAt(0);
    } else {
      cursor4 += org.apache.spark.sql.catalyst.expressions.UnsafeRowWriters$ArrayWriter.write(convertedStruct2, 0, cursor4, convertedArray6);
    }



    return convertedStruct2;
  }
}


	at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
	at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
	at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
	at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
	at com.google.common.cache.LocalCache$LoadingValueReference.waitForValue(LocalCache.java:3620)
	at com.google.common.cache.LocalCache$Segment.waitForLoadingValue(LocalCache.java:2362)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2251)
	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:362)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:469)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateUnsafeProjection$.create(GenerateUnsafeProjection.scala:32)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:425)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:124)
	at org.apache.spark.sql.catalyst.expressions.UnsafeProjection$.create(Projection.scala:134)
	at org.apache.spark.sql.execution.TungstenProject$$anonfun$2.apply(basicOperators.scala:85)
	at org.apache.spark.sql.execution.TungstenProject$$anonfun$2.apply(basicOperators.scala:80)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

Here's the {{explain}} output:

{code}
scala\> sc.parallelize(Seq((Array(Array[Byte](1)), 1))).toDF.select("\_1").explain(true)
== Parsed Logical Plan ==
'Project [unresolvedalias('\_1)]
 LogicalRDD [\_1#161,\_2#162], MapPartitionsRDD[187] at rddToDataFrameHolder at \<console\>:22

== Analyzed Logical Plan ==
\_1: array\<binary\>
Project [\_1#161]
 LogicalRDD [\_1#161,\_2#162], MapPartitionsRDD[187] at rddToDataFrameHolder at \<console\>:22

== Optimized Logical Plan ==
Project [\_1#161]
 LogicalRDD [\_1#161,\_2#162], MapPartitionsRDD[187] at rddToDataFrameHolder at \<console\>:22

== Physical Plan ==
TungstenProject [\_1#161]
 Scan PhysicalRDD[\_1#161,\_2#162]
{code}


---

* [SPARK-10036](https://issues.apache.org/jira/browse/SPARK-10036) | *Major* | **DataFrameReader.json and DataFrameWriter.json don't load the JDBC driver class before creating JDBC connection**

Here is the reproduce code and the stack trace

{code}
val url = "jdbc:postgresql://.../mytest"
import java.util.Properties

val prop = new Properties()
prop.put("driver", "org.postgresql.Driver")
prop.put("user", "...")
prop.put("password", "...")

val df = sqlContext.read.jdbc(url, "mytest", prop)
{code}

{code}
java.sql.SQLException: No suitable driver found for jdbc:postgresql://.../mytest
	at java.sql.DriverManager.getConnection(DriverManager.java:689)
	at java.sql.DriverManager.getConnection(DriverManager.java:208)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRDD$.resolveTable(JDBCRDD.scala:121)
	at org.apache.spark.sql.execution.datasources.jdbc.JDBCRelation.\<init\>(JDBCRelation.scala:91)
	at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:200)
	at org.apache.spark.sql.DataFrameReader.jdbc(DataFrameReader.scala:130)
{code}


---

* [SPARK-10032](https://issues.apache.org/jira/browse/SPARK-10032) | *Minor* | **Add Python example for mllib LDAModel user guide**

Add Python example for mllib LDAModel user guide


---

* [SPARK-10029](https://issues.apache.org/jira/browse/SPARK-10029) | *Minor* | **Add Python examples for mllib IsotonicRegression user guide**

Add Python examples for mllib IsotonicRegression user guide


---

* [SPARK-10012](https://issues.apache.org/jira/browse/SPARK-10012) | *Trivial* | **Missing test case for Params#arrayLengthGt**

Currently there is no test case for {{Params#arrayLengthGt}}.


---

* [SPARK-10008](https://issues.apache.org/jira/browse/SPARK-10008) | *Major* | **Shuffle locality can take precedence over narrow dependencies for RDDs with both**

The shuffle locality patch made the DAGScheduler aware of shuffle data, but for RDDs that have both narrow and shuffle dependencies, it can cause them to place tasks based on the shuffle dependency instead of the narrow one. This case is common in iterative join-based algorithms like PageRank and ALS, where one RDD is hash-partitioned and one isn't.


---

* [SPARK-10007](https://issues.apache.org/jira/browse/SPARK-10007) | *Major* | **Update `NAMESPACE` file in SparkR for simple parameters functions**

I appreciate that I forgot to update {{NAMESPACE}} file for the simple parameters functions, such as {{ascii}}, {{base64}} and so on.


---

* [SPARK-10005](https://issues.apache.org/jira/browse/SPARK-10005) | *Blocker* | **Parquet reader doesn't handle schema merging properly for nested structs**

Spark shell snippet to reproduce this issue (note that both {{DataFrame}} written below contain a single struct column with multiple fields):
{code}
import sqlContext.implicits.\_

val path = "file:///tmp/foo"

(0 until 3).map(i =\> Tuple1((s"a\_$i", s"b\_$i"))).toDF().coalesce(1).write.mode("overwrite").parquet(path)
(0 until 3).map(i =\> Tuple1((s"a\_$i", s"b\_$i", s"c\_$i"))).toDF().coalesce(1).write.mode("append").parquet(path)

sqlContext.read.option("schemaMerging", "true").parquet(path).show()
{code}
Exception:
{noformat}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 39.0 failed 1 times, most recent failure: Lost task 0.0 in stage 39.0 (TID 122, localhost): org.apache.parquet.io.ParquetDecodingException: Can not read value at 0 in block -1 in file file:/tmp/foo/part-r-00000-ba9dc7cf-3210-4006-9cf7-02c3d57483cd.gz.parquet
        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:228)
        at org.apache.parquet.hadoop.ParquetRecordReader.nextKeyValue(ParquetRecordReader.java:201)
        at org.apache.spark.rdd.SqlNewHadoopRDD$$anon$1.hasNext(SqlNewHadoopRDD.scala:168)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:308)
        at scala.collection.Iterator$class.foreach(Iterator.scala:727)
        at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
        at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
        at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
        at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
        at scala.collection.AbstractIterator.to(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
        at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
        at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
        at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(SparkPlan.scala:215)
        at org.apache.spark.sql.execution.SparkPlan$$anonfun$5.apply(SparkPlan.scala:215)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1826)
        at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1826)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
        at org.apache.spark.scheduler.Task.run(Task.scala:88)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 2
        at org.apache.spark.sql.execution.datasources.parquet.CatalystRowConverter.getConverter(CatalystRowConverter.scala:136)
        at org.apache.parquet.io.RecordReaderImplementation.\<init\>(RecordReaderImplementation.java:269)
        at org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:134)
        at org.apache.parquet.io.MessageColumnIO$1.visit(MessageColumnIO.java:99)
        at org.apache.parquet.filter2.compat.FilterCompat$NoOpFilter.accept(FilterCompat.java:154)
        at org.apache.parquet.io.MessageColumnIO.getRecordReader(MessageColumnIO.java:99)
        at org.apache.parquet.hadoop.InternalParquetRecordReader.checkRead(InternalParquetRecordReader.java:137)
        at org.apache.parquet.hadoop.InternalParquetRecordReader.nextKeyValue(InternalParquetRecordReader.java:208)
        ... 25 more
{noformat}


---

* [SPARK-9982](https://issues.apache.org/jira/browse/SPARK-9982) | *Major* | **SparkR DataFrame fail to return data of Decimal type**

Got an issue similar to https://issues.apache.org/jira/browse/SPARK-8897, but with the Decimal datatype coming from a Postgres DB:

//Set up SparkR

\>Sys.setenv(SPARK\_HOME="/Users/ashkurenko/work/git\_repos/spark")
\>Sys.setenv(SPARKR\_SUBMIT\_ARGS="--driver-class-path ~/Downloads/postgresql-9.4-1201.jdbc4.jar sparkr-shell")
\>.libPaths(c(file.path(Sys.getenv("SPARK\_HOME"), "R", "lib"), .libPaths()))
\>library(SparkR)
\>sc \<- sparkR.init(master="local")

// Connect to a Postgres DB via JDBC
\>sqlContext \<- sparkRSQL.init(sc)
\>sql(sqlContext, "
    CREATE TEMPORARY TABLE mytable 
    USING org.apache.spark.sql.jdbc 
    OPTIONS (url 'jdbc:postgresql://servername:5432/dbname'
    ,dbtable 'mydbtable'
)
")

// Try pulling a Decimal column from a table
\>myDataFrame \<- sql(sqlContext,("select a\_decimal\_column  from mytable "))

// The schema shows up fine

\>show(myDataFrame)

DataFrame[a\_decimal\_column:decimal(10,0)]

\>schema(myDataFrame)

StructType
\|-name = "a\_decimal\_column", type = "DecimalType(10,0)", nullable = TRUE

// ... but pulling data fails:

localDF \<- collect(myDataFrame)

Error in as.data.frame.default(x[[i]], optional = TRUE) : 
  cannot coerce class ""jobj"" to a data.frame


---

* [SPARK-9981](https://issues.apache.org/jira/browse/SPARK-9981) | *Major* | **Make labels public in StringIndexerModel**

Necessary for creating inverse (IndexToString)


---

* [SPARK-9980](https://issues.apache.org/jira/browse/SPARK-9980) | *Trivial* | **SBT publishLocal error due to invalid characters in doc**

When issuing the following command in SBT:
{noformat}
\> unsafe/publishLocal
{noformat}

It fails with the following error:
{noformat}
[info] Generating /media/hvanhovell/Data/QT/IT/Software/spark/unsafe/target/scala-2.10/api/org/apache/spark/unsafe/memory/TaskMemoryManager.html...
[error] /media/hvanhovell/Data/QT/IT/Software/spark/unsafe/src/main/java/org/apache/spark/unsafe/memory/TaskMemoryManager.java:63: error: malformed HTML
[error]    \* (1L \<\< OFFSET\_BITS) bytes, which is 2+ petabytes. However, the on-heap allocator's maximum page
[error]          ^
[error] /media/hvanhovell/Data/QT/IT/Software/spark/unsafe/src/main/java/org/apache/spark/unsafe/memory/TaskMemoryManager.java:63: error: malformed HTML
[error]    \* (1L \<\< OFFSET\_BITS) bytes, which is 2+ petabytes. However, the on-heap allocator's maximum page
[error]   
{noformat}
It is a tiny modification of the documentation to fix this. I'll submit a PR.


---

* [SPARK-9978](https://issues.apache.org/jira/browse/SPARK-9978) | *Major* | **Window functions require partitionBy to work as expected**

I am trying to reproduce following SQL query:

{code}
df.registerTempTable("df")
sqlContext.sql("SELECT x, row\_number() OVER (ORDER BY x) as rn FROM df").show()

+----+--+
\|   x\|rn\|
+----+--+
\|0.25\| 1\|
\| 0.5\| 2\|
\|0.75\| 3\|
+----+--+
{code}

using PySpark API. Unfortunately it doesn't work as expected:

{code}
from pyspark.sql.window import Window
from pyspark.sql.functions import rowNumber

df = sqlContext.createDataFrame([{"x": 0.25}, {"x": 0.5}, {"x": 0.75}])
df.select(df["x"], rowNumber().over(Window.orderBy("x")).alias("rn")).show()

+----+--+
\|   x\|rn\|
+----+--+
\| 0.5\| 1\|
\|0.25\| 1\|
\|0.75\| 1\|
+----+--+
{code}

As a workaround It is possible to call partitionBy without additional arguments:

{code}
df.select(
    df["x"],
    rowNumber().over(Window.partitionBy().orderBy("x")).alias("rn")
).show()

+----+--+
\|   x\|rn\|
+----+--+
\|0.25\| 1\|
\| 0.5\| 2\|
\|0.75\| 3\|
+----+--+
{code}

but as far as I can tell it is not documented and is rather counterintuitive considering SQL syntax. Moreover this problem doesn't affect Scala API:

{code}
import org.apache.spark.sql.expressions.Window
import org.apache.spark.sql.functions.rowNumber

case class Record(x: Double)
val df = sqlContext.createDataFrame(Record(0.25) :: Record(0.5) :: Record(0.75))
df.select($"x", rowNumber().over(Window.orderBy($"x")).alias("rn")).show

+----+--+
\|   x\|rn\|
+----+--+
\|0.25\| 1\|
\| 0.5\| 2\|
\|0.75\| 3\|
+----+--+
{code}


---

* [SPARK-9977](https://issues.apache.org/jira/browse/SPARK-9977) | *Trivial* | **The usage of a label generated by StringIndexer**

By using {{StringIndexer}}, we can obtain indexed label on new column. So a following estimator should use this new column through pipeline if it wants to use string indexed label. 
I think it is better to make it explicit on documentation.


---

* [SPARK-9974](https://issues.apache.org/jira/browse/SPARK-9974) | *Blocker* | **SBT build: com.twitter:parquet-hadoop-bundle:1.6.0 is not packaged into the assembly jar**

One of the consequence of this issue is that Parquet tables created in Hive are not accessible from Spark SQL built with SBT. Maven build is OK. This issue can be worked around by adding {{lib\_managed/jars/parquet-hadoop-bundle-1.6.0.jar}} to {{--driver-class-path}}.

Git commit: [69930310115501f0de094fe6f5c6c60dade342bd\|https://github.com/apache/spark/commit/69930310115501f0de094fe6f5c6c60dade342bd]

Build with SBT and check the assembly jar for classes in package {{parquet.hadoop.api}}:
{noformat}
$ ./build/sbt -Phive -Phive-thriftserver -Phadoop-1 -Dhadoop.version=1.2.1 clean assembly/assembly
...
$ jar tf assembly/target/scala-2.10/spark-assembly-1.5.0-SNAPSHOT-hadoop1.2.1.jar \| fgrep "parquet/hadoop/api"
org/apache/parquet/hadoop/api/
org/apache/parquet/hadoop/api/DelegatingReadSupport.class
org/apache/parquet/hadoop/api/DelegatingWriteSupport.class
org/apache/parquet/hadoop/api/InitContext.class
org/apache/parquet/hadoop/api/ReadSupport$ReadContext.class
org/apache/parquet/hadoop/api/ReadSupport.class
org/apache/parquet/hadoop/api/WriteSupport$FinalizedWriteContext.class
org/apache/parquet/hadoop/api/WriteSupport$WriteContext.class
org/apache/parquet/hadoop/api/WriteSupport.class
{noformat}
Only classes of {{org.apache.parquet:parquet-mr:1.7.0}}. Note that classes in {{com.twitter:parquet-hadoop-bundle:1.6.0}} are not under the {{org.apache}} namespace.

Build with Maven and check the assembly jar for classes in package {{parquet.hadoop.api}}:
{noformat}
$ ./build/mvn -Phive -Phive-thriftserver -Phadoop-1 -Dhadoop.version=1.2.1 -DskipTests clean package
...
$ jar tf assembly/target/scala-2.10/spark-assembly-1.5.0-SNAPSHOT-hadoop1.2.1.jar \| fgrep "parquet/hadoop/api"
org/apache/parquet/hadoop/api/
org/apache/parquet/hadoop/api/DelegatingReadSupport.class
org/apache/parquet/hadoop/api/DelegatingWriteSupport.class
org/apache/parquet/hadoop/api/InitContext.class
org/apache/parquet/hadoop/api/ReadSupport$ReadContext.class
org/apache/parquet/hadoop/api/ReadSupport.class
org/apache/parquet/hadoop/api/WriteSupport$FinalizedWriteContext.class
org/apache/parquet/hadoop/api/WriteSupport$WriteContext.class
org/apache/parquet/hadoop/api/WriteSupport.class
parquet/hadoop/api/
parquet/hadoop/api/DelegatingReadSupport.class
parquet/hadoop/api/DelegatingWriteSupport.class
parquet/hadoop/api/InitContext.class
parquet/hadoop/api/ReadSupport$ReadContext.class
parquet/hadoop/api/ReadSupport.class
parquet/hadoop/api/WriteSupport$FinalizedWriteContext.class
parquet/hadoop/api/WriteSupport$WriteContext.class
parquet/hadoop/api/WriteSupport.class
{noformat}
Expected classes are packaged properly.

To reproduce the Parquet table access issue, first create a Parquet table with Hive (say 0.13.1):
{noformat}
hive\> CREATE TABLE parquet\_test STORED AS PARQUET AS SELECT 1;
{noformat}
Build Spark assembly jar with the SBT command above, start {{spark-shell}}:
{noformat}
scala\> sqlContext.table("parquet\_test").show()
15/08/14 17:52:50 INFO HiveMetaStore: 0: get\_table : db=default tbl=parquet\_test
15/08/14 17:52:50 INFO audit: ugi=lian  ip=unknown-ip-addr      cmd=get\_table : db=default tbl=parquet\_test
java.lang.NoClassDefFoundError: parquet/hadoop/api/WriteSupport
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:270)
        at org.apache.hadoop.hive.ql.metadata.Table.getOutputFormatClass(Table.java:328)
        at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$getTableOption$1$$anonfun$2.apply(ClientWrapper.scala:320)
        at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$getTableOption$1$$anonfun$2.apply(ClientWrapper.scala:303)
        at scala.Option.map(Option.scala:145)
        at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$getTableOption$1.apply(ClientWrapper.scala:303)
        at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$getTableOption$1.apply(ClientWrapper.scala:298)
        at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:256)
        at org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:211)
        at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:248)
        at org.apache.spark.sql.hive.client.ClientWrapper.getTableOption(ClientWrapper.scala:298)
        at org.apache.spark.sql.hive.client.ClientInterface$class.getTable(ClientInterface.scala:123)
        at org.apache.spark.sql.hive.client.ClientWrapper.getTable(ClientWrapper.scala:60)
        at org.apache.spark.sql.hive.HiveMetastoreCatalog.lookupRelation(HiveMetastoreCatalog.scala:397)
        at org.apache.spark.sql.hive.HiveContext$$anon$1.org$apache$spark$sql$catalyst$analysis$OverrideCatalog$$super$lookupRelation(HiveContext.scala:403)
        at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:170)
        at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$$anonfun$lookupRelation$3.apply(Catalog.scala:170)
        at scala.Option.getOrElse(Option.scala:120)
        at org.apache.spark.sql.catalyst.analysis.OverrideCatalog$class.lookupRelation(Catalog.scala:170)
        at org.apache.spark.sql.hive.HiveContext$$anon$1.lookupRelation(HiveContext.scala:403)
        at org.apache.spark.sql.SQLContext.table(SQLContext.scala:806)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\<init\>(\<console\>:20)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\<init\>(\<console\>:25)
        at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\<init\>(\<console\>:27)
        at $iwC$$iwC$$iwC$$iwC$$iwC.\<init\>(\<console\>:29)
        at $iwC$$iwC$$iwC$$iwC.\<init\>(\<console\>:31)
        at $iwC$$iwC$$iwC.\<init\>(\<console\>:33)
        at $iwC$$iwC.\<init\>(\<console\>:35)
        at $iwC.\<init\>(\<console\>:37)
        at \<init\>(\<console\>:39)
        at .\<init\>(\<console\>:43)
        at .\<clinit\>(\<console\>)
        at .\<init\>(\<console\>:7)
        at .\<clinit\>(\<console\>)
        at $print(\<console\>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
        at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1340)
        at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
        at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
        at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
        at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
        at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)
        at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)
        at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
        at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
        at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
        at org.apache.spark.repl.Main$.main(Main.scala:31)
        at org.apache.spark.repl.Main.main(Main.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:205)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassNotFoundException: parquet.hadoop.api.WriteSupport
        at scala.tools.nsc.interpreter.AbstractFileClassLoader.findClass(AbstractFileClassLoader.scala:83)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.doLoadClass(IsolatedClientLoader.scala:169)
        at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1.loadClass(IsolatedClientLoader.scala:153)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 68 more
{noformat}


---

* [SPARK-9973](https://issues.apache.org/jira/browse/SPARK-9973) | *Major* | **Wrong initial size of in-memory columnar buffers**

Two much memory is allocated for in-memory columnar buffers. The {{initialSize}} argument in {{ColumnBuilder.initialize}} is the initial number of rows rather than bytes, but the value passed in in {{InMemoryColumnarTableScan}} is the latter:
{code}
// Class InMemoryColumnarTableScan
  val initialBufferSize = columnType.defaultSize \* batchSize
  ColumnBuilder(attribute.dataType, initialBufferSize, attribute.name, useCompression)
{code}
Then it's converted to byte size again by multiplying {{columnType.defaultSize}}:
{code}
// Class BasicColumnBuilder
  buffer = ByteBuffer.allocate(4 + size \* columnType.defaultSize)
{code}


---

* [SPARK-9968](https://issues.apache.org/jira/browse/SPARK-9968) | *Major* | **BlockGenerator lock structure can cause lock starvation of the block updating thread**

When the rate limiter is actually limiting the rate at which data is inserted into the buffer, the synchronized block of BlockGenerator.addData stays blocked for long time. This causes the thread switching the buffer and generating blocks (synchronized with addData) to starve and not generate blocks for seconds. The correct solution is to not block on the rate limiter within the synchronized block for adding data to the buffer.


---

* [SPARK-9967](https://issues.apache.org/jira/browse/SPARK-9967) | *Minor* | **Rename the SparkConf property to spark.streaming.backpressure.{enable --\> enabled}**

... to better align with most other spark parameters having "enable" in them...


---

* [SPARK-9966](https://issues.apache.org/jira/browse/SPARK-9966) | *Blocker* | **Handle a couple of corner cases in the PID rate estimator**

1. The rate estimator should not estimate any rate when there are no records in the batch, as there is no data to estimate the rate. In the current state, it estimates and set the rate to zero. That is incorrect.

2. The rate estimator should not never set the rate to zero under any circumstances. Otherwise the system will stop receiving data, and stop generating useful estimates (see reason 1). So the fix is to define a parameters that sets a lower bound on the estimated rate, so that the system always receives some data.


---

* [SPARK-9959](https://issues.apache.org/jira/browse/SPARK-9959) | *Critical* | **Association Rules needs JavaItems**

We need a java-friendly way to extract `List[Item]` out of `AssociationRule.Rule`.


---

* [SPARK-9958](https://issues.apache.org/jira/browse/SPARK-9958) | *Major* | **HiveThriftServer2Listener is not thread-safe**

HiveThriftServer2Listener should be thread-safe since it's accessed by multiple threads.


---

* [SPARK-9956](https://issues.apache.org/jira/browse/SPARK-9956) | *Major* | **Spark ML trees and ensembles fail for categorical features with 1 category**

Spark ML trees and ensembles can be given metadata (e.g., from VectorIndexer) indicating that a certain feature is categorical with a single possible value.  This causes learning to fail.

Proposal: For now, fix this by making sure the algorithm still runs with such a feature, and remove the checks (which currently cause failure).  In the future, we can filter out these features to improve performance when there are many useless features.


---

* [SPARK-9955](https://issues.apache.org/jira/browse/SPARK-9955) | *Critical* | **TPCDS Q8 failed in 1.5**

{code}
-- start query 1 in stream 0 using template query8.tpl
select
  s\_store\_name,
  sum(ss\_net\_profit)
from
  store\_sales
  join store on (store\_sales.ss\_store\_sk = store.s\_store\_sk)
  -- join date\_dim on (store\_sales.ss\_sold\_date\_sk = date\_dim.d\_date\_sk)
  join
  (select
    a.ca\_zip
  from
    (select
      substr(ca\_zip, 1, 5) ca\_zip,
      count( \*) cnt
    from
      customer\_address
      join customer on (customer\_address.ca\_address\_sk = customer.c\_current\_addr\_sk)
    where
      c\_preferred\_cust\_flag = 'Y'
    group by
      ca\_zip
    having
      count( \*) \> 10
    ) a
    left semi join
    (select
      substr(ca\_zip, 1, 5) ca\_zip
    from
      customer\_address
    where
      substr(ca\_zip, 1, 5) in ('89436', '30868', '65085', '22977', '83927', '77557', '58429', '40697', '80614', '10502', '32779',
      '91137', '61265', '98294', '17921', '18427', '21203', '59362', '87291', '84093', '21505', '17184', '10866', '67898', '25797',
      '28055', '18377', '80332', '74535', '21757', '29742', '90885', '29898', '17819', '40811', '25990', '47513', '89531', '91068',
      '10391', '18846', '99223', '82637', '41368', '83658', '86199', '81625', '26696', '89338', '88425', '32200', '81427', '19053',
      '77471', '36610', '99823', '43276', '41249', '48584', '83550', '82276', '18842', '78890', '14090', '38123', '40936', '34425',
      '19850', '43286', '80072', '79188', '54191', '11395', '50497', '84861', '90733', '21068', '57666', '37119', '25004', '57835',
      '70067', '62878', '95806', '19303', '18840', '19124', '29785', '16737', '16022', '49613', '89977', '68310', '60069', '98360',
      '48649', '39050', '41793', '25002', '27413', '39736', '47208', '16515', '94808', '57648', '15009', '80015', '42961', '63982',
      '21744', '71853', '81087', '67468', '34175', '64008', '20261', '11201', '51799', '48043', '45645', '61163', '48375', '36447',
      '57042', '21218', '41100', '89951', '22745', '35851', '83326', '61125', '78298', '80752', '49858', '52940', '96976', '63792',
      '11376', '53582', '18717', '90226', '50530', '94203', '99447', '27670', '96577', '57856', '56372', '16165', '23427', '54561',
      '28806', '44439', '22926', '30123', '61451', '92397', '56979', '92309', '70873', '13355', '21801', '46346', '37562', '56458',
      '28286', '47306', '99555', '69399', '26234', '47546', '49661', '88601', '35943', '39936', '25632', '24611', '44166', '56648',
      '30379', '59785', '11110', '14329', '93815', '52226', '71381', '13842', '25612', '63294', '14664', '21077', '82626', '18799',
      '60915', '81020', '56447', '76619', '11433', '13414', '42548', '92713', '70467', '30884', '47484', '16072', '38936', '13036',
      '88376', '45539', '35901', '19506', '65690', '73957', '71850', '49231', '14276', '20005', '18384', '76615', '11635', '38177',
      '55607', '41369', '95447', '58581', '58149', '91946', '33790', '76232', '75692', '95464', '22246', '51061', '56692', '53121',
      '77209', '15482', '10688', '14868', '45907', '73520', '72666', '25734', '17959', '24677', '66446', '94627', '53535', '15560',
      '41967', '69297', '11929', '59403', '33283', '52232', '57350', '43933', '40921', '36635', '10827', '71286', '19736', '80619',
      '25251', '95042', '15526', '36496', '55854', '49124', '81980', '35375', '49157', '63512', '28944', '14946', '36503', '54010',
      '18767', '23969', '43905', '66979', '33113', '21286', '58471', '59080', '13395', '79144', '70373', '67031', '38360', '26705',
      '50906', '52406', '26066', '73146', '15884', '31897', '30045', '61068', '45550', '92454', '13376', '14354', '19770', '22928',
      '97790', '50723', '46081', '30202', '14410', '20223', '88500', '67298', '13261', '14172', '81410', '93578', '83583', '46047',
      '94167', '82564', '21156', '15799', '86709', '37931', '74703', '83103', '23054', '70470', '72008', '49247', '91911', '69998',
      '20961', '70070', '63197', '54853', '88191', '91830', '49521', '19454', '81450', '89091', '62378', '25683', '61869', '51744',
      '36580', '85778', '36871', '48121', '28810', '83712', '45486', '67393', '26935', '42393', '20132', '55349', '86057', '21309',
      '80218', '10094', '11357', '48819', '39734', '40758', '30432', '21204', '29467', '30214', '61024', '55307', '74621', '11622',
      '68908', '33032', '52868', '99194', '99900', '84936', '69036', '99149', '45013', '32895', '59004', '32322', '14933', '32936',
      '33562', '72550', '27385', '58049', '58200', '16808', '21360', '32961', '18586', '79307', '15492')
    ) b
  on (a.ca\_zip = b.ca\_zip)
  ) v1 on (substr(store.s\_zip, 1, 2) = substr(v1.ca\_zip, 1, 2))
where
  ss\_date between '2002-01-01' and '2002-04-01'
  -- and d\_qoy = 1
  -- and d\_year = 2002
group by
  s\_store\_name
order by
  s\_store\_name
limit 100
-- end query 1 in stream 0 using template query8.tpl
{code}

{code}
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to toAttribute on unresolved object, tree: unresolvedalias('s\_store\_name)
	at org.apache.spark.sql.catalyst.analysis.UnresolvedAlias.toAttribute(unresolved.scala:257)
	at org.apache.spark.sql.catalyst.plans.logical.Aggregate$$anonfun$output$6.apply(basicOperators.scala:220)
	at org.apache.spark.sql.catalyst.plans.logical.Aggregate$$anonfun$output$6.apply(basicOperators.scala:220)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.plans.logical.Aggregate.output(basicOperators.scala:220)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$PullOutNondeterministic$$anonfun$apply$18.applyOrElse(Analyzer.scala:940)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$PullOutNondeterministic$$anonfun$apply$18.applyOrElse(Analyzer.scala:933)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$resolveOperators$1.apply(LogicalPlan.scala:57)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:56)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan$$anonfun$1.apply(LogicalPlan.scala:54)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:249)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildren(TreeNode.scala:279)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperators(LogicalPlan.scala:54)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$PullOutNondeterministic$.apply(Analyzer.scala:933)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$PullOutNondeterministic$.apply(Analyzer.scala:932)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:83)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:80)
	at scala.collection.IndexedSeqOptimized$class.foldl(IndexedSeqOptimized.scala:51)
	at scala.collection.IndexedSeqOptimized$class.foldLeft(IndexedSeqOptimized.scala:60)
	at scala.collection.mutable.WrappedArray.foldLeft(WrappedArray.scala:34)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:72)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:72)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:983)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:983)
	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:981)
	at org.apache.spark.sql.DataFrame.\<init\>(DataFrame.scala:132)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:795)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-9952](https://issues.apache.org/jira/browse/SPARK-9952) | *Critical* | **Fix N^2 loop when DAGScheduler.getPreferredLocsInternal accesses cacheLocs**

In Scala, Seq.fill always returns a List. Accessing a list by index is an O(N) operation. Thus, the following code will be really slow (~10 seconds on my machine):

{code}
val numItems = 100000
val s = Seq.fill(numItems)(1)
for (i \<- 0 until numItems) s(i)
{code}

It turns out that we had a loop like this in DAGScheduler code. In getPreferredLocsInternal, there's a call to {{getCacheLocs(rdd)(partition)}}.  The {{getCacheLocs}} call returns a Seq. If this Seq is a List and the RDD contains many partitions, then indexing into this list will cost O(partitions). Thus, when we loop over our tasks to compute their individual preferred locations we implicitly perform an N^2 loop, reducing scheduling throughput.

We can easily fix this by replacing Seq with Array in this code.


---

* [SPARK-9950](https://issues.apache.org/jira/browse/SPARK-9950) | *Blocker* | **Wrong Analysis Error for grouping/aggregating on struct fields**

Spark 1.4:
{code}
import org.apache.spark.sql.functions.\_
val df = Seq(("x", (1,1)), ("y", (2, 2))).toDF("a", "b")
df.groupBy("b.\_1").agg(sum("b.\_2")).collect()

df: org.apache.spark.sql.DataFrame = [a: string, b: struct\<\_1:int,\_2:int\>]
res0: Array[org.apache.spark.sql.Row] = Array([1,1], [2,2])
{code}

Spark 1.5
{code}
org.apache.spark.sql.AnalysisException: expression 'b' is neither present in the group by, nor is it an aggregate function. Add to group by or wrap in first() if you don't care which value you get.;
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:37)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:44)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.org$apache$spark$sql$catalyst$analysis$CheckAnalysis$class$$anonfun$$checkValidAggregateExpression$1(CheckAnalysis.scala:110)
{code}


---

* [SPARK-9949](https://issues.apache.org/jira/browse/SPARK-9949) | *Blocker* | **TakeOrderedAndProject returns wrong output attributes when project is pushed in to it**

In {{TakeOrderedAndProject}}, we have
{code}
case class TakeOrderedAndProject(
    limit: Int,
    sortOrder: Seq[SortOrder],
    projectList: Option[Seq[NamedExpression]],
    child: SparkPlan) extends UnaryNode {

  override def output: Seq[Attribute] = child.output
{code}

When projectList is set, we should use its attributes as the output.


---

* [SPARK-9948](https://issues.apache.org/jira/browse/SPARK-9948) | *Critical* | **Flaky test: o.a.s.AccumulatorSuite - internal accumulators**

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB\_JENKINS\_BUILD\_PROFILE=hadoop2.0,label=centos/3193
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB\_JENKINS\_BUILD\_PROFILE=hadoop1.0,label=centos/3201
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB\_JENKINS\_BUILD\_PROFILE=hadoop2.2,label=centos/3226

... many others


---

* [SPARK-9946](https://issues.apache.org/jira/browse/SPARK-9946) | *Blocker* | **NPE in TaskMemoryManager**

{code}
Failed to execute query using catalyst:
[info]   Error: Job aborted due to stage failure: Task 6 in stage 6801.0 failed 1 times, most recent failure: Lost task 6.0 in stage 6801.0 (TID 41123, localhost): java.lang.NullPointerException
[info]   	at org.apache.spark.unsafe.memory.TaskMemoryManager.getPage(TaskMemoryManager.java:226)
[info]   	at org.apache.spark.util.collection.unsafe.sort.UnsafeInMemorySorter$SortedIterator.loadNext(UnsafeInMemorySorter.java:165)
[info]   	at org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.next(UnsafeExternalRowSorter.java:142)
[info]   	at org.apache.spark.sql.execution.UnsafeExternalRowSorter$1.next(UnsafeExternalRowSorter.java:129)
[info]   	at org.apache.spark.sql.execution.RowIteratorFromScala.advanceNext(RowIterator.scala:84)
[info]   	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.advancedBufferedToRowWithNullFreeJoinKey(SortMergeJoin.scala:302)
[info]   	at org.apache.spark.sql.execution.joins.SortMergeJoinScanner.findNextInnerJoinRows(SortMergeJoin.scala:218)
[info]   	at org.apache.spark.sql.execution.joins.SortMergeJoin$$anonfun$doExecute$1$$anon$1.advanceNext(SortMergeJoin.scala:110)
[info]   	at org.apache.spark.sql.execution.RowIteratorToScala.hasNext(RowIterator.scala:68)
[info]   	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
[info]   	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
[info]   	at org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.insertAll(BypassMergeSortShuffleWriter.java:99)
[info]   	at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:73)
[info]   	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
[info]   	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
[info]   	at org.apache.spark.scheduler.Task.run(Task.scala:88)
[info]   	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
13:27:17.435 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 23.0 in stage 6801.0 (TID 41140, localhost): TaskKilled (killed intentionally)
[info]   	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]   	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
13:27:17.436 WARN org.apache.spark.scheduler.TaskSetManager: Lost task 12.0 in stage 6801.0 (TID 41129, localhost): TaskKilled (killed intentionally)
[info]   	at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-9945](https://issues.apache.org/jira/browse/SPARK-9945) | *Critical* | **pageSize is calculated from driver.memory instead of executor.memory**

If the driver.memory is larger than executor.memory, then it's easy to run out of memory in executor (tasks can start, because of preserved memory pages)


---

* [SPARK-9943](https://issues.apache.org/jira/browse/SPARK-9943) | *Critical* | **Failed to serialize a deserialized UnsafeHashedRelation**

When the free memory is executor go low, the cached broadcast object need to serialized into disk, a deserialized UnsafeHashedRelation can't be serialized  , fail with NPE

{code}
15/08/13 11:13:35 WARN TaskSetManager: Lost task 1.0 in stage 26.0 (TID 41, 192.168.1.236): java.io.IOException: java.lang.NullPointerException
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1163)
	at org.apache.spark.sql.execution.joins.UnsafeHashedRelation.writeExternal(HashedRelation.scala:249)
	at java.io.ObjectOutputStream.writeExternalData(ObjectOutputStream.java:1458)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1429)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.serializer.SerializationStream.writeAll(Serializer.scala:153)
	at org.apache.spark.storage.BlockManager.dataSerializeStream(BlockManager.scala:1190)
	at org.apache.spark.storage.DiskStore$$anonfun$putIterator$1.apply$mcV$sp(DiskStore.scala:81)
	at org.apache.spark.storage.DiskStore$$anonfun$putIterator$1.apply(DiskStore.scala:81)
	at org.apache.spark.storage.DiskStore$$anonfun$putIterator$1.apply(DiskStore.scala:81)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1206)
	at org.apache.spark.storage.DiskStore.putIterator(DiskStore.scala:82)
	at org.apache.spark.storage.DiskStore.putArray(DiskStore.scala:66)
	at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1041)
	at org.apache.spark.storage.BlockManager.dropFromMemory(BlockManager.scala:1002)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:468)
	at org.apache.spark.storage.MemoryStore$$anonfun$ensureFreeSpace$4.apply(MemoryStore.scala:457)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.storage.MemoryStore.ensureFreeSpace(MemoryStore.scala:457)
	at org.apache.spark.storage.MemoryStore.unrollSafely(MemoryStore.scala:292)
	at org.apache.spark.storage.MemoryStore.putIterator(MemoryStore.scala:165)
	at org.apache.spark.storage.MemoryStore.putIterator(MemoryStore.scala:143)
	at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:791)
	at org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:638)
	at org.apache.spark.storage.BlockManager.putSingle(BlockManager.scala:996)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:182)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1175)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)
	at org.apache.spark.broadcast.TorrentBroadcast.\_value$lzycompute(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.\_value(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$2.apply(BroadcastHashJoin.scala:113)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$2.apply(BroadcastHashJoin.scala:112)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsWithPreparationRDD.compute(MapPartitionsWithPreparationRDD.scala:46)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.NullPointerException
	at org.apache.spark.sql.execution.joins.UnsafeHashedRelation$$anonfun$writeExternal$1.apply$mcV$sp(HashedRelation.scala:250)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1160)
	... 57 more

{code}


---

* [SPARK-9942](https://issues.apache.org/jira/browse/SPARK-9942) | *Blocker* | **Broken pandas could crash PySpark SQL**

{code}
Error from python worker:
  Traceback (most recent call last):
    File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py", line 151, in \_run\_module\_as\_main
      mod\_name, loader, code, fname = \_get\_module\_details(mod\_name)
    File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/runpy.py", line 101, in \_get\_module\_details
      loader = get\_loader(mod\_name)
    File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pkgutil.py", line 464, in get\_loader
      return find\_loader(fullname)
    File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pkgutil.py", line 474, in find\_loader
      for importer in iter\_importers(fullname):
    File "/System/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pkgutil.py", line 430, in iter\_importers
      \_\_import\_\_(pkg)
    File "/Users/davies/work/spark/python/pyspark/\_\_init\_\_.py", line 52, in \<module\>
      from pyspark.sql import SQLContext, HiveContext, SchemaRDD, Row
    File "/Users/davies/work/spark/python/pyspark/sql/\_\_init\_\_.py", line 63, in \<module\>
      from pyspark.sql.context import SQLContext, HiveContext
    File "/Users/davies/work/spark/python/pyspark/sql/context.py", line 40, in \<module\>
      import pandas
    File "/Library/Python/2.7/site-packages/pandas/\_\_init\_\_.py", line 7, in \<module\>
      from pandas import hashtable, tslib, lib
    File "pandas/src/numpy.pxd", line 157, in init pandas.hashtable (pandas/hashtable.c:23654)
  ValueError: numpy.dtype has the wrong size, try recompiling
PYTHONPATH was:
  /Users/davies/work/spark/python/lib/pyspark.zip:/Users/davies/work/spark/python/lib/py4j-0.8.2.1-src.zip:/Users/davies/work/spark/sbin/../python/lib/py4j-0.8.2.1-src.zip:/Users/davies/work/spark/sbin/../python:/Users/davies/work/spark/sbin/../python/lib/py4j-0.8.2.1-src.zip:/Users/davies/work/spark/sbin/../python:
{code}


---

* [SPARK-9939](https://issues.apache.org/jira/browse/SPARK-9939) | *Critical* | **Resort to Java process API in test suites forking subprocesses**

The following SQL test suites fork subprocesses and have been flaky for quite a while and fail now and then due to mysterious timeouts:

- {{CliSuite}}
- {{HiveThriftBinaryServerSuite}}
- {{HiveSparkSubmitSuite}}
- {{HiveThriftHttpServerSuite}}

One of the possible reasons of this is a known bug of Scala process API  ([SI-8768\|https://issues.scala-lang.org/browse/SI-8768]). Resorting to Java process API may possibly fix these flaky tests.


---

* [SPARK-9936](https://issues.apache.org/jira/browse/SPARK-9936) | *Major* | **decimal precision lost when loading DataFrame from RDD**

It seems that when converting an RDD that contains BigDecimals into a DataFrame (using SQLContext.createDataFrame without specifying schema), precision info is lost, which means saving as Parquet file will fail (Parquet tries to verify precision \< 18, so fails if it's unset).

This seems to be similar to [SPARK-7196\|https://issues.apache.org/jira/browse/SPARK-7196], which fixed the same issue for DataFrames created via JDBC.

To reproduce:
{code:none}
scala\> val rdd: RDD[(String, BigDecimal)] = sc.parallelize(Seq(("a", BigDecimal.valueOf(0.234))))
rdd: org.apache.spark.rdd.RDD[(String, BigDecimal)] = ParallelCollectionRDD[0] at parallelize at \<console\>:23

scala\> val df: DataFrame = new SQLContext(rdd.context).createDataFrame(rdd)
df: org.apache.spark.sql.DataFrame = [\_1: string, \_2: decimal(10,0)]

scala\> df.write.parquet("/data/parquet-file")
15/08/13 10:30:07 ERROR executor.Executor: Exception in task 0.0 in stage 0.0 (TID 0)
java.lang.RuntimeException: Unsupported datatype DecimalType()
{code}

To verify this is indeed caused by the precision being lost, I've tried manually changing the schema to include precision (by traversing the StructFields and replacing the DecimalTypes with altered DecimalTypes), creating a new DataFrame using this updated schema - and indeed it fixes the problem.

I'm using Spark 1.4.0.


---

* [SPARK-9934](https://issues.apache.org/jira/browse/SPARK-9934) | *Major* | **Deprecate NIO ConnectionManager**

We should deprecate ConnectionManager in 1.5 before removing it in 1.6.


---

* [SPARK-9933](https://issues.apache.org/jira/browse/SPARK-9933) | *Major* | **Test the new receiver scheduling**

1. Test if receivers are load balanced in the beginning by running the app many times and verifying whether they are load balanced. Maybe tweak the minRegistered.
2. Implement and test a timeout after which if all the receivers have not started, it should fail.
3. Test whether receivers are restarted after any failure, independent of the max task attempts.


---

* [SPARK-9927](https://issues.apache.org/jira/browse/SPARK-9927) | *Blocker* | **Revert fix of 9182 since it's pushing the wrong filter down**

I made a mistake in https://github.com/apache/spark/pull/8049 by casting literal value to attribute's data type, which would cause simply truncate the literal value and push a wrong filter down.


---

* [SPARK-9922](https://issues.apache.org/jira/browse/SPARK-9922) | *Major* | **Rename StringIndexerInverse to IndexToString**

What StringIndexerInverse does is not strictly associated with StringIndexer, and the name is not super clear.


---

* [SPARK-9920](https://issues.apache.org/jira/browse/SPARK-9920) | *Minor* | **The simpleString of TungstenAggregate does not show its output**

It only shows grouping expressions and agg functions.


---

* [SPARK-9918](https://issues.apache.org/jira/browse/SPARK-9918) | *Major* | **Remove runs from KMeans under the pipeline API**

This requires some discussion. I'm not sure whether `runs` is a useful parameter. It certainly complicates the implementation. We might want to optimize the k-means implementation with block matrix operations. In this case, having `runs` may not be worth the trade-offs.


---

* [SPARK-9917](https://issues.apache.org/jira/browse/SPARK-9917) | *Trivial* | **MinMaxScaler missing getters and docs**

\* missing getMin / getMax
\* originalMin / originalMax doesn't have API doc


---

* [SPARK-9916](https://issues.apache.org/jira/browse/SPARK-9916) | *Blocker* | **Clear leftover sparkr.zip copies and creations (e.g. make-distribution.sh)**

sparkr.zip is now created during Spark Submit only for Yarn Cluster mode users. The leftover cp commands (e.g. in make-distribution.sh) for sparkr.zip cause failures.


---

* [SPARK-9913](https://issues.apache.org/jira/browse/SPARK-9913) | *Trivial* | **LDAUtils should be private**

It contains no public APIs.


---

* [SPARK-9911](https://issues.apache.org/jira/browse/SPARK-9911) | *Major* | **User guide for MulticlassClassificationEvaluator**

SPARK-7690 adds MulticlassClassificationEvaluator to ML Pipelines which is not present in MLlib. We need to update the user-guide ({{ml-guide#Algorithm Guides}} to document this feature.


---

* [SPARK-9910](https://issues.apache.org/jira/browse/SPARK-9910) | *Major* | **User guide for train validation split**

SPARK-8484 adds a TrainValidationSplit transformer which needs user guide docs and example code.


---

* [SPARK-9909](https://issues.apache.org/jira/browse/SPARK-9909) | *Minor* | **Move weightCol to sharedParams**

As per the TODO in IsotonicRegression.scala move the weightCol Param to Shared params.


---

* [SPARK-9908](https://issues.apache.org/jira/browse/SPARK-9908) | *Blocker* | **TPCDS Q98 failed when tungsten is off**

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 43.0 failed 4 times, most recent failure: Lost task 0.3 in stage 43.0 (TID 519, 10.0.237.253): java.io.InvalidClassException: org.apache.spark.sql.execution.joins.UniqueKeyHashedRelation; no valid constructor
	at java.io.ObjectStreamClass$ExceptionInfo.newInvalidClassException(ObjectStreamClass.java:150)
	at java.io.ObjectStreamClass.checkDeserialize(ObjectStreamClass.java:768)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1772)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:217)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:178)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1276)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)
	at org.apache.spark.broadcast.TorrentBroadcast.\_value$lzycompute(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.\_value(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$2.apply(BroadcastHashJoin.scala:91)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$2.apply(BroadcastHashJoin.scala:90)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1256)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1247)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1246)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1246)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:681)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:681)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:681)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1466)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1428)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1417)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:554)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1795)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:1915)
	at org.apache.spark.rdd.RDD$$anonfun$reduce$1.apply(RDD.scala:1003)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.reduce(RDD.scala:985)
	at org.apache.spark.rdd.RDD$$anonfun$takeOrdered$1.apply(RDD.scala:1366)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:306)
	at org.apache.spark.rdd.RDD.takeOrdered(RDD.scala:1353)
	at org.apache.spark.sql.execution.TakeOrderedAndProject.collectData(basicOperators.scala:234)
	at org.apache.spark.sql.execution.TakeOrderedAndProject.executeCollect(basicOperators.scala:240)
	at org.apache.spark.sql.DataFrame$$anonfun$collect$1.apply(DataFrame.scala:1383)
	at org.apache.spark.sql.DataFrame$$anonfun$collect$1.apply(DataFrame.scala:1383)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:56)
	at org.apache.spark.sql.DataFrame.withNewExecutionId(DataFrame.scala:1899)
	at org.apache.spark.sql.DataFrame.collect(DataFrame.scala:1382)
	at org.apache.spark.sql.DataFrame.head(DataFrame.scala:1312)
	at org.apache.spark.sql.DataFrame.take(DataFrame.scala:1375)
	at com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation0(OutputAggregator.scala:70)
	at com.databricks.backend.daemon.driver.OutputAggregator$.withOutputAggregation(OutputAggregator.scala:42)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$2.apply(ScalaDriverLocal.scala:171)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal$$anonfun$repl$2.apply(ScalaDriverLocal.scala:164)
	at scala.Option.map(Option.scala:145)
	at com.databricks.backend.daemon.driver.ScalaDriverLocal.repl(ScalaDriverLocal.scala:164)
	at com.databricks.backend.daemon.driver.DriverLocal.execute(DriverLocal.scala:175)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:484)
	at com.databricks.backend.daemon.driver.DriverWrapper$$anonfun$3.apply(DriverWrapper.scala:484)
	at scala.util.Try$.apply(Try.scala:161)
	at com.databricks.backend.daemon.driver.DriverWrapper.executeCommand(DriverWrapper.scala:481)
	at com.databricks.backend.daemon.driver.DriverWrapper.runInner(DriverWrapper.scala:383)
	at com.databricks.backend.daemon.driver.DriverWrapper.run(DriverWrapper.scala:194)
	at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-9906](https://issues.apache.org/jira/browse/SPARK-9906) | *Major* | **User guide for LogisticRegressionSummary**

SPARK-9112 introduces {{LogisticRegressionSummary}} to provide R-like model statistics to ML pipeline logistic regression models. This feature is not present in mllib and should be documented within {{ml-linear-methods}}


---

* [SPARK-9905](https://issues.apache.org/jira/browse/SPARK-9905) | *Major* | **User guide for LinearRegressionSummary**

SPARK-8539 introduces {{LinearRegressionSummary}} to provide R-like model statistics to ML pipeline linear regression models. This feature is not present in mllib and should be documented within {{ml-guide}}


---

* [SPARK-9903](https://issues.apache.org/jira/browse/SPARK-9903) | *Major* | **Skip local processing in PrefixSpan if there are no small prefixes**

There exists a chance that the prefixes keep growing to the maximum pattern length. Then the final local processing step becomes unnecessary.


---

* [SPARK-9902](https://issues.apache.org/jira/browse/SPARK-9902) | *Major* | **Add Java and Python examples to user guide for 1-sample KS test**

SPARK-8598 adds 1-sample kolmogorov-smirnov tests, which needs Java and python code examples in {{mllib-statistics#hypothesis-testing}}.


---

* [SPARK-9901](https://issues.apache.org/jira/browse/SPARK-9901) | *Major* | **User guide for RowMatrix Tall-and-skinny QR**

SPARK-7368 adds Tall-and-Skinny QR factorization. {{mllib-data-types#rowmatrix}} should be updated to document this feature.


---

* [SPARK-9900](https://issues.apache.org/jira/browse/SPARK-9900) | *Major* | **User Guide for Association Rule Generation**

SPARK-8559 extends {{FPGrowth}} to allow for association rule generation. This should be documented in the MLlib user guides.


---

* [SPARK-9899](https://issues.apache.org/jira/browse/SPARK-9899) | *Blocker* | **JSON/Parquet writing on retry or speculation broken with direct output committer**

If the first task fails all subsequent tasks will.  We probably need to set a different boolean when calling create.

{code}
java.io.IOException: File already exists: ...
...
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:564)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:545)
	at org.apache.hadoop.fs.FileSystem.create(FileSystem.java:452)
	at org.apache.hadoop.mapreduce.lib.output.TextOutputFormat.getRecordWriter(TextOutputFormat.java:128)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.\<init\>(JSONRelation.scala:185)
	at org.apache.spark.sql.execution.datasources.json.JSONRelation$$anon$1.newInstance(JSONRelation.scala:160)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:217)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelation$$anonfun$run$1$$anonfun$apply$mcV$sp$3.apply(InsertIntoHadoopFsRelation.scala:150)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

{code}

The reason behind this issue is that speculation shouldn't be used together with direct output committer. As there are multiple corner cases that this combination may cause data corruption and/or data loss. Please refer to this [GitHub comment\|https://github.com/apache/spark/pull/8191#issuecomment-131598385] for more details about these corner cases.


---

* [SPARK-9898](https://issues.apache.org/jira/browse/SPARK-9898) | *Major* | **User guide for PrefixSpan**

PrefixSpan was added by SPARK-6487 and needs an accompanying user guide/example code. This should be included in the MLlib docs.


---

* [SPARK-9895](https://issues.apache.org/jira/browse/SPARK-9895) | *Major* | **User Guide for RFormula Feature Transformer**

SPARK-8774 adds RFormulaModel as a feature transformer. The user guide should include a description and example code for using this.


---

* [SPARK-9894](https://issues.apache.org/jira/browse/SPARK-9894) | *Blocker* | **Writing of JSON files with MapType is broken**

{code}
Caused by: scala.MatchError: (MapType(StringType,StringType,true),keys: [StandardRun], values: [true]) (of class scala.Tuple2)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:126)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$$anonfun$org$apache$spark$sql$execution$datasources$json$JacksonGenerator$$valWriter$2$1.apply(JacksonGenerator.scala:89)
	at org.apache.spark.sql.execution.datasources.json.JacksonGenerator$.apply(JacksonGenerator.scala:133)
	at org.apache.spark.sql.execution.datasources.json.JsonOutputWriter.writeInternal(JSONRelation.scala:191)
	at org.apache.spark.sql.execution.datasources.DefaultWriterContainer.writeRows(WriterContainer.scala:224)
{code}


---

* [SPARK-9893](https://issues.apache.org/jira/browse/SPARK-9893) | *Major* | **User guide for VectorSlicer**

SPARK-5895 added VectorSlicer to ml feature transformers. We should add an accompanying user guide to {{ml-features}}


---

* [SPARK-9890](https://issues.apache.org/jira/browse/SPARK-9890) | *Major* | **User guide for CountVectorizer**

SPARK-8703 added a count vectorizer as a ML transformer. We should add an accompanying user guide to {{ml-features}}


---

* [SPARK-9888](https://issues.apache.org/jira/browse/SPARK-9888) | *Major* | **Update LDA User Guide**

LDA has received numerous updates in 1.5, including:
 \* OnlineLDAOptimizer:
        \* Asymmetric document-topic priors
        \* Document-topic hyperparameter optimization
 \* LocalLDAModel
        \* predict
        \* logPerplexity / logLikelihood
 \* DistributedLDAModel:
        \* topDocumentsPerTopic
        \* topTopicsPerDoc
 \* Save/load

It is important to note that OnlineLDAOptimizer=\>LocalLDAModel and EMLDAOptimizer=\>DistributedLDAModel now support different features. The user guide should document these differences.


---

* [SPARK-9885](https://issues.apache.org/jira/browse/SPARK-9885) | *Critical* | **IsolatedClientLoader ignores shared prefixes and barrier prefixes when "spark.sql.hive.metastore.jars" is set to "maven"**

I have a local Hive 0.13.1 metastore backed by MySQL. MySQL JDBC connector is put under {{$JAVA\_HOME/jre/lib/ext}}. With the following {{spark-defaults.conf}}
{noformat}
spark.sql.hive.metastore.version 0.13.1
spark.sql.hive.metastore.jars maven
{noformat}
Spark shell fails to create {{HiveContext}} because MySQL JDBC driver couldn't be properly loaded.

This is probably because {{IsolatedClientLoader}} ignores shared prefixes and barrier prefixes when {{spark.sql.hive.metastore.jars}} is set to {{maven}}. See [this line\|https://github.com/apache/spark/blob/v1.4.1/sql/hive/src/main/scala/org/apache/spark/sql/hive/client/IsolatedClientLoader.scala#L45].


---

* [SPARK-9877](https://issues.apache.org/jira/browse/SPARK-9877) | *Blocker* | **StandaloneRestServer NPE when submitting application**

When submitting a simple SparkPi application using standalone cluster mode:

{code}
./bin/spark-submit --verbose --master spark://hw12100.local:6066 --deploy-mode cluster --class org.apache.spark.examples.SparkPi examples/target/scala-2.10/spark-examples-1.5.0-SNAPSHOT-hadoop2.6.0.jar
{code}

Client side throw such exceptions:

{noformat}
15/08/12 17:14:16 INFO rest.RestSubmissionClient: Submitting a request to launch an application in spark://hw12100.local:6066.
Exception in thread "main" org.apache.spark.deploy.rest.SubmitRestProtocolException: Malformed response received from server
	at org.apache.spark.deploy.rest.RestSubmissionClient.readResponse(RestSubmissionClient.scala:258)
	at org.apache.spark.deploy.rest.RestSubmissionClient.org$apache$spark$deploy$rest$RestSubmissionClient$$postJson(RestSubmissionClient.scala:219)
	at org.apache.spark.deploy.rest.RestSubmissionClient$$anonfun$createSubmission$3.apply(RestSubmissionClient.scala:84)
	at org.apache.spark.deploy.rest.RestSubmissionClient$$anonfun$createSubmission$3.apply(RestSubmissionClient.scala:80)
	at scala.collection.TraversableLike$WithFilter$$anonfun$foreach$1.apply(TraversableLike.scala:772)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$WithFilter.foreach(TraversableLike.scala:771)
	at org.apache.spark.deploy.rest.RestSubmissionClient.createSubmission(RestSubmissionClient.scala:80)
	at org.apache.spark.deploy.rest.RestSubmissionClient$.run(RestSubmissionClient.scala:404)
	at org.apache.spark.deploy.rest.RestSubmissionClient$.main(RestSubmissionClient.scala:416)
	at org.apache.spark.deploy.rest.RestSubmissionClient.main(RestSubmissionClient.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:672)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:194)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: com.fasterxml.jackson.core.JsonParseException: Unexpected character ('\<' (code 60)): expected a valid value (number, String, array, object, 'true', 'false' or 'null')
 at [Source: \<html\>
\<head\>
\<meta http-equiv="Content-Type" content="text/html;charset=ISO-8859-1"/\>
\<title\>Error 500 Server Error\</title\>
\</head\>
\<body\>
\<h2\>HTTP ERROR: 500\</h2\>
\<p\>Problem accessing /v1/submissions/create. Reason:
\<pre\>    Server Error\</pre\>\</p\>
\<hr /\>\<i\>\<small\>Powered by Jetty://\</small\>\</i\>




















\</body\>
\</html\>
; line: 1, column: 2]
	at com.fasterxml.jackson.core.JsonParser.\_constructError(JsonParser.java:1419)
	at com.fasterxml.jackson.core.base.ParserMinimalBase.\_reportError(ParserMinimalBase.java:508)
	at com.fasterxml.jackson.core.base.ParserMinimalBase.\_reportUnexpectedChar(ParserMinimalBase.java:437)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.\_handleOddValue(ReaderBasedJsonParser.java:1462)
	at com.fasterxml.jackson.core.json.ReaderBasedJsonParser.nextToken(ReaderBasedJsonParser.java:683)
	at com.fasterxml.jackson.databind.ObjectMapper.\_initForReading(ObjectMapper.java:3105)
	at com.fasterxml.jackson.databind.ObjectMapper.\_readMapAndClose(ObjectMapper.java:3051)
	at com.fasterxml.jackson.databind.ObjectMapper.readValue(ObjectMapper.java:2161)
	at org.json4s.jackson.JsonMethods$class.parse(JsonMethods.scala:19)
	at org.json4s.jackson.JsonMethods$.parse(JsonMethods.scala:44)
	at org.apache.spark.deploy.rest.SubmitRestProtocolMessage$.parseAction(SubmitRestProtocolMessage.scala:112)
	at org.apache.spark.deploy.rest.SubmitRestProtocolMessage$.fromJson(SubmitRestProtocolMessage.scala:130)
	at org.apache.spark.deploy.rest.RestSubmissionClient.readResponse(RestSubmissionClient.scala:241)
	... 20 more
{noformat}

also following exception in the master side:
{noformat}
java.lang.NullPointerException
        at org.apache.spark.deploy.rest.StandaloneSubmitRequestServlet.handleSubmit(StandaloneRestServer.scala:177)
        at org.apache.spark.deploy.rest.SubmitRequestServlet.doPost(RestSubmissionServer.scala:258)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:755)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:848)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:684)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:501)
        at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:1086)
        at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:428)
        at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:1020)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:135)
        at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)
        at org.eclipse.jetty.server.Server.handle(Server.java:370)
        at org.eclipse.jetty.server.AbstractHttpConnection.handleRequest(AbstractHttpConnection.java:494)
        at org.eclipse.jetty.server.AbstractHttpConnection.content(AbstractHttpConnection.java:982)
        at org.eclipse.jetty.server.AbstractHttpConnection$RequestHandler.content(AbstractHttpConnection.java:1043)
        at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:865)
        at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:240)
        at org.eclipse.jetty.server.AsyncHttpConnection.handle(AsyncHttpConnection.java:82)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:667)
        at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:52)
        at org.eclipse.jetty.util.thread.QueuedThreadPool.runJob(QueuedThreadPool.java:608)
        at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:543)
        at java.lang.Thread.run(Thread.java:745)
{noformat}

Through investigating the code, {{masterEndpoint}} is null when creating {{StandaloneRestServer}} object, this object should be created after {{masterEndpoint}} is not null.


---

* [SPARK-9871](https://issues.apache.org/jira/browse/SPARK-9871) | *Major* | **Add expression functions into SparkR which have a variable parameter**

Add expression functions into SparkR which has a variable parameter, like {{concat}}


---

* [SPARK-9870](https://issues.apache.org/jira/browse/SPARK-9870) | *Major* | **Disable driver UI and Master REST server in SparkSubmitSuite**

I think that we should pass additional configuration flags to disable the driver UI and Master REST server in SparkSubmitSuite and HiveSparkSubmitSuite. This should cut down on port-contention-related flakiness in Jenkins.


---

* [SPARK-9864](https://issues.apache.org/jira/browse/SPARK-9864) | *Critical* | **Replace `@since` JavaDoc tag by `@Since` annotation in MLlib**

After SPARK-8967, we should use @Since annotation instead of @since tag in JavaDoc. This helps reuse API doc for inherited methods and public variables in the constructor, e.g.:

{code}
@Since("1.0.0")
override def predict(...)
{code}

{code}
/\*\*
 \* ...
 \*/
@Since("1.2.0")
class KMeansModel(
    @Since("1.3.0") override val uid: String,
    @Since("1.4.0") val centers: Array[Vector])
{code}


---

* [SPARK-9856](https://issues.apache.org/jira/browse/SPARK-9856) | *Major* | **Add expression functions into SparkR whose params are complicated**

Add expression functions whose parameters are a little complicated, like {{regexp\_extract(e: Column, exp: String, groupIdx: Int)}} and {{regexp\_replace(e: Column, pattern: String, replacement: String)}}.


---

* [SPARK-9855](https://issues.apache.org/jira/browse/SPARK-9855) | *Major* | **Add expression functions into SparkR whose params are simple**

Add expression functions whose parameters are only {{(Column)}} or {{(Column, Column)}}, like {{md5}}.


---

* [SPARK-9854](https://issues.apache.org/jira/browse/SPARK-9854) | *Blocker* | **RuleExecutor.timeMap should be thread-safe**

{{RuleExecutor.timeMap}} is currently a non-thread-safe mutable HashMap; this can lead to infinite loops if multiple threads are concurrently modifying the map.


---

* [SPARK-9849](https://issues.apache.org/jira/browse/SPARK-9849) | *Blocker* | **DirectParquetOutputCommitter qualified name should be backward compatible**

DirectParquetOutputCommitter was moved in SPARK-9763. However, users can explicitly set the class as a config option, so we must be able to resolve the old committer qualified name.


---

* [SPARK-9847](https://issues.apache.org/jira/browse/SPARK-9847) | *Critical* | **ML Params copyValues should copy default values to default map, not set map**

Currently, Params.copyValues copies default parameter values to the paramMap of the target instance, rather than the defaultParamMap.  It should copy to the defaultParamMap because explicitly setting a parameter can change the semantics.

This issue arose in [SPARK-9789], where 2 params "threshold" and "thresholds" for LogisticRegression can have mutually exclusive values.  If thresholds is set, then fit() will copy the default value of threshold as well, easily resulting in inconsistent settings for the 2 params.


---

* [SPARK-9832](https://issues.apache.org/jira/browse/SPARK-9832) | *Blocker* | **TPCDS Q98 Fails**

{code}
select
  i\_item\_desc,
  i\_category,
  i\_class,
  i\_current\_price,
  sum(ss\_ext\_sales\_price) as itemrevenue
  -- sum(ss\_ext\_sales\_price) \* 100 / sum(sum(ss\_ext\_sales\_price)) over (partition by i\_class) as revenueratio
from
  store\_sales
  join item on (store\_sales.ss\_item\_sk = item.i\_item\_sk)
  join date\_dim on (store\_sales.ss\_sold\_date\_sk = date\_dim.d\_date\_sk)
where
  i\_category in('Jewelry', 'Sports', 'Books')
  -- and d\_date between cast('2001-01-12' as date) and (cast('2001-01-12' as date) + 30)
  -- and d\_date between '2001-01-12' and '2001-02-11'
  -- and ss\_date between '2001-01-12' and '2001-02-11'
  -- and ss\_sold\_date\_sk between 2451922 and 2451952  -- partition key filter
  and ss\_sold\_date\_sk between 2451911 and 2451941  -- partition key filter (1 calendar month)
  and d\_date between '2001-01-01' and '2001-01-31'
group by
  i\_item\_id,
  i\_item\_desc,
  i\_category,
  i\_class,
  i\_current\_price
order by
  i\_category,
  i\_class,
  i\_item\_id,
  i\_item\_desc
  -- revenueratio
limit 1000
{code}

{code}
Job aborted due to stage failure: Task 11 in stage 62.0 failed 4 times, most recent failure: Lost task 11.3 in stage 62.0 (TID 5289, 10.0.227.73): java.lang.IllegalArgumentException: Unscaled value too large for precision
	at org.apache.spark.sql.types.Decimal.set(Decimal.scala:76)
	at org.apache.spark.sql.types.Decimal$.apply(Decimal.scala:338)
	at org.apache.spark.sql.types.Decimal.apply(Decimal.scala)
	at org.apache.spark.sql.catalyst.expressions.UnsafeRow.getDecimal(UnsafeRow.java:386)
	at org.apache.spark.sql.catalyst.expressions.JoinedRow.getDecimal(JoinedRow.scala:97)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.catalyst.expressions.GeneratedClass$SpecificUnsafeProjection.apply(Unknown Source)
	at org.apache.spark.sql.execution.joins.HashJoin$$anon$1.next(HashJoin.scala:101)
	at org.apache.spark.sql.execution.joins.HashJoin$$anon$1.next(HashJoin.scala:74)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.sql.execution.joins.HashJoin$$anon$1.fetchNext(HashJoin.scala:115)
	at org.apache.spark.sql.execution.joins.HashJoin$$anon$1.hasNext(HashJoin.scala:93)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.processInputs(TungstenAggregationIterator.scala:353)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregationIterator.\<init\>(TungstenAggregationIterator.scala:587)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$1.apply(TungstenAggregate.scala:72)
	at org.apache.spark.sql.execution.aggregate.TungstenAggregate$$anonfun$doExecute$1$$anonfun$1.apply(TungstenAggregate.scala:64)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-9831](https://issues.apache.org/jira/browse/SPARK-9831) | *Blocker* | **TPCDS Q73 Fails**

{code}
select
  c\_last\_name,
  c\_first\_name,
  c\_salutation,
  c\_preferred\_cust\_flag,
  ss\_ticket\_number,
  cnt
from
  (select
    ss\_ticket\_number,
    ss\_customer\_sk,
    count(\*) cnt
  from
    store\_sales
    join household\_demographics on (store\_sales.ss\_hdemo\_sk = household\_demographics.hd\_demo\_sk)
    join store on (store\_sales.ss\_store\_sk = store.s\_store\_sk)
    -- join date\_dim on (store\_sales.ss\_sold\_date\_sk = date\_dim.d\_date\_sk)
  where
    store.s\_county in ('Saginaw County', 'Sumner County', 'Appanoose County', 'Daviess County')
    -- and date\_dim.d\_dom between 1 and 2
    -- and date\_dim.d\_year in(1998, 1998 + 1, 1998 + 2)
    -- and ss\_date between '1999-01-01' and '2001-12-02'
    -- and dayofmonth(ss\_date) in (1,2)
    -- partition key filter
    -- and ss\_sold\_date\_sk in (2450816, 2450846, 2450847, 2450874, 2450875, 2450905, 2450906, 2450935, 2450936, 2450966, 2450967,
    --                         2450996, 2450997, 2451027, 2451028, 2451058, 2451059, 2451088, 2451089, 2451119, 2451120, 2451149,
    --                         2451150, 2451180, 2451181, 2451211, 2451212, 2451239, 2451240, 2451270, 2451271, 2451300, 2451301,
    --                         2451331, 2451332, 2451361, 2451362, 2451392, 2451393, 2451423, 2451424, 2451453, 2451454, 2451484,
    --                         2451485, 2451514, 2451515, 2451545, 2451546, 2451576, 2451577, 2451605, 2451606, 2451636, 2451637,
    --                         2451666, 2451667, 2451697, 2451698, 2451727, 2451728, 2451758, 2451759, 2451789, 2451790, 2451819,
    --                         2451820, 2451850, 2451851, 2451880, 2451881)
    and (household\_demographics.hd\_buy\_potential = '\>10000'
      or household\_demographics.hd\_buy\_potential = 'unknown')
    and household\_demographics.hd\_vehicle\_count \> 0
    and case when household\_demographics.hd\_vehicle\_count \> 0 then household\_demographics.hd\_dep\_count / household\_demographics.hd\_vehicle\_count else null end \> 1
    and ss\_sold\_date\_sk between 2451180 and 2451269 -- partition key filter (3 months)
  group by
    ss\_ticket\_number,
    ss\_customer\_sk
  ) dj
  join customer on (dj.ss\_customer\_sk = customer.c\_customer\_sk)
where
  cnt between 1 and 5
order by
  cnt desc
limit 1000
{code}

{code}
Job aborted due to stage failure: Task 63 in stage 57.0 failed 4 times, most recent failure: Lost task 63.3 in stage 57.0 (TID 5103, 10.0.197.102): java.io.IOException: java.lang.IllegalArgumentException: Initial capacity must be greater than 0
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1264)
	at org.apache.spark.sql.execution.joins.UnsafeHashedRelation.readExternal(HashedRelation.scala:280)
	at java.io.ObjectInputStream.readExternalData(ObjectInputStream.java:1837)
	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1796)
	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1350)
	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:370)
	at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:72)
	at org.apache.spark.broadcast.TorrentBroadcast$.unBlockifyObject(TorrentBroadcast.scala:217)
	at org.apache.spark.broadcast.TorrentBroadcast$$anonfun$readBroadcastBlock$1.apply(TorrentBroadcast.scala:178)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1276)
	at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:165)
	at org.apache.spark.broadcast.TorrentBroadcast.\_value$lzycompute(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.\_value(TorrentBroadcast.scala:64)
	at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:88)
	at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$2.apply(BroadcastHashJoin.scala:91)
	at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$2.apply(BroadcastHashJoin.scala:90)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:38)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.IllegalArgumentException: Initial capacity must be greater than 0
	at org.apache.spark.unsafe.map.BytesToBytesMap.\<init\>(BytesToBytesMap.java:185)
	at org.apache.spark.unsafe.map.BytesToBytesMap.\<init\>(BytesToBytesMap.java:203)
	at org.apache.spark.sql.execution.joins.UnsafeHashedRelation$$anonfun$readExternal$1.apply$mcV$sp(HashedRelation.scala:295)
	at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1261)
	... 40 more
{code}


---

* [SPARK-9829](https://issues.apache.org/jira/browse/SPARK-9829) | *Major* | **peakExecutionMemory is not correct in task table**

When run a query with 8G memory, the peakExecutionMemory in WebUI said that 40344371200 (40G).

Alos there are lots of accumulators with the same name, can't know what do they mean
{code}
Accumulable	Value
number of output rows	439614
number of output rows	7711
number of output rows	965
number of rows	7829
number of rows	7711
number of input rows	965
number of rows	52
number of input rows	439614
number of output rows	30
number of input rows	7726
number of rows	277000
peakExecutionMemory	40344371200
number of rows	7829
number of rows	965
number of rows	7726
number of rows	30
number of rows	138000
number of rows	8028
number of rows	439614
number of input rows	30
{code}

How to reproduce:

run TPCDS q19 with scale=5, checkout out the Web UI


---

* [SPARK-9828](https://issues.apache.org/jira/browse/SPARK-9828) | *Critical* | **Should not share `{}` among instances**

We use `{}` as the initial value in some places, e.g., https://github.com/apache/spark/blob/master/python/pyspark/ml/param/\_\_init\_\_.py#L64. This makes instances sharing the same param map.


---

* [SPARK-9827](https://issues.apache.org/jira/browse/SPARK-9827) | *Blocker* | **Too many open files in TungstenExchange**

When run q19 on TPCDS (scale=5) dataset with 8G memory, it open 10k shuffle files, crash many things (even Chrome).

{code}
davies@localhost:~/work/spark$ jps
95385 Jps
95316 SparkSubmit
davies@localhost:~/work/spark$ lsof -p 95316 \| wc -l
9827
davies@localhost:~/work/spark$ lsof -p 95316 \| tail
java    95316 davies 9772r     REG                1,2      9522 97350739 /private/var/folders/r1/j51v8t\_x4bq6fqt43nymzddw0000gn/T/blockmgr-6dd1dd1b-735d-4eae-a7eb-72820e0a2e7b/2a/shuffle\_0\_112\_0.data
java    95316 davies 9773r     REG                1,2      8449 97351388 /private/var/folders/r1/j51v8t\_x4bq6fqt43nymzddw0000gn/T/blockmgr-6dd1dd1b-735d-4eae-a7eb-72820e0a2e7b/1a/shuffle\_0\_116\_0.data
java    95316 davies 9774r     REG                1,2      8200 97351134 /private/var/folders/r1/j51v8t\_x4bq6fqt43nymzddw0000gn/T/blockmgr-6dd1dd1b-735d-4eae-a7eb-72820e0a2e7b/09/shuffle\_0\_113\_0.data
java    95316 davies 9775r     REG                1,2      8057 97351941 /private/var/folders/r1/j51v8t\_x4bq6fqt43nymzddw0000gn/T/blockmgr-6dd1dd1b-735d-4eae-a7eb-72820e0a2e7b/05/shuffle\_0\_117\_0.data
java    95316 davies 9776r     REG                1,2      8565 97351133 /private/var/folders/r1/j51v8t\_x4bq6fqt43nymzddw0000gn/T/blockmgr-6dd1dd1b-735d-4eae-a7eb-72820e0a2e7b/18/shuffle\_0\_114\_0.data
java    95316 davies 9777r     REG                1,2      8185 97351942 /private/var/folders/r1/j51v8t\_x4bq6fqt43nymzddw0000gn/T/blockmgr-6dd1dd1b-735d-4eae-a7eb-72820e0a2e7b/1c/shuffle\_0\_118\_0.data
java    95316 davies 9778r     REG                1,2      8865 97351135 /private/var/folders/r1/j51v8t\_x4bq6fqt43nymzddw0000gn/T/blockmgr-6dd1dd1b-735d-4eae-a7eb-72820e0a2e7b/07/shuffle\_0\_115\_0.data
java    95316 davies 9779r     REG                1,2      8255 97351987 /private/var/folders/r1/j51v8t\_x4bq6fqt43nymzddw0000gn/T/blockmgr-6dd1dd1b-735d-4eae-a7eb-72820e0a2e7b/3d/shuffle\_0\_119\_0.data
java    95316 davies 9780r     REG                1,2      8449 97351388 /private/var/folders/r1/j51v8t\_x4bq6fqt43nymzddw0000gn/T/blockmgr-6dd1dd1b-735d-4eae-a7eb-72820e0a2e7b/1a/shuffle\_0\_116\_0.data
java    95316 davies 9781r     REG                1,2      9105 97352148 /private/var/folders/r1/j51v8t\_x4bq6fqt43nymzddw0000gn/T/blockmgr-6dd1dd1b-735d-4eae-a7eb-72820e0a2e7b/13/shuffle\_0\_120\_0.data

davies@localhost:~/work/spark$ ls -l /private/var/folders/r1/j51v8t\_x4bq6fqt43nymzddw0000gn/T/blockmgr-71afa3af-f2a5-4b72-8b2d-45aa70ff7466//3a/
total 68
-rw-r--r-- 1 davies staff 8272 Aug 11 09:57 shuffle\_0\_105\_0.data
-rw-r--r-- 1 davies staff 1608 Aug 11 09:57 shuffle\_0\_109\_0.index
-rw-r--r-- 1 davies staff 8414 Aug 11 09:57 shuffle\_0\_127\_0.data
-rw-r--r-- 1 davies staff 8368 Aug 11 09:57 shuffle\_0\_149\_0.data
-rw-r--r-- 1 davies staff 1608 Aug 11 09:57 shuffle\_0\_40\_0.index
-rw-r--r-- 1 davies staff 1608 Aug 11 09:57 shuffle\_0\_62\_0.index
-rw-r--r-- 1 davies staff 7965 Aug 11 09:57 shuffle\_0\_6\_0.data
-rw-r--r-- 1 davies staff 8419 Aug 11 09:57 shuffle\_0\_80\_0.data
{code}


---

* [SPARK-9826](https://issues.apache.org/jira/browse/SPARK-9826) | *Minor* | **Cannot use custom classes in log4j.properties**

log4j is initialized before spark class loader is set on the thread context.
Therefore it cannot use classes embedded in fat-jars submitted to spark.

While parsing arguments, spark calls methods on Utils class and triggers ShutdownHookManager static initialization.  This then leads to log4j being initialized before spark gets the chance to specify custom class MutableURLClassLoader on the thread context.

See detailed explanation here:
http://apache-spark-user-list.1001560.n3.nabble.com/log4j-custom-appender-ClassNotFoundException-with-spark-1-4-1-tt24159.html


---

* [SPARK-9824](https://issues.apache.org/jira/browse/SPARK-9824) | *Blocker* | **Internal Accumulators will leak WeakReferences**

InternalAccumulator.create doesn't call `registerAccumulatorForCleanup` to register itself with ContextCleaner, so `WeakReference`s for these accumulators in Accumulators.originals won't be removed.


---

* [SPARK-9815](https://issues.apache.org/jira/browse/SPARK-9815) | *Major* | **Rename PlatformDependent.UNSAFE -\> Platform**

PlatformDependent.UNSAFE is way too verbose.


---

* [SPARK-9814](https://issues.apache.org/jira/browse/SPARK-9814) | *Minor* | **EqualNullSafe not passing to data sources**

When data sources (such as Parquet) tries to filter data when reading from HDFS (not in memory), Physical planing phase passes the filter objects in {{org.apache.spark.sql.sources}}, which are appropriately built and picked up by {{selectFilters()}} in {{org.apache.spark.sql.sources.DataSourceStrategy}}.

On the other hand, it does not pass {{EqualNullSafe}} filter in {{org.apache.spark.sql.catalyst.expressions}} even though this seems possible to pass for other datasources such as Parquet and JSON. In more detail, it does not pass {{EqualNullSafe}} to (below) {{buildScan()}} in {{PrunedFilteredScan}} and {{PrunedScan}}, 

{code}
def buildScan(requiredColumns: Array[String], filters: Array[Filter]): RDD[Row]
{code}

even though the binary capability issue is solved.(https://issues.apache.org/jira/browse/SPARK-8747).

I understand that {{CatalystScan}} can take the all raw expressions accessing to the query planner. However, it is experimental and also it needs different interfaces (as well as unstable for the reasons such as binary capability).


In general, the problem below can happen.

1.
{code:sql}
SELECT \* FROM table WHERE field = 1;
{code}
 
2. 
{code:sql}
SELECT \* FROM table WHERE field \<=\> 1;
{code}

The second query can be hugely slow although the functionally is almost identical because of the possible large network traffic (etc.) by not filtered data from the source RDD.


---

* [SPARK-9813](https://issues.apache.org/jira/browse/SPARK-9813) | *Major* | **Incorrect UNION ALL behavior**

According to the [Hive Language Manual\|https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Union] for UNION ALL:

{quote}
The number and names of columns returned by each select\_statement have to be the same. Otherwise, a schema error is thrown.
{quote}

Spark SQL silently swallows an error when the tables being joined with UNION ALL have the same number of columns but different names.

Reproducible example:

{code}
// This test is meant to run in spark-shell
import java.io.File
import java.io.PrintWriter
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.SaveMode

val ctx = sqlContext.asInstanceOf[HiveContext]
import ctx.implicits.\_

def dataPath(name:String) = sys.env("HOME") + "/" + name + ".jsonlines"

def tempTable(name: String, json: String) = {
  val path = dataPath(name)
  new PrintWriter(path) { write(json); close }
  ctx.read.json("file://" + path).registerTempTable(name)
}

// Note category vs. cat names of first column
tempTable("test\_one", """{"category" : "A", "num" : 5}""")
tempTable("test\_another", """{"cat" : "A", "num" : 5}""")

//  +--------+---+
//  \|category\|num\|
//  +--------+---+
//  \|       A\|  5\|
//  \|       A\|  5\|
//  +--------+---+
//
//  Instead, an error should have been generated due to incompatible schema
ctx.sql("select \* from test\_one union all select \* from test\_another").show

// Cleanup
new File(dataPath("test\_one")).delete()
new File(dataPath("test\_another")).delete()
{code}

When the number of columns is different, Spark can even mix in datatypes. 

Reproducible example (requires a new spark-shell session):

{code}
// This test is meant to run in spark-shell
import java.io.File
import java.io.PrintWriter
import org.apache.spark.sql.hive.HiveContext
import org.apache.spark.sql.SaveMode

val ctx = sqlContext.asInstanceOf[HiveContext]
import ctx.implicits.\_

def dataPath(name:String) = sys.env("HOME") + "/" + name + ".jsonlines"

def tempTable(name: String, json: String) = {
  val path = dataPath(name)
  new PrintWriter(path) { write(json); close }
  ctx.read.json("file://" + path).registerTempTable(name)
}

// Note test\_another is missing category column
tempTable("test\_one", """{"category" : "A", "num" : 5}""")
tempTable("test\_another", """{"num" : 5}""")

//  +--------+
//  \|category\|
//  +--------+
//  \|       A\|
//  \|       5\| 
//  +--------+
//
//  Instead, an error should have been generated due to incompatible schema
ctx.sql("select \* from test\_one union all select \* from test\_another").show

// Cleanup
new File(dataPath("test\_one")).delete()
new File(dataPath("test\_another")).delete()
{code}

At other times, when the schema are complex, Spark SQL produces a misleading error about an unresolved Union operator:

{code}
scala\> ctx.sql("""select \* from view\_clicks
     \| union all
     \| select \* from view\_clicks\_aug
     \| """)
15/08/11 02:40:25 INFO ParseDriver: Parsing command: select \* from view\_clicks
union all
select \* from view\_clicks\_aug
15/08/11 02:40:25 INFO ParseDriver: Parse Completed
15/08/11 02:40:25 INFO HiveMetaStore: 0: get\_table : db=default tbl=view\_clicks
15/08/11 02:40:25 INFO audit: ugi=ubuntu	ip=unknown-ip-addr	cmd=get\_table : db=default tbl=view\_clicks
15/08/11 02:40:25 INFO HiveMetaStore: 0: get\_table : db=default tbl=view\_clicks
15/08/11 02:40:25 INFO audit: ugi=ubuntu	ip=unknown-ip-addr	cmd=get\_table : db=default tbl=view\_clicks
15/08/11 02:40:25 INFO HiveMetaStore: 0: get\_table : db=default tbl=view\_clicks\_aug
15/08/11 02:40:25 INFO audit: ugi=ubuntu	ip=unknown-ip-addr	cmd=get\_table : db=default tbl=view\_clicks\_aug
15/08/11 02:40:25 INFO HiveMetaStore: 0: get\_table : db=default tbl=view\_clicks\_aug
15/08/11 02:40:25 INFO audit: ugi=ubuntu	ip=unknown-ip-addr	cmd=get\_table : db=default tbl=view\_clicks\_aug
org.apache.spark.sql.AnalysisException: unresolved operator 'Union;
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.failAnalysis(CheckAnalysis.scala:38)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.failAnalysis(Analyzer.scala:42)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:126)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1.apply(CheckAnalysis.scala:50)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:98)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:97)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:97)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:97)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:97)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$foreachUp$1.apply(TreeNode.scala:97)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:97)
	at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$class.checkAnalysis(CheckAnalysis.scala:50)
	at org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:42)
	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)
	at org.apache.spark.sql.DataFrame.\<init\>(DataFrame.scala:131)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.sql(SQLContext.scala:755){code}


---

* [SPARK-9809](https://issues.apache.org/jira/browse/SPARK-9809) | *Blocker* | **Task crashes because the internal accumulators are not properly initialized**

When a stage failed and another stage was resubmitted with only part of partitions to compute, all the tasks failed with error message: java.util.NoSuchElementException: key not found: peakExecutionMemory.
This is because the internal accumulators are not properly initialized for this stage while other codes assume the internal accumulators always exist.

{code}
Job aborted due to stage failure: Task 4 in stage 12.0 failed 4 times, most recent failure: Lost task 4.3 in stage 12.0 (TID 4460, 1
0.1.2.40): java.util.NoSuchElementException: key not found: peakExecutionMemory
        at scala.collection.MapLike$class.default(MapLike.scala:228)
        at scala.collection.AbstractMap.default(Map.scala:58)
        at scala.collection.MapLike$class.apply(MapLike.scala:141)
        at scala.collection.AbstractMap.apply(Map.scala:58)
        at org.apache.spark.util.collection.ExternalSorter.writePartitionedFile(ExternalSorter.scala:699)
        at org.apache.spark.shuffle.sort.SortShuffleWriter.write(SortShuffleWriter.scala:80)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:88)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
{code}


---

* [SPARK-9806](https://issues.apache.org/jira/browse/SPARK-9806) | *Minor* | **Don't share ReplayListenerBus between multiple applications**

Currently, we are sharing {{ReplayListenerBus}} for replaying the event logs of various apps.
https://github.com/apache/spark/blob/v1.4.0/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L226

While replaying the event log for a particular app, we add an {{ApplicationEventListener}} to the bus.
https://github.com/apache/spark/blob/v1.4.0/core/src/main/scala/org/apache/spark/deploy/history/FsHistoryProvider.scala#L391
But we never remove it.

This results in one {{ReplayListenerBus}} being associated with multiple {{ApplicationEventListener}}:
{code}
15/08/11 00:04:00 log-replay-executor INFO FsHistoryProvider: Replaying log path: hdfs://localhost:9000/spark-history/application\_1438993108319\_0146\_1.snappy
15/08/11 00:04:01 log-replay-executor INFO ApplicationEventListener: onEnvUpdate
15/08/11 00:04:01 log-replay-executor INFO ApplicationEventListener: onApplicationStart
15/08/11 00:04:40 log-replay-executor INFO ApplicationEventListener: onApplicationEnd
15/08/11 00:04:40 log-replay-executor INFO FsHistoryProvider: Elapsed time: 39.730114407s
15/08/11 00:04:40 log-replay-executor INFO FsHistoryProvider: Application log application\_1438993108319\_0146\_1.snappy loaded successfully.
15/08/11 00:04:40 log-replay-executor INFO FsHistoryProvider: Replaying log path: hdfs://localhost:9000/spark-history/application\_1438993108319\_0126\_1.snappy
15/08/11 00:04:40 log-replay-executor INFO ApplicationEventListener: onEnvUpdate
15/08/11 00:04:40 log-replay-executor INFO ApplicationEventListener: onEnvUpdate
15/08/11 00:04:40 log-replay-executor INFO ApplicationEventListener: onApplicationStart
15/08/11 00:04:40 log-replay-executor INFO ApplicationEventListener: onApplicationStart
15/08/11 00:05:00 log-replay-executor INFO ApplicationEventListener: onApplicationEnd
15/08/11 00:05:00 log-replay-executor INFO ApplicationEventListener: onApplicationEnd
15/08/11 00:05:00 log-replay-executor INFO FsHistoryProvider: Elapsed time: 20.483128154s
15/08/11 00:05:00 log-replay-executor INFO FsHistoryProvider: Application log application\_1438993108319\_0126\_1.snappy loaded successfully.
15/08/11 00:05:00 log-replay-executor INFO FsHistoryProvider: Replaying log path: hdfs://localhost:9000/spark-history/application\_1438993108319\_0116\_1.snappy
15/08/11 00:05:00 log-replay-executor INFO ApplicationEventListener: onEnvUpdate
15/08/11 00:05:00 log-replay-executor INFO ApplicationEventListener: onEnvUpdate
15/08/11 00:05:00 log-replay-executor INFO ApplicationEventListener: onEnvUpdate
15/08/11 00:05:00 log-replay-executor INFO ApplicationEventListener: onApplicationStart
15/08/11 00:05:00 log-replay-executor INFO ApplicationEventListener: onApplicationStart
15/08/11 00:05:00 log-replay-executor INFO ApplicationEventListener: onApplicationStart
15/08/11 00:05:29 log-replay-executor INFO ApplicationEventListener: onApplicationEnd
15/08/11 00:05:29 log-replay-executor INFO ApplicationEventListener: onApplicationEnd
15/08/11 00:05:29 log-replay-executor INFO ApplicationEventListener: onApplicationEnd
15/08/11 00:05:29 log-replay-executor INFO FsHistoryProvider: Elapsed time: 29.110070845s
15/08/11 00:05:29 log-replay-executor INFO FsHistoryProvider: Application log application\_1438993108319\_0116\_1.snappy loaded successfully.
{code}

We should either remove the listener from the bus or create a new bus for each app.


---

* [SPARK-9805](https://issues.apache.org/jira/browse/SPARK-9805) | *Major* | **Make streaming PySpark ML tests more robust using termination conditions**

Recently, PySpark ML streaming tests have been flaky, most likely because of the batches not being processed in time.  Proposal: Replace the use of \_ssc\_wait (which waits for a fixed amount of time) with a method which waits for a fixed amount of time but can terminate early based on a termination condition method.  With this, we can extend the waiting period (to make tests less flaky) but also stop early when possible (making tests faster on average).


---

* [SPARK-9804](https://issues.apache.org/jira/browse/SPARK-9804) | *Major* | **"isSrcLocal" parameter in loadTable / loadPartition is incorrect for HDFS source data**

The shims for Hive \>= 0.14 hardcode the value of the {{isSrcLocal}} parameter to true. If the source data is not actually local, you get errors like this:

{noformat}
Exception in thread "main" java.lang.IllegalArgumentException: Wrong FS: hdfs://vanzin-st1-1.vpc.cloudera.com:8020/user/hive/warehouse/spark\_hive.db/src/.hive-staging\_hive\_2015-08-10\_15-20-28\_215\_840551940044534110-1/-ext-10000/part-00000, expected: file:///
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:648)
        at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:80)
        at org.apache.hadoop.fs.RawLocalFileSystem.deprecatedGetFileStatus(RawLocalFileSystem.java:529)
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileLinkStatusInternal(RawLocalFileSystem.java:747)
        at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:524)
        at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:409)
        at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:340)
        at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1908)
        at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1876)
        at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1841)
        at org.apache.hadoop.hive.ql.metadata.Hive.moveFile(Hive.java:2517)
        at org.apache.hadoop.hive.ql.metadata.Hive.copyFiles(Hive.java:2589)
        at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1395)
        at org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Hive.java:1319)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.sql.hive.client.Shim\_v0\_14.loadPartition(HiveShim.scala:430)
        at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$loadPartition$1.apply$mcV$sp(ClientWrapper.scala:473)
...
{noformat}

This can be triggered by running a query like the following:

{code}
INSERT INTO TABLE blah PARTITION(key=value) SELECT ...;
{code}

Where "key=value" is a new partition being added to the existing table.


---

* [SPARK-9801](https://issues.apache.org/jira/browse/SPARK-9801) | *Minor* | **Spark streaming deletes the temp file and backup files without checking if they exist or not**

For spark streaming, when checkpoint is happening, it is getting below error message from spark driver log: 

{code}
15/07/29 11:04:50 INFO CheckpointWriter: Saving checkpoint for time 1438135490000 ms to file 'maprfs:/user/mapr/spark-checkpoint2/checkpoint-1438135490000' 
15/07/29 11:04:50 ERROR MapRFileSystem: Failed to delete path maprfs:/user/mapr/spark-checkpoint2/temp, error: No such file or directory (2) 
15/07/29 11:04:50 ERROR MapRFileSystem: Failed to delete path maprfs:/user/mapr/spark-checkpoint2/checkpoint-1438135490000.bk, error: No such file or directory (2) 
15/07/29 11:04:50 INFO CheckpointWriter: Deleting maprfs:///user/mapr/spark-checkpoint2/checkpoint-1438135480000 
15/07/29 11:04:50 INFO CheckpointWriter: Checkpoint for time 1438135490000 ms saved to file 'maprfs:/user/mapr/spark-checkpoint2/checkpoint-1438135490000', took 8729 bytes and 14 ms 
15/07/29 11:04:50 INFO DStreamGraph: Clearing checkpoint data for time 1438135490000 ms 
15/07/29 11:04:50 INFO DStreamGraph: Cleared checkpoint data for time 1438135490000 ms
{code}

From the source code : 
https://github.com/apache/spark/blob/master/streaming/src/main/scala/org/apache/spark/streaming/Checkpoint.scala

When Spark tries to delete the 2 files, it did not check if the 2 files exist or not. 
fs.delete(tempFile, true) // just in case it exists 
fs.delete(backupFile, true) // just in case it exists

We should add the logic to check if the files exist or not before deleting.


---

* [SPARK-9800](https://issues.apache.org/jira/browse/SPARK-9800) | *Minor* | **GradientDescent$.runMiniBatchSGD docs**

GradientDescent$.runMiniBatchSGD currently has two overloads, one of which uses a default convergence tolerance which is not documented.

It should either use a default argument (rather than overloading) or specify the default convergence tolerance in the method overload's documentation.


---

* [SPARK-9797](https://issues.apache.org/jira/browse/SPARK-9797) | *Trivial* | **Document StreamingLinearRegressionWithSGD default parameter values**

Document default values for StreamingLinearRegressionWithSGD's 
 \* setConvergenceTol


---

* [SPARK-9795](https://issues.apache.org/jira/browse/SPARK-9795) | *Critical* | **Dynamic allocation: avoid double counting when killing same executor twice**

Currently, if we kill the same executor twice in rapid succession, we will lower the executor target by 2 instead of 1. In cases where we don't re-adjust the target upwards frequently, this will result in jobs hanging.

This may or may not be the same as SPARK-9745. Until we can verify the correlation, however, this will remain a separate issue.


---

* [SPARK-9789](https://issues.apache.org/jira/browse/SPARK-9789) | *Major* | **Reinstate LogisticRegression threshold Param**

From [SPARK-9658]:

LogisticRegression.threshold was replaced by thresholds, but we could keep "threshold" for backwards compatibility.  We should add it back, but we should maintain the current semantics whereby "thresholds" overrides "threshold."


---

* [SPARK-9788](https://issues.apache.org/jira/browse/SPARK-9788) | *Major* | **LDA docConcentration, gammaShape 1.5 binary incompatibility fixes**

From [SPARK-9658]:

1. LDA.docConcentration

It will be nice to keep the old APIs unchanged.  Proposal:
\* Add asymmetricDocConcentration and revert docConcentration changes.
\* If the (internal) doc concentration vector is a single value, getDocConcentration" returns it.  If it is a constant vector, getDocConcentration returns the first item, and fails otherwise.

2. LDAModel.gammaShape

This should be given a default value.


---

* [SPARK-9785](https://issues.apache.org/jira/browse/SPARK-9785) | *Blocker* | **HashPartitioning compatibility should consider expression ordering**

HashPartitioning compatibility is defined w.r.t the \_set\_ of expressions, but in other contexts the ordering of those expressions matters.  This is illustrated by the following regression test:

{code}
  test("HashPartitioning compatibility") {
    val expressions = Seq(Literal(2), Literal(3))
    // Consider two HashPartitionings that have the same \_set\_ of hash expressions but which are
    // created with different orderings of those expressions:
    val partitioningA = HashPartitioning(expressions, 100)
    val partitioningB = HashPartitioning(expressions.reverse, 100)
    // These partitionings are not considered equal:
    assert(partitioningA != partitioningB)
    // However, they both satisfy the same clustered distribution:
    val distribution = ClusteredDistribution(expressions)
    assert(partitioningA.satisfies(distribution))
    assert(partitioningB.satisfies(distribution))
    // Both partitionings are compatible with and guarantee each other:
    assert(partitioningA.compatibleWith(partitioningB))
    assert(partitioningB.compatibleWith(partitioningA))
    assert(partitioningA.guarantees(partitioningB))
    assert(partitioningB.guarantees(partitioningA))
    // Given all of this, we would expect these partitionings to compute the same hashcode for
    // any given row:
    def computeHashCode(partitioning: HashPartitioning): Int = {
      val hashExprProj = new InterpretedMutableProjection(partitioning.expressions, Seq.empty)
      hashExprProj.apply(InternalRow.empty).hashCode()
    }
    assert(computeHashCode(partitioningA) === computeHashCode(partitioningB))
  }
{code}


---

* [SPARK-9784](https://issues.apache.org/jira/browse/SPARK-9784) | *Blocker* | **Exchange.isUnsafe should check whether codegen and unsafe are enabled**

Exchange needs to check whether unsafe mode is enabled in its {{tungstenMode}} method:

{code}

  override def nodeName: String = if (tungstenMode) "TungstenExchange" else "Exchange"

  /\*\*
   \* Returns true iff we can support the data type, and we are not doing range partitioning.
   \*/
  private lazy val tungstenMode: Boolean = {
    GenerateUnsafeProjection.canSupport(child.schema) &&
      !newPartitioning.isInstanceOf[RangePartitioning]
  }
{code}


---

* [SPARK-9780](https://issues.apache.org/jira/browse/SPARK-9780) | *Minor* | **In case of invalid initialization of KafkaDirectStream, NPE is thrown**

[o.a.s.streaming.kafka.KafkaRDD.scala#L143\|https://github.com/apache/spark/blob/master/external/kafka/src/main/scala/org/apache/spark/streaming/kafka/KafkaRDD.scala#L143]
In initialization of KafkaRDDIterator, there is an addition of TaskCompletionListener to the context, which calls close() to the consumer, which is not initialized yet (and will be initialized 12 lines after that).
If something happens in this 12 lines (in my case there was a private constructor for valueDecoder), an Exception, which is thrown, triggers context.markTaskCompleted() in
[o.a.s.scheduler.Task.scala#L90\|https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/scheduler/Task.scala#L90]
which throws NullPointerException, when tries to call close() for non-initialized consumer.
This masks original exception - so it is very hard to understand, what is happening.


---

* [SPARK-9777](https://issues.apache.org/jira/browse/SPARK-9777) | *Major* | **Window operator can accept UnsafeRows**

We can set {{  override def canProcessUnsafeRows: Boolean = true}} in {{Window}} operator since it does not care if its input rows are UnsafeRows or SafeRows.


---

* [SPARK-9768](https://issues.apache.org/jira/browse/SPARK-9768) | *Minor* | **Add Python API for ml.feature.ElementwiseProduct**

Add Python API, user guide and example for ml.feature.ElementwiseProduct.


---

* [SPARK-9766](https://issues.apache.org/jira/browse/SPARK-9766) | *Major* | **check and add missing docs for PySpark ML**

Check and add miss docs for PySpark ML (#this issue only check miss docs for o.a.s.ml not o.a.s.mllib).


---

* [SPARK-9760](https://issues.apache.org/jira/browse/SPARK-9760) | *Blocker* | **SparkSubmit doesn't work with --packages when --repositories is not specified**

Running `./bin/sparkR --packages com.databricks:spark-csv\_2.10:1.2.0` gives

{code}
Exception in thread "main" java.lang.NullPointerException
        at org.apache.spark.deploy.SparkSubmitUtils$.createRepoResolvers(SparkSubmit.scala:812)
        at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:962)
        at org.apache.spark.deploy.SparkSubmit$.prepareSubmitEnvironment(SparkSubmit.scala:286)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:153)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:120)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}


---

* [SPARK-9759](https://issues.apache.org/jira/browse/SPARK-9759) | *Major* | **Improve performance of Decimal.times() and casting from integral**

see discussion here: https://github.com/apache/spark/pull/8018#issuecomment-129044057


---

* [SPARK-9758](https://issues.apache.org/jira/browse/SPARK-9758) | *Trivial* | **Compilation issue for hive test**

There is compilation issue in hive test package.

hive-test directory structure is as below
src/test/java
  package- test/org/apache/spark/\*/\*
Here package starts with test/org/apache/spark/\*/\* but in code
package structure doesn't start with test.

Affected files 
files under below folder hive\src\test\java\test\org\apache\spark\sql\hive\execution\\*
hive\src\test\scala\org\apache\spark\sql\hive\execution\HiveUdfSuite.scala


---

* [SPARK-9757](https://issues.apache.org/jira/browse/SPARK-9757) | *Blocker* | **Can't create persistent data source tables with decimal**

{{ParquetHiveSerDe}} in Hive versions \< 1.2.0 doesn't support decimal. Persisting Parquet relations to metastore of such versions (say 0.13.1) throws the following exception after SPARK-6923.
{code}
Caused by: java.lang.UnsupportedOperationException: Parquet does not support decimal. See HIVE-6384
	at org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.getObjectInspector(ArrayWritableObjectInspector.java:102)
	at org.apache.hadoop.hive.ql.io.parquet.serde.ArrayWritableObjectInspector.\<init\>(ArrayWritableObjectInspector.java:60)
	at org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe.initialize(ParquetHiveSerDe.java:113)
	at org.apache.hadoop.hive.metastore.MetaStoreUtils.getDeserializer(MetaStoreUtils.java:339)
	at org.apache.hadoop.hive.ql.metadata.Table.getDeserializerFromMetaStore(Table.java:288)
	at org.apache.hadoop.hive.ql.metadata.Table.checkValidity(Table.java:194)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:597)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:576)
	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$createTable$1.apply$mcV$sp(ClientWrapper.scala:358)
	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$createTable$1.apply(ClientWrapper.scala:356)
	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$createTable$1.apply(ClientWrapper.scala:356)
	at org.apache.spark.sql.hive.client.ClientWrapper$$anonfun$withHiveState$1.apply(ClientWrapper.scala:256)
	at org.apache.spark.sql.hive.client.ClientWrapper.retryLocked(ClientWrapper.scala:211)
	at org.apache.spark.sql.hive.client.ClientWrapper.withHiveState(ClientWrapper.scala:248)
	at org.apache.spark.sql.hive.client.ClientWrapper.createTable(ClientWrapper.scala:356)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.createDataSourceTable(HiveMetastoreCatalog.scala:351)
	at org.apache.spark.sql.hive.HiveMetastoreCatalog.createDataSourceTable(HiveMetastoreCatalog.scala:198)
	at org.apache.spark.sql.hive.execution.CreateMetastoreDataSource.run(commands.scala:152)
{code}


---

* [SPARK-9756](https://issues.apache.org/jira/browse/SPARK-9756) | *Minor* | **Make auxillary constructors for ML decision trees private**

These classes should not (and actually can not) be instantiated directly because there is currently no public constructor for {{Node}}.


---

* [SPARK-9755](https://issues.apache.org/jira/browse/SPARK-9755) | *Minor* | **Add method documentation to MultivariateOnlineSummarizer**

Docs present in 1.4 are lost in current 1.5 branch.


---

* [SPARK-9754](https://issues.apache.org/jira/browse/SPARK-9754) | *Major* | **Remove TypeCheck in debug package**

TypeCheck no longer applies in the new Tungsten world.


---

* [SPARK-9753](https://issues.apache.org/jira/browse/SPARK-9753) | *Blocker* | **TungstenAggregate should also accept InternalRow instead of just UnsafeRow**

Since we need to project out key and value out, there is no need to only accept UnsafeRows.


---

* [SPARK-9751](https://issues.apache.org/jira/browse/SPARK-9751) | *Major* | **Audit operators to make sure they can support UnsafeRows**

An umbrella ticket to track various operators that should be able to support UnsafeRow to avoid copying.


---

* [SPARK-9750](https://issues.apache.org/jira/browse/SPARK-9750) | *Critical* | **SparseMatrix should override equals**

[SparseMatrix\|https://github.com/apache/spark/blob/9897cc5e3d6c70f7e45e887e2c6fc24dfa1adada/mllib/src/main/scala/org/apache/spark/mllib/linalg/Matrices.scala#L479] should override equals to ensure that two instances of the same matrix are equal.

This implementation should take into account the {{isTransposed}} flag and {{values}} may not be in the same order.


---

* [SPARK-9747](https://issues.apache.org/jira/browse/SPARK-9747) | *Blocker* | **Avoid starving an unsafe operator in an aggregate**

This mainly concerns TungstenAggregate.


---

* [SPARK-9745](https://issues.apache.org/jira/browse/SPARK-9745) | *Blocker* | **Applications hangs when the last executor fails with dynamic allocation**

When a job has only a single executor remaining and that executor dies (due to something like an OOM), the application fails to notice that there are no executors left and it hangs indefinitely.

This only happens when dynamic allocation is enabled.

The following images were taken from a hung application with no executors:

!logs\_hung\_job.png!

^^ \*Notice how 1 executor was lost, but the application never requested it to be removed\*





!am\_hung\_job.png!

!executors\_hung\_job.png!

!tasks\_hung\_job.png!


---

* [SPARK-9743](https://issues.apache.org/jira/browse/SPARK-9743) | *Blocker* | **Scanning a HadoopFsRelation shouldn't requrire refreshing**

PR #7969 added {{HadoopFsRelation.refresh()}} calls in {{DataSourceStrategy}} to make test case {{InsertSuite.save directly to the path of a JSON table}} pass. However, this forces every {{HadoopFsRelation}} table scan to do a refreshing, which can be super expensive for tables with large number of partitions.


---

* [SPARK-9738](https://issues.apache.org/jira/browse/SPARK-9738) | *Major* | **remove FromUnsafe and add its codegen version to GenerateSafe**

In https://github.com/apache/spark/pull/7752 we added `FromUnsafe` to convert nexted unsafe data like array/map/struct to safe versions. It's a quick solution and we already have `GenerateSafe` to do the conversion which is codegened. So we should remove `FromUnsafe` and implement its codegen version in `GenerateSafe`.


---

* [SPARK-9736](https://issues.apache.org/jira/browse/SPARK-9736) | *Major* | **JoinedRow.anyNull should delegate to the underlying rows**

JoinedRow.anyNull currently loops through every field to check for null, which is inefficient if the underlying rows are UnsafeRows. It should just delegate to the underlying implementation.


---

* [SPARK-9734](https://issues.apache.org/jira/browse/SPARK-9734) | *Major* | **java.lang.IllegalArgumentException: Don't know how to save StructField(sal,DecimalType(7,2),true) to JDBC**

When using a basic example of reading the EMP table from Redshift via spark-redshift, and writing the data back to Redshift, Spark fails with the below error, related to Numeric/Decimal data types.

Redshift table:

{code}
testdb=# \d emp
              Table "public.emp"
  Column  \|         Type          \| Modifiers
----------+-----------------------+-----------
 empno    \| integer               \|
 ename    \| character varying(10) \|
 job      \| character varying(9)  \|
 mgr      \| integer               \|
 hiredate \| date                  \|
 sal      \| numeric(7,2)          \|
 comm     \| numeric(7,2)          \|
 deptno   \| integer               \|

testdb=# select \* from emp;
 empno \| ename  \|    job    \| mgr  \|  hiredate  \|   sal   \|  comm   \| deptno
-------+--------+-----------+------+------------+---------+---------+--------
  7369 \| SMITH  \| CLERK     \| 7902 \| 1980-12-17 \|  800.00 \|    NULL \|     20
  7521 \| WARD   \| SALESMAN  \| 7698 \| 1981-02-22 \| 1250.00 \|  500.00 \|     30
  7654 \| MARTIN \| SALESMAN  \| 7698 \| 1981-09-28 \| 1250.00 \| 1400.00 \|     30
  7782 \| CLARK  \| MANAGER   \| 7839 \| 1981-06-09 \| 2450.00 \|    NULL \|     10
  7839 \| KING   \| PRESIDENT \| NULL \| 1981-11-17 \| 5000.00 \|    NULL \|     10
  7876 \| ADAMS  \| CLERK     \| 7788 \| 1983-01-12 \| 1100.00 \|    NULL \|     20
  7902 \| FORD   \| ANALYST   \| 7566 \| 1981-12-03 \| 3000.00 \|    NULL \|     20
  7499 \| ALLEN  \| SALESMAN  \| 7698 \| 1981-02-20 \| 1600.00 \|  300.00 \|     30
  7566 \| JONES  \| MANAGER   \| 7839 \| 1981-04-02 \| 2975.00 \|    NULL \|     20
  7698 \| BLAKE  \| MANAGER   \| 7839 \| 1981-05-01 \| 2850.00 \|    NULL \|     30
  7788 \| SCOTT  \| ANALYST   \| 7566 \| 1982-12-09 \| 3000.00 \|    NULL \|     20
  7844 \| TURNER \| SALESMAN  \| 7698 \| 1981-09-08 \| 1500.00 \|    0.00 \|     30
  7900 \| JAMES  \| CLERK     \| 7698 \| 1981-12-03 \|  950.00 \|    NULL \|     30
  7934 \| MILLER \| CLERK     \| 7782 \| 1982-01-23 \| 1300.00 \|    NULL \|     10
(14 rows)
{code}

Spark Code:

{code}
val url = "jdbc:redshift://rshost:5439/testdb?user=xxx&password=xxx"
val driver = "com.amazon.redshift.jdbc41.Driver"
val t = sqlContext.read.format("com.databricks.spark.redshift").option("jdbcdriver", driver).option("url", url).option("dbtable", "emp").option("tempdir", "s3n://spark-temp-dir").load()
t.registerTempTable("SparkTempTable")
val t1 = sqlContext.sql("select \* from SparkTempTable")
t1.write.format("com.databricks.spark.redshift").option("driver", driver).option("url", url).option("dbtable", "t1").option("tempdir", "s3n://spark-temp-dir").option("avrocompression", "snappy").mode("error").save()
{code}

Error Stack:

{code}
java.lang.IllegalArgumentException: Don't know how to save StructField(sal,DecimalType(7,2),true) to JDBC
	at org.apache.spark.sql.jdbc.package$JDBCWriteDetails$$anonfun$schemaString$1$$anonfun$2.apply(jdbc.scala:149)
	at org.apache.spark.sql.jdbc.package$JDBCWriteDetails$$anonfun$schemaString$1$$anonfun$2.apply(jdbc.scala:136)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.jdbc.package$JDBCWriteDetails$$anonfun$schemaString$1.apply(jdbc.scala:135)
	at org.apache.spark.sql.jdbc.package$JDBCWriteDetails$$anonfun$schemaString$1.apply(jdbc.scala:132)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.sql.jdbc.package$JDBCWriteDetails$.schemaString(jdbc.scala:132)
	at org.apache.spark.sql.jdbc.JDBCWrapper.schemaString(RedshiftJDBCWrapper.scala:28)
	at com.databricks.spark.redshift.RedshiftWriter.createTableSql(RedshiftWriter.scala:39)
	at com.databricks.spark.redshift.RedshiftWriter.doRedshiftLoad(RedshiftWriter.scala:105)
	at com.databricks.spark.redshift.RedshiftWriter.saveToRedshift(RedshiftWriter.scala:145)
	at com.databricks.spark.redshift.DefaultSource.createRelation(DefaultSource.scala:92)
	at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:309)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:144)
{code}


---

* [SPARK-9733](https://issues.apache.org/jira/browse/SPARK-9733) | *Major* | **Improve explain message for data source scan node**

All data sources show up as "PhysicalRDD" in physical plan explain. It'd be better if we can show the name of the data source.

Existing:
{code}
== Physical Plan ==
NewAggregate with UnsafeHybridAggregationIterator ArrayBuffer(date#0, cat#1) ArrayBuffer((sum(CAST((CAST(count#2, IntegerType) + 1), LongType))2,mode=Final,isDistinct=false))
 Exchange hashpartitioning(date#0,cat#1)
  NewAggregate with UnsafeHybridAggregationIterator ArrayBuffer(date#0, cat#1) ArrayBuffer((sum(CAST((CAST(count#2, IntegerType) + 1), LongType))2,mode=Partial,isDistinct=false))
   PhysicalRDD [date#0,cat#1,count#2], MapPartitionsRDD[3] at
{code}

Better version:
{code}
== Physical Plan ==
TungstenAggregate(key=[date#0,cat#1], value=[(sum(CAST((CAST(count#2, IntegerType) + 1), LongType)),mode=Final,isDistinct=false)]
 Exchange hashpartitioning(date#0,cat#1)
  TungstenAggregate(key=[date#0,cat#1], value=[(sum(CAST((CAST(count#2, IntegerType) + 1), LongType)),mode=Partial,isDistinct=false)]
   ConvertToUnsafe
    Scan ParquetRelation[file:/scratch/rxin/spark/sales4][date#0,cat#1,count#2]
{code}


---

* [SPARK-9731](https://issues.apache.org/jira/browse/SPARK-9731) | *Blocker* | **Standalone scheduling incorrect cores if spark.executor.cores is not set**

The issue only happens if `spark.executor.cores` is not set and executor memory is set to a high value. 

For example, if we have a worker with 4G and 10 cores and we set `spark.executor.memory` to 3G, then only 1 core is assigned to the executor. The correct number should be 10 cores.


---

* [SPARK-9728](https://issues.apache.org/jira/browse/SPARK-9728) | *Critical* | **Support CalendarIntervalType in HiveQL**

We need to convert an interval term into CalendarIntervalType in Hive QL.


---

* [SPARK-9727](https://issues.apache.org/jira/browse/SPARK-9727) | *Minor* | **Make the Kinesis project SBT name and consistent with other streaming projects**

pom.xml - SBT project name: kinesis-asl ---\> streaming-kinesis-asl
SparkBuild - project name: sparkKinesisAsl ---\> streamingKinesisAsl


---

* [SPARK-9726](https://issues.apache.org/jira/browse/SPARK-9726) | *Major* | **PySpark Regression: DataFrame join no longer accepts None as join expression**

The patch to add methods to support equi-join broke joins where on is None.  Rather than ending the branch  with  jdf = self.\_jdf.join(other.\_jdf)  it continues to another branch where it fails because you cannot take the index of None.    if isinstance(on[0], basestring):

This was valid in 1.4.1:
df3 = df.join(df2,on=None) 

This is a trivial fix in the attached PR.


---

* [SPARK-9725](https://issues.apache.org/jira/browse/SPARK-9725) | *Blocker* | **spark sql query string field return empty/garbled string**

to reproduce it:
1 deploy spark cluster mode, i use standalone mode locally
2 set executor memory \>= 32g, set following config in spark-default.xml
   spark.executor.memory            36g 

3 run spark-sql.sh with "show tables" it return empty/garbled string


---

* [SPARK-9719](https://issues.apache.org/jira/browse/SPARK-9719) | *Minor* | **spark.ml NaiveBayes doc cleanups**

spark.ml NaiveBayesModel: Add Scala and Python doc for pi, theta

Add setParam tag to NaiveBayes setModelType


---

* [SPARK-9713](https://issues.apache.org/jira/browse/SPARK-9713) | *Critical* | **Document SparkR MLlib glm() integration in Spark 1.5**

The new SparkR functions in mllib.R should be documented: glm(), predict(), and summary().


---

* [SPARK-9712](https://issues.apache.org/jira/browse/SPARK-9712) | *Major* | **List source compatibility issues in Scala API from scaladocs**

Generate raw scaladocs and use {{scala/tools/scaladoc-compare}} to show changes to public APIs and documentations. These results are access-modifier aware since they run on the Scala source rather than generated classfiles, but will include documentation changes which may not affect behavior.

Results attached.


---

* [SPARK-9710](https://issues.apache.org/jira/browse/SPARK-9710) | *Major* | **RPackageUtilsSuite fails if R is not installed**

That's because there's a bug in RUtils.scala. PR soon.


---

* [SPARK-9709](https://issues.apache.org/jira/browse/SPARK-9709) | *Blocker* | **Avoid starving an unsafe operator in a sort**

This concerns mainly TungstenSort.


---

* [SPARK-9706](https://issues.apache.org/jira/browse/SPARK-9706) | *Major* | **List Binary and Source Compatibility Issues with japi-compliance checker**

To identify potential API issues, list public API changes which affect binary and source incompatibility by using command:

{code}
japi-compliance-checker spark-mllib\_2.10-1.4.2-SNAPSHOT.jar spark-mllib\_2.10-1.5.0-SNAPSHOT.jar
{code}

Report result attached.


---

* [SPARK-9705](https://issues.apache.org/jira/browse/SPARK-9705) | *Blocker* | **outdated Python 3 and IPython information**

https://issues.apache.org/jira/browse/SPARK-4897 adds Python 3.4 support to 1.4.0 and above, but the official docs (1.4.1, but the same is for 1.4.0) says explicitly:
"Spark 1.4.1 works with Python 2.6 or higher (but not Python 3)."

Affected:
https://spark.apache.org/docs/1.4.0/programming-guide.html
https://spark.apache.org/docs/1.4.1/programming-guide.html

There are some other Python-related things, which are outdated, e.g. this line:
"For example, to launch the IPython Notebook with PyLab plot support:"
(At least since IPython 3.0 PyLab/Matplotlib support happens inside a notebook; and the line "--pylab inline" is already removed.)


---

* [SPARK-9704](https://issues.apache.org/jira/browse/SPARK-9704) | *Major* | **Make some ML APIs public: VectorUDT, Identifiable, ProbabilisticClassifier**

This JIRA is for making several ML APIs public to make it easier for users to write their own Pipeline stages.

Issue brought up by [~eronwright].  Descriptions below copied from [http://apache-spark-developers-list.1001551.n3.nabble.com/Make-ML-Developer-APIs-public-post-1-4-td13583.html].

We plan to make these APIs public in Spark 1.5.  However, they will be marked DeveloperApi and are \*very likely\* to be broken in the future.
\* VectorUDT: To define a relation with a vector field, VectorUDT must be instantiated.
\* Identifiable trait: The trait generates a unique identifier for the associated pipeline component.  Nice to have a consistent format by reusing the trait.
\* ProbabilisticClassifier.  Third-party components should leverage the complex logic around computing only selected columns.

We will not yet make these public:
\* SchemaUtils: Third-party pipeline components have a need for checking column types and appending columns.
\*\* This will probably be moved into Spark SQL.  Users can copy the methods into their own code as needed.
\* Shared Params (HasLabel, HasFeatures): This is covered in [SPARK-7146] but reiterating it here.
\*\* We need to discuss whether these should be standardized public APIs.  Users can copy the traits into their own code as needed.


---

* [SPARK-9703](https://issues.apache.org/jira/browse/SPARK-9703) | *Major* | **EnsureRequirements should not add unnecessary shuffles when only ordering requirements are unsatisfied**

Consider SortMergeJoin, which requires a sorted, clustered distribution of its input rows. Say that both of SMJ's children produce unsorted output but are both single partition. In this case, we will need to inject sort operators but should not need to inject exchanges. Unfortunately, it looks like the Exchange unnecessarily repartitions using a hash partitioning.

We should update Exchange so that it does not unnecessarily repartition children when only the ordering requirements are unsatisfied.

I'd like to fix this for Spark 1.5 since it makes certain types of unit tests easier to write.


---

* [SPARK-9700](https://issues.apache.org/jira/browse/SPARK-9700) | *Blocker* | **Pick default page size more intelligently**

Previously, we use 64MB as the default page size, which was way too big for a lot of Spark applications (especially for single node).

This patch changes it so that the default page size, if unset by the user, is determined by the number of cores available and the total execution memory available.


---

* [SPARK-9693](https://issues.apache.org/jira/browse/SPARK-9693) | *Blocker* | **Reserve a page in all unsafe operators to avoid starving an operator**

E.g. currently we can do up to 3 sorts within a task:

(1) During the aggregation
(2) During a sort on the same key
(3) During the shuffle

In environments with tight memory restrictions, the first operator may acquire so much memory such that the subsequent ones in the same task are starved. A simple fix is to reserve at least a page in advance in each of these places. The reserved page size need not be the same as the normal page size.

This is a sister problem to SPARK-4452 in Spark Core.


---

* [SPARK-9692](https://issues.apache.org/jira/browse/SPARK-9692) | *Major* | **Remove SqlNewHadoopRDD's generated Tuple2 and InterruptibleIterator**

A small performance optimization -- we don't need to generate a Tuple2 and then immediately discard the key. We also don't need an extra wrapper.


---

* [SPARK-9691](https://issues.apache.org/jira/browse/SPARK-9691) | *Major* | **PySpark SQL rand function treats seed 0 as no seed**

In PySpark SQL's rand() function, it tests for a seed in a way such that seed 0 is treated as no seed, leading to non-deterministic results when a user would expect deterministic results.

See: [https://github.com/apache/spark/blob/98e69467d4fda2c26a951409b5b7c6f1e9345ce4/python/pyspark/sql/functions.py#L271]


---

* [SPARK-9677](https://issues.apache.org/jira/browse/SPARK-9677) | *Blocker* | **Enable SQLQuerySuite."aggregation with codegen updates peak execution memory"**

It was disabled in https://github.com/apache/spark/pull/7983

Looked like the test case was written against the old aggregate. We need to rewrite it to work for the new aggregate (and make sure the memory usage reporting works for the new aggregate).


---

* [SPARK-9674](https://issues.apache.org/jira/browse/SPARK-9674) | *Major* | **Remove GeneratedAggregate**

It is subsumed by the new aggregate implementation.


---

* [SPARK-9671](https://issues.apache.org/jira/browse/SPARK-9671) | *Critical* | **ML 1.5 QA: Programming guide update and migration guide**

Before the release, we need to update the MLlib Programming Guide.  Updates will include:
\* Add migration guide subsection.
\*\* Use the results of the QA audit JIRAs.
\* Check phrasing, especially in main sections (for outdated items such as "In this release, ...")
\* Possibly reorganize parts of the Pipelines guide if needed.


---

* [SPARK-9667](https://issues.apache.org/jira/browse/SPARK-9667) | *Major* | **Remove SparkSqlSerializer2 in favor of Unsafe exchange**

GenerateUnsafeProjection can be used directly as a code generated serializer. We no longer need SparkSqlSerializer2.


---

* [SPARK-9666](https://issues.apache.org/jira/browse/SPARK-9666) | *Major* | **ML 1.5 QA: model save/load audit**

We should check to make sure no changes broke model import/export in spark.mllib.
\* If a model's name, data members, or constructors have changed \_at all\_, then we likely need to support a new save/load format version.  Different versions must be tested in unit tests to ensure backwards compatibility (i.e., verify we can load old model formats).
\* Examples in the programming guide should include save/load when available.  It's important to try running each example in the guide whenever it is modified (since there are no automated tests).


---

* [SPARK-9665](https://issues.apache.org/jira/browse/SPARK-9665) | *Critical* | **ML 1.5 QA: API: Experimental, DeveloperApi, final, sealed audit**

We should make a pass through the items marked as Experimental or DeveloperApi and see if any are stable enough to be unmarked.  This will probably not include the Pipeline APIs yet since some parts (e.g., feature attributes) are still under flux.

We should also check for items marked final or sealed to see if they are stable enough to be opened up as APIs.


---

* [SPARK-9664](https://issues.apache.org/jira/browse/SPARK-9664) | *Blocker* | **Use sqlContext.udf to register UDAFs.**

Right now, we have a UDAFRegistration and users need to use sqlContext.udaf to register a UDAF. It's better to just use sqlContext.udf.


---

* [SPARK-9661](https://issues.apache.org/jira/browse/SPARK-9661) | *Major* | **ML 1.5 QA: API: Java compatibility, docs**

Check Java compatibility for MLlib for this release.

Checking compatibility means:
\* comparing with the Scala doc
\* verifying that Java docs are not messed up by Scala type incompatibilities.  Some items to look out for are:
\*\* Check for generic "Object" types where Java cannot understand complex Scala types.
\*\*\* \*Note\*: The Java docs do not always match the bytecode. If you find a problem, please verify it using {{javap}}.
\*\* Check Scala objects (especially with nesting!) carefully.
\*\* Check for uses of Scala and Java enumerations, which can show up oddly in the other language's doc.
\* If needed for complex issues, create small Java unit tests which execute each method.  (The correctness can be checked in Scala.)

If you find issues, please comment here, or for larger items, create separate JIRAs and link here.
Note that we should not break APIs from previous releases.  So if you find a problem, check if it was introduced in this Spark release (in which case we can fix it) or in a previous one (in which case we can create a java-friendly version of the API).


---

* [SPARK-9660](https://issues.apache.org/jira/browse/SPARK-9660) | *Major* | **ML 1.5 QA: API: New Scala APIs, docs**

Audit new public Scala APIs added to MLlib.  Take note of:
\* Protected/public classes or methods.  If access can be more private, then it should be.
\* Also look for non-sealed traits.
\* Documentation: Missing?  Bad links or formatting?

\*Make sure to check the object doc!\*

As you find issues, please comment here, or better yet create JIRAs and link them.


---

* [SPARK-9659](https://issues.apache.org/jira/browse/SPARK-9659) | *Major* | **Rename inSet to isin to match Pandas function**

And deprecate the old function(s).

Inspiration drawn from this blog post: https://lab.getbase.com/pandarize-spark-dataframes/


---

* [SPARK-9651](https://issues.apache.org/jira/browse/SPARK-9651) | *Major* | **UnsafeExternalSorterSuite is broken**

{{spillingOccursInResponseToMemoryPressure}} is broken, but passes depending on the order in which tests are run, because of another bug in {{setUp}}. Fix coming up soon.


---

* [SPARK-9650](https://issues.apache.org/jira/browse/SPARK-9650) | *Critical* | **Inconsistent quoting behavior for columns in scala**

{{df("`name.with.dots`")}} works but {{$"`name.with.dots"}} does not


---

* [SPARK-9649](https://issues.apache.org/jira/browse/SPARK-9649) | *Critical* | **Flaky test: o.a.s.deploy.master.MasterSuite: recovery**

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven/AMPLAB\_JENKINS\_BUILD\_PROFILE=hadoop1.0,label=centos/169/testReport/junit/org.apache.spark.deploy.master/MasterSuite/can\_use\_a\_custom\_recovery\_mode\_factory/

This is failing due to port contention issues. We just need to randomize the ports.


---

* [SPARK-9646](https://issues.apache.org/jira/browse/SPARK-9646) | *Major* | **Metrics SQL/DataFrame query plans for Spark 1.5**

We now have the infrastructure. We should find the list of operators and come up with what to instrument and report. See attached link to a Google Doc for the list.


---

* [SPARK-9645](https://issues.apache.org/jira/browse/SPARK-9645) | *Blocker* | **External shuffle service does not work with kerberos on**

Lots of errors like this when running apps with the external shuffle service and kerberos enabled:

{noformat}
15/08/05 06:26:18 WARN TaskSetManager: Lost task 2.0 in stage 2.0 (TID 12, spark-nightly-2.vpc.cloudera.com): FetchFailed(BlockManagerId(2, spark-nightly-2.vpc.cloudera.com, 7337), shuffleId=0, mapId=0, reduceId=2, message=
org.apache.spark.shuffle.FetchFailedException: java.lang.RuntimeException: Failed to open file: /yarn/nm/usercache/systest/appcache/application\_1438780049118\_0008/blockmgr-7178b106-6902-4082-8792-1c3e34b80d15/38/shuffle\_0\_0\_0.index
	at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getSortBasedShuffleBlockData(ExternalShuffleBlockResolver.java:203)
	at org.apache.spark.network.shuffle.ExternalShuffleBlockResolver.getBlockData(ExternalShuffleBlockResolver.java:113)
	at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.handleMessage(ExternalShuffleBlockHandler.java:80)
	at org.apache.spark.network.shuffle.ExternalShuffleBlockHandler.receive(ExternalShuffleBlockHandler.java:68)
	at org.apache.spark.network.server.TransportRequestHandler.processRpcRequest(TransportRequestHandler.java:114)
{noformat}

This is caused by commit c4830598 (SPARK-6287), which modified the permissions of the directory storing the shuffle files.


---

* [SPARK-9644](https://issues.apache.org/jira/browse/SPARK-9644) | *Critical* | **Support update DecimalType with precision \> 18 in UnsafeRow**

Currently, we don't support using DecimalType with precision \> 18 in new unsafe aggregation, it's good to support it.


---

* [SPARK-9641](https://issues.apache.org/jira/browse/SPARK-9641) | *Minor* | **spark.shuffle.service.port is not documented**

Looking at the code I see spark.shuffle.service.port being used but I can't find any documentation on it.   I don't see a reason for this to be an internal config so we should document it.


---

* [SPARK-9639](https://issues.apache.org/jira/browse/SPARK-9639) | *Major* | **JobHandler may throw NPE if JobScheduler has been stopped**

Because `JobScheduler.stop(false)` may set `eventLoop` to null when `JobHandler` is running, then it's possible that when `post` is called, `eventLoop` happens to null.


---

* [SPARK-9634](https://issues.apache.org/jira/browse/SPARK-9634) | *Major* | **cleanup unnecessary Aliases in LogicalPlan at the end of analysis**

Also alias the ExtractValue instead of wrapping it with UnresolvedAlias when resolve attribute in LogicalPlan, as this alias will be trimmed if it's unnecessary.


---

* [SPARK-9633](https://issues.apache.org/jira/browse/SPARK-9633) | *Minor* | **SBT download locations outdated; need an update**

The SBT download script tries to download from two locations, typesafe.artifactoryonline.com and repo.typesafe.com. The former is offline; the latter redirects to dl.bintray.com now. In fact, bintray seems like the only place to download SBT at this point. We should update to reference bintray directly.

PS: we should download SBT over HTTPS too, not HTTP


---

* [SPARK-9630](https://issues.apache.org/jira/browse/SPARK-9630) | *Blocker* | **Cleanup Hybrid Aggregate Operator.**

This is the follow-up of SPARK-9240 to address review comments and clean up code.


---

* [SPARK-9627](https://issues.apache.org/jira/browse/SPARK-9627) | *Blocker* | **SQL job failed if the dataframe with string columns is cached**

{code}
r = random.Random()
def gen(i):
    d = date.today() - timedelta(r.randint(0, 5000))
    cat = str(r.randint(0, 20)) \* 5
    c = r.randint(0, 1000)
    price = decimal.Decimal(r.randint(0, 100000)) / 100
    return (d, cat, c, price)

schema = StructType().add('date', DateType()).add('cat', StringType()).add('count', ShortType()).add('price', DecimalType(5, 2))

#df = sqlContext.createDataFrame(sc.range(1\<\<24).map(gen), schema)
#df.show()
#df.write.parquet('sales4')


df = sqlContext.read.parquet('sales4')
df.cache()
df.count()
df.show()
print df.schema
raw\_input()
r = df.groupBy(df.date, df.cat).agg(sum(df['count'] \* df.price))
print r.explain(True)
r.show()
{code}

{code}
StructType(List(StructField(date,DateType,true),StructField(cat,StringType,true),StructField(count,ShortType,true),StructField(price,DecimalType(5,2),true)))


== Parsed Logical Plan ==
'Aggregate [date#0,cat#1], [date#0,cat#1,sum((count#2 \* price#3)) AS sum((count \* price))#70]
 Relation[date#0,cat#1,count#2,price#3] org.apache.spark.sql.parquet.ParquetRelation@5ec8f315

== Analyzed Logical Plan ==
date: date, cat: string, sum((count \* price)): decimal(21,2)
Aggregate [date#0,cat#1], [date#0,cat#1,sum((change\_decimal\_precision(CAST(CAST(count#2, DecimalType(5,0)), DecimalType(11,2))) \* change\_decimal\_precision(CAST(price#3, DecimalType(11,2))))) AS sum((count \* price))#70]
 Relation[date#0,cat#1,count#2,price#3] org.apache.spark.sql.parquet.ParquetRelation@5ec8f315

== Optimized Logical Plan ==
Aggregate [date#0,cat#1], [date#0,cat#1,sum((change\_decimal\_precision(CAST(CAST(count#2, DecimalType(5,0)), DecimalType(11,2))) \* change\_decimal\_precision(CAST(price#3, DecimalType(11,2))))) AS sum((count \* price))#70]
 InMemoryRelation [date#0,cat#1,count#2,price#3], true, 10000, StorageLevel(true, true, false, true, 1), (PhysicalRDD [date#0,cat#1,count#2,price#3], MapPartitionsRDD[3] at), None

== Physical Plan ==
NewAggregate with SortBasedAggregationIterator List(date#0, cat#1) ArrayBuffer((sum((change\_decimal\_precision(CAST(CAST(count#2, DecimalType(5,0)), DecimalType(11,2))) \* change\_decimal\_precision(CAST(price#3, DecimalType(11,2)))))2,mode=Final,isDistinct=false))
 TungstenSort [date#0 ASC,cat#1 ASC], false, 0
  ConvertToUnsafe
   Exchange hashpartitioning(date#0,cat#1)
    NewAggregate with SortBasedAggregationIterator List(date#0, cat#1) ArrayBuffer((sum((change\_decimal\_precision(CAST(CAST(count#2, DecimalType(5,0)), DecimalType(11,2))) \* change\_decimal\_precision(CAST(price#3, DecimalType(11,2)))))2,mode=Partial,isDistinct=false))
     TungstenSort [date#0 ASC,cat#1 ASC], false, 0
      ConvertToUnsafe
       InMemoryColumnarTableScan [date#0,cat#1,count#2,price#3], (InMemoryRelation [date#0,cat#1,count#2,price#3], true, 10000, StorageLevel(true, true, false, true, 1), (PhysicalRDD [date#0,cat#1,count#2,price#3], MapPartitionsRDD[3] at), None)

Code Generation: true
== RDD ==
None

15/08/04 23:21:53 ERROR TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job
Traceback (most recent call last):
  File "t.py", line 34, in \<module\>
    r.show()
  File "/Users/davies/work/spark/python/pyspark/sql/dataframe.py", line 258, in show
    print(self.\_jdf.showString(n, truncate))
  File "/Users/davies/work/spark/python/lib/py4j/java\_gateway.py", line 538, in \_\_call\_\_
    self.target\_id, self.name)
  File "/Users/davies/work/spark/python/pyspark/sql/utils.py", line 36, in deco
    return f(\*a, \*\*kw)
  File "/Users/davies/work/spark/python/lib/py4j/protocol.py", line 300, in get\_return\_value
    format(target\_id, '.', name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling o36.showString.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 10, localhost): java.lang.UnsupportedOperationException: tail of empty list
	at scala.collection.immutable.Nil$.tail(List.scala:339)
	at scala.collection.immutable.Nil$.tail(List.scala:334)
	at scala.reflect.internal.SymbolTable.popPhase(SymbolTable.scala:172)
	at scala.reflect.internal.Symbols$Symbol.typeParams(Symbols.scala:1491)
	at scala.reflect.internal.Types$NoArgsTypeRef.typeParams(Types.scala:2144)
	at scala.reflect.internal.Types$TypeRef.initializedTypeParams(Types.scala:2408)
	at scala.reflect.internal.Types$TypeRef.typeParamsMatchArgs(Types.scala:2409)
	at scala.reflect.internal.Types$AliasTypeRef$class.dealias(Types.scala:2232)
	at scala.reflect.internal.Types$TypeRef$$anon$3.dealias(Types.scala:2539)
	at scala.reflect.runtime.JavaMirrors$JavaMirror.typeToJavaClass(JavaMirrors.scala:1256)
	at scala.reflect.runtime.JavaMirrors$JavaMirror.runtimeClass(JavaMirrors.scala:202)
	at scala.reflect.runtime.JavaMirrors$JavaMirror.runtimeClass(JavaMirrors.scala:65)
	at org.apache.spark.sql.columnar.compression.DictionaryEncoding$Decoder.\<init\>(compressionSchemes.scala:277)
	at org.apache.spark.sql.columnar.compression.DictionaryEncoding$.decoder(compressionSchemes.scala:185)
	at org.apache.spark.sql.columnar.compression.DictionaryEncoding$.decoder(compressionSchemes.scala:177)
	at org.apache.spark.sql.columnar.compression.CompressibleColumnAccessor$class.initialize(CompressibleColumnAccessor.scala:31)
	at org.apache.spark.sql.columnar.NativeColumnAccessor.initialize(ColumnAccessor.scala:64)
	at org.apache.spark.sql.columnar.ColumnAccessor$class.$init$(ColumnAccessor.scala:33)
	at org.apache.spark.sql.columnar.BasicColumnAccessor.\<init\>(ColumnAccessor.scala:44)
	at org.apache.spark.sql.columnar.NativeColumnAccessor.\<init\>(ColumnAccessor.scala:64)
	at org.apache.spark.sql.columnar.StringColumnAccessor.\<init\>(ColumnAccessor.scala:92)
	at org.apache.spark.sql.columnar.ColumnAccessor$.apply(ColumnAccessor.scala:130)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$9$$anonfun$14$$anonfun$15.apply(InMemoryColumnarTableScan.scala:300)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$9$$anonfun$14$$anonfun$15.apply(InMemoryColumnarTableScan.scala:299)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$9$$anonfun$14.apply(InMemoryColumnarTableScan.scala:299)
	at org.apache.spark.sql.columnar.InMemoryColumnarTableScan$$anonfun$9$$anonfun$14.apply(InMemoryColumnarTableScan.scala:297)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.sql.execution.UnsafeExternalRowSorter.sort(UnsafeExternalRowSorter.java:173)
	at org.apache.spark.sql.execution.TungstenSort$$anonfun$doExecute$3.apply(sort.scala:146)
	at org.apache.spark.sql.execution.TungstenSort$$anonfun$doExecute$3.apply(sort.scala:126)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:73)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

{code}


---

* [SPARK-9624](https://issues.apache.org/jira/browse/SPARK-9624) | *Minor* | **Make RateControllerSuite faster and more robust**

Tests in RateControllerSuite runs with 1 second batch, takes almost 10 seconds for the whole test suite. If we reduce the batch interval to 100 ms, then the test " multiple publish rates reach receivers" becomes flaky as multiple rates updates may get applied before the rate is polled.


---

* [SPARK-9620](https://issues.apache.org/jira/browse/SPARK-9620) | *Critical* | **generated UnsafeProjection does not support many columns or large exressions**

We put all the expressions in one function of UnsafeProjection, that could reach the 65k code size limit in JVM.

We should split them into multiple functions, like that we do for MutableProjection and SafeProjection.


---

* [SPARK-9618](https://issues.apache.org/jira/browse/SPARK-9618) | *Minor* | **SQLContext.read.schema().parquet() ignores the supplied schema**

If a user supplies a schema when loading a Parquet file it is ignored and the schema is read off disk instead.


---

* [SPARK-9616](https://issues.apache.org/jira/browse/SPARK-9616) | *Critical* | **Erroneous result in Frequent Items (SQL) when merging FrequentItemCounters**

Existing behavior with max size:
Partition A -\>  Map(1 -\> 3, 2 -\> 3, 3 -\> 4)
Partition B -\> Map(4 -\> 25)

Result -\> Map()

Correct Behavior:
Partition A -\>  Map(1 -\> 3, 2 -\> 3, 3 -\> 4)
Partition B -\> Map(4 -\> 25)

Result -\> Map(3 -\> 1, 4 -\> 22)


---

* [SPARK-9615](https://issues.apache.org/jira/browse/SPARK-9615) | *Major* | **Use rdd.aggregate in FrequentItems**

Please check https://issues.apache.org/jira/browse/SPARK-9614 for the reason.


---

* [SPARK-9614](https://issues.apache.org/jira/browse/SPARK-9614) | *Blocker* | **InternalRow representation during executionPlan.toRdd.aggregete possibly problematic**

For example, in FrequentItems.scala, we have a specialized FrequentItems counter which is backed by a Mutable Map. The keys in the MutableMap are the elements in a column.

After iterating through a partition, all the keys turn out to be the latest element seen in that partition!
Assume a partition is composed of the elements ("0","1","2","3","4") for a column of a dataframe. The expected map is:
{code}
("0" -\> 1,"1" -\> 1,"2" -\> 1,"3" -\> 1,"4" -\> 1)
{code}
But once you print it out, it turns out to be:
{code}
("4" -\> 1,"4" -\> 1,"4" -\> 1,"4" -\> 1,"4" -\> 1)
{code}
There might be other instances where such behavior may be observed, and it must be analyzed before the release.


---

* [SPARK-9611](https://issues.apache.org/jira/browse/SPARK-9611) | *Blocker* | **UnsafeFixedWidthAggregationMap.destructAndCreateExternalSorter will add an empty entry to if the map is empty.**

There are two corner cases related to the destructAndCreateExternalSorter (class UnsafeKVExternalSorter) returned by UnsafeFixedWidthAggregationMap.

1. The constructor of UnsafeKVExternalSorter tries to first create a UnsafeInMemorySorter based on the BytesToBytesMap of UnsafeFixedWidthAggregationMap. However, when there is no entry in the map, UnsafeInMemorySorter will throw an AssertionError because we are using the size of map (0 at here) as the initialSize of UnsafeInMemorySorter, which is not allowed.

2. Once we fixes the first problem, when we use UnsafeKVExternalSorter's KVSorterIterator loads data back, you can find there is one extra records, which is an empty record.


---

* [SPARK-9609](https://issues.apache.org/jira/browse/SPARK-9609) | *Trivial* | **Spelling error in Strategy.defaultStrategy**

There is a misspelling of strategy as "stategy" which is exposed in the public API


---

* [SPARK-9608](https://issues.apache.org/jira/browse/SPARK-9608) | *Minor* | **Incorrect zinc -status check in build/mvn**

{{build/mvn}} [uses a {{-z `zinc -status`}} test\|https://github.com/apache/spark/blob/5a23213c148bfe362514f9c71f5273ebda0a848a/build/mvn#L138] to determine whether a {{zinc}} process is running.

However, {{zinc -status}} checks port {{3030}} by default.

This means that if a {{$ZINC\_PORT}} env var is set to some value besides {{3030}}, and an existing {{zinc}} process is running on port {{3030}}, {{build/mvn}} will skip starting a {{zinc}} process, thinking that a suitable one is running.

Subsequent compilations will look for a {{zinc}} at port {{$ZINC\_PORT}} and not find one.

The {{zinc -status}} call should get the flag {{-port "$ZINC\_PORT"}} added to it.


---

* [SPARK-9607](https://issues.apache.org/jira/browse/SPARK-9607) | *Minor* | **Incorrect zinc check in build/mvn**

[This check\|https://github.com/apache/spark/blob/5a23213c148bfe362514f9c71f5273ebda0a848a/build/mvn#L84-L85] in {{build/mvn}} attempts to determine whether {{zinc}} has been installed, but it fails to add the prefix {{build/}} to the path, so it always thinks that {{zinc}} is not installed, sets {{ZINC\_INSTALL\_FLAG}} to {{1}}, and attempts to install {{zinc}}.

This error manifests later because [the {{zinc -shutdown}} and {{zinc -start}} commands\|https://github.com/apache/spark/blob/5a23213c148bfe362514f9c71f5273ebda0a848a/build/mvn#L140-L143] are always run, even if zinc was not installed and is running.


---

* [SPARK-9606](https://issues.apache.org/jira/browse/SPARK-9606) | *Blocker* | **HiveThriftServer tests failing.**

These have become unstable so I'm going to turn them off after discussion with [~rxin].  We should fix before the release.

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/39670/console


---

* [SPARK-9602](https://issues.apache.org/jira/browse/SPARK-9602) | *Major* | **Remove 'Actor' from the comments**

Although we have hidden Akka behind RPC interface, I found that the Akka/Actor-related comments are still spreading everywhere. To make it consistent, we shall remove "actor"/"akka" words from the comments...


---

* [SPARK-9601](https://issues.apache.org/jira/browse/SPARK-9601) | *Trivial* | **Join example fix in streaming-programming-guide.md**

Stream-Stream Join has the following signature for Java in the guide:

JavaPairDStream\<String, String\> joinedStream = stream1.join(stream2);

It should be:
JavaPairDStream\<String, Tuple2\<String, String\>\> joinedStream = stream1.join(stream2);

Same for windowed stream join. It should be:

JavaPairDStream\<String, Tuple2\<String, String\>\> joinedStream = windowedStream1.join(windowedStream2);


---

* [SPARK-9593](https://issues.apache.org/jira/browse/SPARK-9593) | *Blocker* | **Hive ShimLoader loads wrong Hadoop shims when Spark is compiled against Hadoop 2.0.0-mr1-cdh4.1.1**

Internally, Hive {{ShimLoader}} tries to load different versions of Hadoop shims by checking version information gathered from Hadoop jar files.  If the major version number is 1, {{Hadoop20SShims}} will be loaded.  Otherwise, if the major version number is 2, {{Hadoop23Shims}} will be chosen.  However, CDH Hadoop versions like 2.0.0-mr1-cdh4.1.1 have 2 as major version number, but contain Hadoop 1 code.  This confuses Hive {{ShimLoader}} and loads wrong version of shims.


---

* [SPARK-9592](https://issues.apache.org/jira/browse/SPARK-9592) | *Minor* | **Last implemented based on AggregateExpression1 are calculating the values for entire DataFrame partition not on GroupedData partition.**

In current implementation, First and Last aggregates were calculating the values for entire DataFrame partition and then the same value was returned for all GroupedData in the partition.
sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/aggregates.scala
Fixed the First and Last aggregates should compute first and last value per GroupedData instead of entire DataFrame.


---

* [SPARK-9589](https://issues.apache.org/jira/browse/SPARK-9589) | *Blocker* | **Flaky test: HiveCompatibilitySuite.groupby8**

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/39662/testReport/org.apache.spark.sql.hive.execution/HiveCompatibilitySuite/groupby8/

{code}
sbt.ForkMain$ForkError: 
Failed to execute query using catalyst:
Error: Job aborted due to stage failure: Task 24 in stage 3081.0 failed 1 times, most recent failure: Lost task 24.0 in stage 3081.0 (TID 14919, localhost): java.lang.NullPointerException
	at org.apache.spark.unsafe.memory.TaskMemoryManager.getPage(TaskMemoryManager.java:226)
	at org.apache.spark.unsafe.map.BytesToBytesMap$Location.updateAddressesAndSizes(BytesToBytesMap.java:366)
	at org.apache.spark.unsafe.map.BytesToBytesMap$Location.putNewKey(BytesToBytesMap.java:600)
	at org.apache.spark.sql.execution.UnsafeFixedWidthAggregationMap.getAggregationBuffer(UnsafeFixedWidthAggregationMap.java:134)
	at org.apache.spark.sql.execution.aggregate.UnsafeHybridAggregationIterator.initialize(UnsafeHybridAggregationIterator.scala:276)
	at org.apache.spark.sql.execution.aggregate.UnsafeHybridAggregationIterator.\<init\>(UnsafeHybridAggregationIterator.scala:290)
	at org.apache.spark.sql.execution.aggregate.UnsafeHybridAggregationIterator$.createFromInputIterator(UnsafeHybridAggregationIterator.scala:358)
	at org.apache.spark.sql.execution.aggregate.Aggregate$$anonfun$doExecute$1$$anonfun$5.apply(Aggregate.scala:130)
	at org.apache.spark.sql.execution.aggregate.Aggregate$$anonfun$doExecute$1$$anonfun$5.apply(Aggregate.scala:121)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:706)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:297)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:264)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:66)
	at org.apache.spark.scheduler.Task.run(Task.scala:88)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:214)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-9586](https://issues.apache.org/jira/browse/SPARK-9586) | *Trivial* | **Update BinaryClassificationEvaluator to use setRawPredictionCol**

Update BinaryClassificationEvaluator to use setRawPredictionCol, rather than setScore


---

* [SPARK-9583](https://issues.apache.org/jira/browse/SPARK-9583) | *Minor* | **build/mvn script should not print debug messages to stdout**

Doing that means it cannot be used to run {{make-distribution.sh}}, which parses the stdout of maven commands.


---

* [SPARK-9582](https://issues.apache.org/jira/browse/SPARK-9582) | *Minor* | **LDA cleanups**

Small cleanups to LDA code and recent additions

CC: [~fliang]


---

* [SPARK-9580](https://issues.apache.org/jira/browse/SPARK-9580) | *Blocker* | **Refactor TestSQLContext to make it non-singleton**

Because the TestSQLContext is a singleton object, there is literally no way to start a SparkContext in the SQL tests since we disallow multiple SparkContexts in the same JVM. Starting a custom SparkContext is useful when we want to run Spark in "local-cluster" mode or enable the UI, which is normally disabled.

This is a blocker for 1.5 because we currently have tests entirely commented out due to this limitation.

https://github.com/apache/spark/blob/7abaaad5b169520fbf7299808b2bafde089a16a2/sql/core/src/test/scala/org/apache/spark/sql/execution/joins/BroadcastJoinSuite.scala


---

* [SPARK-9577](https://issues.apache.org/jira/browse/SPARK-9577) | *Major* | **Surface concrete iterator types in various sort classes**

We often return abstract iterator types in various sort-related classes (e.g. UnsafeKVExternalSorter). 

It is actually better to return a more concrete type, so the callsite uses that type and JIT can inline the iterator calls.


---

* [SPARK-9574](https://issues.apache.org/jira/browse/SPARK-9574) | *Major* | **Review the contents of uber JARs spark-streaming-XXX-assembly**

It should not contain Spark core and its dependencies, especially the following.
- Hadoop and its dependencies
- Scala libraries


---

* [SPARK-9562](https://issues.apache.org/jira/browse/SPARK-9562) | *Major* | **Move spark-ec2 from mesos to amplab**

See http://apache-spark-developers-list.1001551.n3.nabble.com/Re-Should-spark-ec2-get-its-own-repo-td13151.html for more details


---

* [SPARK-9561](https://issues.apache.org/jira/browse/SPARK-9561) | *Blocker* | **Enable BroadcastJoinSuite**

This is introduced in SPARK-8735, but due to complexities with TestSQLContext this needs to be commented out completely. We need to find a way to make it work before releasing 1.5.

For more detail, see the discussion: https://github.com/apache/spark/pull/7770


---

* [SPARK-9558](https://issues.apache.org/jira/browse/SPARK-9558) | *Minor* | **Update docs to follow the increase of memory defaults.**

Now the memory defaults of master and slave in Standalone mode and History Server is 1g, not 512m. So let's update docs.


---

* [SPARK-9548](https://issues.apache.org/jira/browse/SPARK-9548) | *Blocker* | **BytesToBytesMap could have a destructive iterator**

BytesToBytesMap.iterator() could be destructive, freeing each page as it moves onto the next one.  There are some circumstances where we don't want a destructive iterator (such as when we're building a KV sorter from a map), so there should be a flag to control this.


---

* [SPARK-9546](https://issues.apache.org/jira/browse/SPARK-9546) | *Major* | **Centralize orderable data type checking**

We have a lot of places that check whether we could perform ordering for some expressions based on the data type. We should centralize the checking into one place.


---

* [SPARK-9544](https://issues.apache.org/jira/browse/SPARK-9544) | *Major* | **RFormula in Python**

We can expose RFormula as a transformer in Python.


---

* [SPARK-9543](https://issues.apache.org/jira/browse/SPARK-9543) | *Major* | **Add randomized testing for UnsafeKVExternalSorter**

For these low level operations that we can define correct behaviors, it is often useful to employ randomized testing.


---

* [SPARK-9540](https://issues.apache.org/jira/browse/SPARK-9540) | *Critical* | **Optimize PrefixSpan implementation**

Current `PrefixSpan` implementation contains some major issues:

1. We should expand the prefix by one item at a time instead of by one itemset.
2. Some set operations should be changed to array operations, which should be more efficient.


---

* [SPARK-9538](https://issues.apache.org/jira/browse/SPARK-9538) | *Minor* | **LogisticRegression support raw and probability prediction for PySpark.ml**

LogisticRegression support raw and probability prediction for PySpark.ml


---

* [SPARK-9537](https://issues.apache.org/jira/browse/SPARK-9537) | *Minor* | **DecisionTreeClassifierModel support probability prediction for PySpark.ml**

DecisionTreeClassifierModel support probability prediction for PySpark.ml


---

* [SPARK-9536](https://issues.apache.org/jira/browse/SPARK-9536) | *Minor* | **NaiveBayesModel support probability prediction for PySpark.ml**

NaiveBayesModel support probability prediction for PySpark.ml


---

* [SPARK-9535](https://issues.apache.org/jira/browse/SPARK-9535) | *Minor* | **Modify document for codegen**

SPARK-7184 made codegen enabled by default so let's modify the corresponding documents.


---

* [SPARK-9534](https://issues.apache.org/jira/browse/SPARK-9534) | *Minor* | **Enable javac lint for scalac parity; fix a lot of build warnings, 1.5.0 edition**

For parity with the kinds of warnings scalac emits, we should turn on some of javac's lint options. This reports, for example use of deprecated APIs and unchecked casts as scalac does.

And it's a good time to sweep through build warnings and fix a bunch before the release.

PR coming which shows and explains the fixes


---

* [SPARK-9533](https://issues.apache.org/jira/browse/SPARK-9533) | *Minor* | **Add missing methods in Word2Vec ML (Python API)**

After 8874 is resolved, we can add python wrappers for the same.


---

* [SPARK-9530](https://issues.apache.org/jira/browse/SPARK-9530) | *Minor* | **ScalaDoc should not indicate LDAModel.describeTopics and DistributedLDAModel.topDocumentsPerTopic as approximate.**

Currently the ScalaDoc for LDAModel.describeTopics and DistributedLDAModel.topDocumentsPerTopic suggests that these methods are  approximate. However, both methods are actually precise and there is no need to increase maxTermsPerTopic or maxDocumentsPerTopic to get a more precise set of top terms.


---

* [SPARK-9529](https://issues.apache.org/jira/browse/SPARK-9529) | *Critical* | **Improve sort on Decimal**

Right now, it's really slow, just hang there in random tests 

{code}
pool-1-thread-1-ScalaTest-running-TungstenSortSuite" prio=5 tid=0x00007f822bc82800 nid=0x5103 runnable [0x000000011d1be000]
   java.lang.Thread.State: RUNNABLE
	at java.math.BigInteger.\<init\>(BigInteger.java:405)
	at java.math.BigDecimal.bigTenToThe(BigDecimal.java:3380)
	at java.math.BigDecimal.bigMultiplyPowerTen(BigDecimal.java:3508)
	at java.math.BigDecimal.setScale(BigDecimal.java:2394)
	at java.math.BigDecimal.divide(BigDecimal.java:1691)
	at java.math.BigDecimal.divideToIntegralValue(BigDecimal.java:1734)
	at java.math.BigDecimal.divideAndRemainder(BigDecimal.java:1891)
	at java.math.BigDecimal.remainder(BigDecimal.java:1833)
	at scala.math.BigDecimal.remainder(BigDecimal.scala:281)
	at scala.math.BigDecimal.isWhole(BigDecimal.scala:215)
	at scala.math.BigDecimal.hashCode(BigDecimal.scala:180)
	at org.apache.spark.sql.types.Decimal.hashCode(Decimal.scala:260)
	at org.apache.spark.sql.catalyst.InternalRow.hashCode(InternalRow.scala:121)
	at org.apache.spark.RangePartitioner.hashCode(Partitioner.scala:201)
	at java.lang.Object.toString(Object.java:237)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1418)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1547)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1508)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1431)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1177)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)
	at org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:84)
	at org.apache.spark.util.ClosureCleaner$.ensureSerializable(ClosureCleaner.scala:301)
	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:294)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:122)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:2003)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1.apply(RDD.scala:683)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1.apply(RDD.scala:682)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:108)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)
	at org.apache.spark.rdd.RDD.mapPartitions(RDD.scala:682)
	at org.apache.spark.sql.execution.Exchange$$anonfun$doExecute$1.apply(Exchange.scala:181)
	at org.apache.spark.sql.execution.Exchange$$anonfun$doExecute$1.apply(Exchange.scala:148)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:48)
	at org.apache.spark.sql.execution.Exchange.doExecute(Exchange.scala:148)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:113)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:112)
	at org.apache.spark.sql.execution.Sort$$anonfun$doExecute$1.apply(sort.scala:48)
	at org.apache.spark.sql.execution.Sort$$anonfun$doExecute$1.apply(sort.scala:48)
	at org.apache.spark.sql.catalyst.errors.package$.attachTree(package.scala:48)
	at org.apache.spark.sql.execution.Sort.doExecute(sort.scala:47)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:113)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$5.apply(SparkPlan.scala:113)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:147)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:112)
	at org.apache.spark.sql.execution.SparkPlan.executeCollect(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlanTest$.executePlan(SparkPlanTest.scala:297)
	at org.apache.spark.sql.execution.SparkPlanTest$.checkAnswer(SparkPlanTest.scala:160)
	at org.apache.spark.sql.execution.SparkPlanTest.checkThatPlansAgree(SparkPlanTest.scala:126)
	at org.apache.spark.sql.execution.TungstenSortSuite$$anonfun$3$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4$$anonfun$apply$1.apply$mcV$sp(TungstenSortSuite.scala:76)
	at org.apache.spark.sql.execution.TungstenSortSuite$$anonfun$3$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4$$anonfun$apply$1.apply(TungstenSortSuite.scala:69)
	at org.apache.spark.sql.execution.TungstenSortSuite$$anonfun$3$$anonfun$apply$2$$anonfun$apply$3$$anonfun$apply$4$$anonfun$apply$1.apply(TungstenSortSuite.scala:69)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.execution.TungstenSortSuite.org$scalatest$BeforeAndAfterAll$$super$run(TungstenSortSuite.scala:32)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.execution.TungstenSortSuite.run(TungstenSortSuite.scala:32)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-9528](https://issues.apache.org/jira/browse/SPARK-9528) | *Major* | **RandomForestClassifier should extend ProbabilisticClassifier**

Now that DecisionTreeClassifier extends ProbabilisticClassifier, we can have RandomForestClassifier extends ProbabilisticClassifier as well.


---

* [SPARK-9527](https://issues.apache.org/jira/browse/SPARK-9527) | *Critical* | **PrefixSpan.run should return a PrefixSpanModel instead of an RDD and it should be Java-friendly**

With a model wrapping the result RDD, it would be more flexible to add features in the future. And it should be Java-friendly.


---

* [SPARK-9521](https://issues.apache.org/jira/browse/SPARK-9521) | *Trivial* | **Require Maven 3.3.3+ in the build**

Patrick recently discovered a build problem that manifested because he was using the Maven 3.2.x installed on his system, and which was resolved by using Maven 3.3.x. Since we have a script that can install Maven 3.3.3 for anyone, it probably makes sense to just enforce use of Maven 3.3.3+ in the build. (Currently it's just 3.0.4+).


---

* [SPARK-9520](https://issues.apache.org/jira/browse/SPARK-9520) | *Major* | **UnsafeFixedWidthAggregationMap should support in-place sorting of its own records**

In order to support sort-based external aggregation fallback, UnsafeFixedWidthAggregationMap needs to support sorting all of its records in-place.


---

* [SPARK-9519](https://issues.apache.org/jira/browse/SPARK-9519) | *Minor* | **Confirm stop sc successfully when application was killed**

Currently, when we kill application on Yarn, then will call sc.stop() at Yarn application state monitor thread, then in YarnClientSchedulerBackend.stop() will call interrupt this will cause SparkContext not stop fully as we will wait executor to exit.


---

* [SPARK-9518](https://issues.apache.org/jira/browse/SPARK-9518) | *Major* | **Clean up GenerateUnsafeRowJoiner**

[~davies]'s suggestion on how to rewrite it to be more clear:

{code}
Seq.tabulate(outputBitsetWords) { i =\> 
  val bitset = if (i\< bitset1Words) {
    getLong(obj1, i \* 8)
  } else if (i == bitset1Words && bitset1Remainder \> 0) {
    getLong(obj1, i\* 8) \| getLong(obj2, 0) \>\>\> bitset1Remainder
  } else {
    getLong(obj2, i - bitset1Words) \<\<\< bitset1Remainder \| getLong(obj2 i - bitset1Words + 1) \>\>\> bitset1Remainder
  }
  putLong(buf, i \* 8, bitset)
}
{code}


---

* [SPARK-9517](https://issues.apache.org/jira/browse/SPARK-9517) | *Major* | **BytesToBytesMap should encode data the same way as UnsafeExternalSorter**

BytesToBytesMap current encodes key/value data in the following format:

{code}
8B key length, key data, 8B value length, value data
{code}

UnsafeExternalSorter, on the other hand, encodes data this way:

{code}
4B record length, data
{code}

As a result, we cannot pass records encoded by BytesToBytesMap directly into UnsafeExternalSorter for sorting. However, if we rearrange data slightly, we can then pass the key/value records directly into UnsafeExternalSorter:

{code}
4B key+value length, 4B key length, key data, value data
{code}


---

* [SPARK-9513](https://issues.apache.org/jira/browse/SPARK-9513) | *Blocker* | **Create Python API for all SQL functions**

Check all the SQL functions, make sure they have python API


---

* [SPARK-9512](https://issues.apache.org/jira/browse/SPARK-9512) | *Blocker* | **RemoveEvaluationFromSort reorders sort order**

Please refer to the comment in https://github.com/apache/spark/pull/7593 for details.


---

* [SPARK-9511](https://issues.apache.org/jira/browse/SPARK-9511) | *Blocker* | **Table names starting with numbers no longer supported**

{code}
java.lang.RuntimeException: [1.1] failure: identifier expected

1milints
^
    at scala.sys.package$.error(package.scala:27)
    at org.apache.spark.sql.catalyst.SqlParser.parseTableIdentifier(SqlParser.scala:56)
{code}


---

* [SPARK-9510](https://issues.apache.org/jira/browse/SPARK-9510) | *Major* | **Fix remaining SparkR style violations**

lint-r should report no errors / warnings before we can turn it on in Jenkins.


---

* [SPARK-9509](https://issues.apache.org/jira/browse/SPARK-9509) | *Major* | **AppClient.stop() may throw an exception**

AppClient.stop() calls RPCEndpointRef.askWithRetry, which throws a SparkException if it fails.  This exception is not caught (stop() only catches timeout exceptions) which can lead to a failure during shutdown, causing Spark not to clean itself up properly.  This behavior was changed in this commit: https://github.com/apache/spark/commit/3bee0f1466ddd69f26e95297b5e0d2398b6c6268#diff-a240aa7b4630dc389590147f96cf3431R174, and this seems to be the root cause of the recent Distributed Suite test failures described in SPARK-9497 (the flakiness of DistributedSuite coincides with when the above commit was added to master).


---

* [SPARK-9508](https://issues.apache.org/jira/browse/SPARK-9508) | *Minor* | **Align graphx programming guide with the updated Pregel code**

SPARK-9436 simplifies the Pregel code. graphx-programming-guide needs to be modified accordingly since it lists the old Pregel code


---

* [SPARK-9507](https://issues.apache.org/jira/browse/SPARK-9507) | *Minor* | **Remove dependency reduced POM hack now that shade plugin is updated**

See https://issues.apache.org/jira/browse/SPARK-8819 for the original problem. The shade plugin is fixed, and so I believe this workaround can be removed.


---

* [SPARK-9504](https://issues.apache.org/jira/browse/SPARK-9504) | *Major* | **Flaky test: o.a.s.streaming.StreamingContextSuite.stop gracefully**

Failure build: https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/39149/ 

{code}
[info] - stop gracefully \*\*\* FAILED \*\*\* (3 seconds, 522 milliseconds)
[info]   0 was not greater than 0 (StreamingContextSuite.scala:277)
[info]   org.scalatest.exceptions.TestFailedException:
[info]   at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:500)
[info]   at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
[info]   at org.scalatest.Assertions$AssertionsHelper.macroAssert(Assertions.scala:466)
[info]   at org.apache.spark.streaming.StreamingContextSuite$$anonfun$21$$anonfun$apply$mcV$sp$3.apply$mcVI$sp(StreamingContextSuite.scala:277)
[info]   at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:141)
[info]   at org.apache.spark.streaming.StreamingContextSuite$$anonfun$21.apply$mcV$sp(StreamingContextSuite.scala:261)
[info]   at org.apache.spark.streaming.StreamingContextSuite$$anonfun$21.apply(StreamingContextSuite.scala:257)
[info]   at org.apache.spark.streaming.StreamingContextSuite$$anonfun$21.apply(StreamingContextSuite.scala:257)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.apache.spark.streaming.StreamingContextSuite.org$scalatest$BeforeAndAfter$$super$runTest(StreamingContextSuite.scala:42)
[info]   at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
[info]   at org.apache.spark.streaming.StreamingContextSuite.runTest(StreamingContextSuite.scala:42)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.apache.spark.streaming.StreamingContextSuite.org$scalatest$BeforeAndAfter$$super$run(StreamingContextSuite.scala:42)
[info]   at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
[info]   at org.apache.spark.streaming.StreamingContextSuite.run(StreamingContextSuite.scala:42)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[info]   at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-9500](https://issues.apache.org/jira/browse/SPARK-9500) | *Minor* | **Add TernaryExpression to simplify implementations**

There lots of duplicated code in ternary expressions, create a TernaryExpression for them to reduce duplicated code.


---

* [SPARK-9497](https://issues.apache.org/jira/browse/SPARK-9497) | *Major* | **Flaky test: DistributedSuite failed after the test of "repeatedly failing task that crashes JVM"**

Seems it is pretty often to see DistributedSuite failed right after "repeatedly failing task that crashes JVM".

One example jenkins can be found at 
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/3117/AMPLAB\_JENKINS\_BUILD\_PROFILE=hadoop1.0,label=centos/testReport/junit/org.apache.spark/DistributedSuite/

The log of it can be found at https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/3117/AMPLAB\_JENKINS\_BUILD\_PROFILE=hadoop1.0,label=centos/artifact/core/target/unit-tests.log (search StopAppClient).


---

* [SPARK-9496](https://issues.apache.org/jira/browse/SPARK-9496) | *Minor* | **Do not print password in Hive Config**

We better do not print the password in log.


---

* [SPARK-9495](https://issues.apache.org/jira/browse/SPARK-9495) | *Major* | **Support prefix generation for date / timestamp data type**

There are two files to change:

SortPrefixUtils

and

SortPrefix (in SortOrder.scala)


---

* [SPARK-9493](https://issues.apache.org/jira/browse/SPARK-9493) | *Major* | **Chain logistic regression with isotonic regression under the pipeline API**

One use case of isotonic regression is to calibrate the probabilities output by logistic regression. We should make this easier in the pipeline API.


---

* [SPARK-9491](https://issues.apache.org/jira/browse/SPARK-9491) | *Blocker* | **App running on secure YARN with no HBase config will hang**

Because HBase may not be available, or the default config may be pointing at the wrong information for HBase, the YARN backend may end up waiting forever at this point:

{noformat}
"main" prio=10 tid=0x00007f96c8016000 nid=0x1aa6 waiting on condition [0x00007f96cda96000]
   java.lang.Thread.State: TIMED\_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.hbase.zookeeper.MetaTableLocator.blockUntilAvailable(MetaTableLocator.java:443)
        at org.apache.hadoop.hbase.client.ZooKeeperRegistry.getMetaRegionLocation(ZooKeeperRegistry.java:60)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1123)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1110)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.locateRegion(ConnectionManager.java:1067)
        at org.apache.hadoop.hbase.client.ConnectionManager$HConnectionImplementation.getRegionLocation(ConnectionManager.java:902)
        at org.apache.hadoop.hbase.client.RegionServerCallable.prepare(RegionServerCallable.java:78)
        at org.apache.hadoop.hbase.client.RpcRetryingCaller.callWithRetries(RpcRetryingCaller.java:124)
        at org.apache.hadoop.hbase.ipc.RegionCoprocessorRpcChannel.callExecService(RegionCoprocessorRpcChannel.java:95)
        at org.apache.hadoop.hbase.ipc.CoprocessorRpcChannel.callBlockingMethod(CoprocessorRpcChannel.java:73)
        at org.apache.hadoop.hbase.protobuf.generated.AuthenticationProtos$AuthenticationService$BlockingStub.getAuthenticationToken(AuthenticationProtos.java:4512)
        at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:86)
        at org.apache.hadoop.hbase.security.token.TokenUtil.obtainToken(TokenUtil.java:69)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.yarn.Client$.obtainTokenForHBase(Client.scala:1299)
        at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:270)
{noformat}

The code shouldn't try to fetch HBase delegation tokens when HBase is not configured.


---

* [SPARK-9490](https://issues.apache.org/jira/browse/SPARK-9490) | *Trivial* | **MLlib evaluation metrics guide example python code uses deprecated print statement**

The python examples in the evaluation metrics user guide (MLlib) use "print x" style print statements, which won't work in python 3. These should be changed to {code}print(x){code}


---

* [SPARK-9489](https://issues.apache.org/jira/browse/SPARK-9489) | *Major* | **Remove compatibleWith, meetsRequirements, and needsAnySort checks from Exchange**

While reviewing [~yhuai]'s patch for SPARK-2205, I noticed that Exchange's {{compatible}} check may be incorrectly returning {{false}} in many cases.  As far as I know, this is not actually a problem because the {{compatible}}, {{meetsRequirements}}, and {{needsAnySort}} checks are serving only as short-circuit performance optimizations that are not necessary for correctness.

In order to reduce code complexity, I think that we should remove these checks and unconditionally rewrite the operator's children.  This should be safe because we rewrite the tree in a single bottom-up pass.


---

* [SPARK-9486](https://issues.apache.org/jira/browse/SPARK-9486) | *Minor* | **Add aliasing to data sources to allow external packages to register themselves with Spark**

Currently Spark allows users to use external data sources like spark-avro, spark-csv, etc by having them specifying their full class name:

{code:java}
sqlContext.read.format("com.databricks.spark.avro").load(path)
{code}

Typing in a full class is not the best idea so it would be nice to allow the external packages to be able to register themselves with Spark to allow users to do something like:

{code:java}
sqlContext.read.format("avro").load(path)
{code}

This would make it so that the external data source packages follow the same convention as the built in data sources do, parquet, json, jdbc, etc.

This could be accomplished by using a ServiceLoader.


---

* [SPARK-9483](https://issues.apache.org/jira/browse/SPARK-9483) | *Critical* | **UTF8String.getPrefix only works in little-endian order**

There are 2 bit masking and a reverse bytes that should probably be handled differently on big-endian order.


---

* [SPARK-9482](https://issues.apache.org/jira/browse/SPARK-9482) | *Blocker* | **Broadcast join projection is not thread safe**

This manifests as a flaky test (semijoin):

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/39059/testReport/org.apache.spark.sql.hive.execution/HiveCompatibilitySuite/semijoin/

{code}
Regression

org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.semijoin

Failing for the past 1 build (Since Failed#39059 )
Took 7.7 sec.
Error Message

 Results do not match for semijoin: == Parsed Logical Plan == 'Sort ['a.key ASC], false  'Project [unresolvedalias('a.key)]   'Join RightOuter, Some(('a.key = 'c.key))    'Join LeftSemi, Some(('a.key = 'b.key))     'UnresolvedRelation [t3], Some(a)     'UnresolvedRelation [t2], Some(b)    'UnresolvedRelation [t1], Some(c)  == Analyzed Logical Plan == key: int Sort [key#176228 ASC], false  Project [key#176228]   Join RightOuter, Some((key#176228 = key#176232))    Join LeftSemi, Some((key#176228 = key#176230))     MetastoreRelation default, t3, Some(a)     MetastoreRelation default, t2, Some(b)    MetastoreRelation default, t1, Some(c)  == Optimized Logical Plan == Sort [key#176228 ASC], false  Project [key#176228]   Join RightOuter, Some((key#176228 = key#176232))    Project [key#176228]     Join LeftSemi, Some((key#176228 = key#176230))      Project [key#176228]       MetastoreRelation default, t3, Some(a)      Project [key#176230]       MetastoreRelation default, t2, Some(b)    Project [key#176232]     MetastoreRelation default, t1, Some(c)  == Physical Plan == ExternalSort [key#176228 ASC], false  Project [key#176228]   ConvertToSafe    BroadcastHashOuterJoin [key#176228], [key#176232], RightOuter, None     ConvertToUnsafe      Project [key#176228]       ConvertToSafe        BroadcastLeftSemiJoinHash [key#176228], [key#176230], None         ConvertToUnsafe          HiveTableScan [key#176228], (MetastoreRelation default, t3, Some(a))         ConvertToUnsafe          HiveTableScan [key#176230], (MetastoreRelation default, t2, Some(b))     ConvertToUnsafe      HiveTableScan [key#176232], (MetastoreRelation default, t1, Some(c))  Code Generation: true == RDD == key !== HIVE - 31 row(s) ==   == CATALYST - 30 row(s) ==  0                        0  0                        0  0                        0  0                        0  0                        0  0                        0  0                        0  0                        0  0                        0  0                        0  0                        0  0                        0  0                        0  0                        0  0                        0  0                        0  0                        0  0                        0  10                       10  10                       10  10                       10  10                       10 !4                        8 !4                        8 !8                        NULL !8                        NULL  NULL                     NULL  NULL                     NULL  NULL                     NULL  NULL                     NULL !NULL                                      
Stacktrace

sbt.ForkMain$ForkError: 
Results do not match for semijoin:
== Parsed Logical Plan ==
'Sort ['a.key ASC], false
 'Project [unresolvedalias('a.key)]
  'Join RightOuter, Some(('a.key = 'c.key))
   'Join LeftSemi, Some(('a.key = 'b.key))
    'UnresolvedRelation [t3], Some(a)
    'UnresolvedRelation [t2], Some(b)
   'UnresolvedRelation [t1], Some(c)

== Analyzed Logical Plan ==
key: int
Sort [key#176228 ASC], false
 Project [key#176228]
  Join RightOuter, Some((key#176228 = key#176232))
   Join LeftSemi, Some((key#176228 = key#176230))
    MetastoreRelation default, t3, Some(a)
    MetastoreRelation default, t2, Some(b)
   MetastoreRelation default, t1, Some(c)

== Optimized Logical Plan ==
Sort [key#176228 ASC], false
 Project [key#176228]
  Join RightOuter, Some((key#176228 = key#176232))
   Project [key#176228]
    Join LeftSemi, Some((key#176228 = key#176230))
     Project [key#176228]
      MetastoreRelation default, t3, Some(a)
     Project [key#176230]
      MetastoreRelation default, t2, Some(b)
   Project [key#176232]
    MetastoreRelation default, t1, Some(c)

== Physical Plan ==
ExternalSort [key#176228 ASC], false
 Project [key#176228]
  ConvertToSafe
   BroadcastHashOuterJoin [key#176228], [key#176232], RightOuter, None
    ConvertToUnsafe
     Project [key#176228]
      ConvertToSafe
       BroadcastLeftSemiJoinHash [key#176228], [key#176230], None
        ConvertToUnsafe
         HiveTableScan [key#176228], (MetastoreRelation default, t3, Some(a))
        ConvertToUnsafe
         HiveTableScan [key#176230], (MetastoreRelation default, t2, Some(b))
    ConvertToUnsafe
     HiveTableScan [key#176232], (MetastoreRelation default, t1, Some(c))

Code Generation: true
== RDD ==
key
!== HIVE - 31 row(s) ==   == CATALYST - 30 row(s) ==
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 0                        0
 10                       10
 10                       10
 10                       10
 10                       10
!4                        8
!4                        8
!8                        NULL
!8                        NULL
 NULL                     NULL
 NULL                     NULL
 NULL                     NULL
 NULL                     NULL
!NULL                     
                
	at org.scalatest.Assertions$class.newAssertionFailedException(Assertions.scala:495)
	at org.scalatest.FunSuite.newAssertionFailedException(FunSuite.scala:1555)
	at org.scalatest.Assertions$class.fail(Assertions.scala:1328)
	at org.scalatest.FunSuite.fail(FunSuite.scala:1555)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$apply$mcV$sp$6.apply(HiveComparisonTest.scala:397)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1$$anonfun$apply$mcV$sp$6.apply(HiveComparisonTest.scala:368)
	at scala.runtime.Tuple3Zipped$$anonfun$foreach$extension$1.apply(Tuple3Zipped.scala:109)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.runtime.Tuple3Zipped$.foreach$extension(Tuple3Zipped.scala:107)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply$mcV$sp(HiveComparisonTest.scala:368)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:238)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest$$anonfun$createQueryTest$1.apply(HiveComparisonTest.scala:238)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
	at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$runTest(HiveCompatibilitySuite.scala:31)
	at org.scalatest.BeforeAndAfter$class.runTest(BeforeAndAfter.scala:200)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.runTest(HiveCompatibilitySuite.scala:31)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
	at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
	at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
	at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
	at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
	at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
	at org.scalatest.Suite$class.run(Suite.scala:1424)
	at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
	at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
	at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
	at org.apache.spark.sql.hive.execution.HiveComparisonTest.org$scalatest$BeforeAndAfterAll$$super$run(HiveComparisonTest.scala:42)
	at org.scalatest.BeforeAndAfterAll$class.liftedTree1$1(BeforeAndAfterAll.scala:257)
	at org.scalatest.BeforeAndAfterAll$class.run(BeforeAndAfterAll.scala:256)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.org$scalatest$BeforeAndAfter$$super$run(HiveCompatibilitySuite.scala:31)
	at org.scalatest.BeforeAndAfter$class.run(BeforeAndAfter.scala:241)
	at org.apache.spark.sql.hive.execution.HiveCompatibilitySuite.run(HiveCompatibilitySuite.scala:31)
	at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
	at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
	at sbt.ForkMain$Run$2.call(ForkMain.java:294)
	at sbt.ForkMain$Run$2.call(ForkMain.java:284)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-9481](https://issues.apache.org/jira/browse/SPARK-9481) | *Trivial* | **LocalLDAModel logLikelihood**

We already have a variational {{bound}} method so we should provide a public {{logLikelihood}} that uses the model's parameters


---

* [SPARK-9479](https://issues.apache.org/jira/browse/SPARK-9479) | *Major* | **ReceiverTrackerSuite fails for maven build**

The test failure is here: https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-with-YARN/3109/

I saw the following exception in the log:
{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task serialization failed: java.lang.NullPointerException
org.apache.spark.broadcast.TorrentBroadcast.\<init\>(TorrentBroadcast.scala:80)
org.apache.spark.broadcast.TorrentBroadcastFactory.newBroadcast(TorrentBroadcastFactory.scala:34)
org.apache.spark.broadcast.BroadcastManager.newBroadcast(BroadcastManager.scala:63)
org.apache.spark.SparkContext.broadcast(SparkContext.scala:1297)
org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$submitMissingTasks(DAGScheduler.scala:834)
{code}

This exception is because SparkEnv.get returns null.

I found the maven build is different from the sbt build. The maven build will create all Suite classes at the beginning. `ReceiverTrackerSuite` creates StreamingContext (SparkContext) in the constructor. That means SparkContext is created very early. And the global SparkEnv will be set to null in the previous test. Therefore we saw the above exception when running `Receiver tracker - propagates rate limit` in `ReceiverTrackerSuite`. This test was added recently.

Note: the previous tests in `ReceiverTrackerSuite` didn't use SparkContext actually, that's why we didn't see such failure before.


---

* [SPARK-9471](https://issues.apache.org/jira/browse/SPARK-9471) | *Major* | **Multilayer perceptron classifier**

Implement Multilayer Perceptron for Spark ML. Requirements:
1) ML pipelines interface
2) Extensible internal interface for further development of artificial neural networks for ML
3) Efficient and scalable: use vectors and BLAS


---

* [SPARK-9469](https://issues.apache.org/jira/browse/SPARK-9469) | *Critical* | **TungstenSort should not do safe -\> unsafe conversion itself**

TungstenSort itself assumes input rows are safe rows, and uses a projection to turn the safe rows into UnsafeRows. We should take that part of the logic out of TungstenSort, and let the planner take care of the conversion. In that case, if the input is UnsafeRow already, no conversion is needed.


---

* [SPARK-9467](https://issues.apache.org/jira/browse/SPARK-9467) | *Major* | **Specialized accumulators**

In order for SQL metrics to work better,  we'd need to create a subtype of accumulator called LongAccumulator and DoubleAccumulator, that includes specialized methods for updating.


---

* [SPARK-9464](https://issues.apache.org/jira/browse/SPARK-9464) | *Critical* | **Add property-based tests for UTF8String**

UTF8String is a class that can benefit from ScalaCheck-style property checks. Let's add these.


---

* [SPARK-9463](https://issues.apache.org/jira/browse/SPARK-9463) | *Major* | **Expose model coefficients with names in SparkR RFormula**

Currently you cannot retrieve model statistics from the R side, we should at least allow showing the coefficients for 1.5

Design doc from umbrella task: https://docs.google.com/document/d/10NZNSEurN2EdWM31uFYsgayIPfCFHiuIu3pCWrUmP\_c/edit


---

* [SPARK-9460](https://issues.apache.org/jira/browse/SPARK-9460) | *Major* | **Avoid byte array allocation in StringPrefixComparator**

StringPrefixComparator converts the long values back to byte arrays in order to compare them. We should be able to optimize this to compare the longs directly, rather than turning the longs into byte arrays and comparing them byte by byte. 

{code}
    public int compare(long aPrefix, long bPrefix) {
      // TODO: can done more efficiently
      byte[] a = Longs.toByteArray(aPrefix);
      byte[] b = Longs.toByteArray(bPrefix);
      for (int i = 0; i \< 8; i++) {
        int c = UnsignedBytes.compare(a[i], b[i]);
        if (c != 0) return c;
      }
      return 0;
    }
{code}


---

* [SPARK-9458](https://issues.apache.org/jira/browse/SPARK-9458) | *Major* | **Avoid object allocation in prefix generation**

In our existing sort prefix generation code, we use expression's eval method to generate the prefix, which results in object allocation for every prefix.

We can use the specialized getters available on InternalRow directly to avoid the object allocation.


---

* [SPARK-9457](https://issues.apache.org/jira/browse/SPARK-9457) | *Major* | **Sorting improvements**

An umbrella ticket to improve sorting in Tungsten.


---

* [SPARK-9454](https://issues.apache.org/jira/browse/SPARK-9454) | *Minor* | **LDASuite should use vector comparisons**

{{LDASuite}}'s "OnlineLDAOptimizer one iteration" currently compares correctness using hacky string comparisons. We should compare the vectors instead.


---

* [SPARK-9449](https://issues.apache.org/jira/browse/SPARK-9449) | *Critical* | **inputFiles misses files from MetastoreRelation**

We need to create a trait that {{DataFrame.inputFiles}} can detect and {{MetastoreRelation}} can inherit from.


---

* [SPARK-9448](https://issues.apache.org/jira/browse/SPARK-9448) | *Blocker* | **GenerateUnsafeProjection should not share expressions across instances**

We accidentally moved the list of expressions from the generated code instance to the class wrapper, and as a result, different threads are sharing the same set of expressions, which cause problems when the expressions have mutable state.


---

* [SPARK-9447](https://issues.apache.org/jira/browse/SPARK-9447) | *Major* | **Python RandomForestClassifier probabilityCol, rawPredictionCol**

The API should still work after SPARK-9016-make-random-forest-classifiers-implement-classification-trait gets merged in, but we might want to extend & provide predictRaw and similar in the Python API.


---

* [SPARK-9446](https://issues.apache.org/jira/browse/SPARK-9446) | *Minor* | **Clear Active SparkContext in stop() method**

In thread 'stopped SparkContext remaining active' on mailing list, Andres observed the following in driver log:
{code}
15/07/29 15:17:09 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster has disassociated: \<address removed\>
15/07/29 15:17:09 INFO YarnClientSchedulerBackend: Shutting down all executors
Exception in thread "Yarn application state monitor" org.apache.spark.SparkException: Error asking standalone scheduler to shut down executors
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stopExecutors(CoarseGrainedSchedulerBackend.scala:261)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stop(CoarseGrainedSchedulerBackend.scala:266)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.stop(YarnClientSchedulerBackend.scala:158)
        at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:416)
        at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1411)
        at org.apache.spark.SparkContext.stop(SparkContext.scala:1644)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend$$anon$1.run(YarnClientSchedulerBackend.scala:139)
Caused by: java.lang.InterruptedException
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.tryAcquireSharedNanos(AbstractQueuedSynchronizer.java:1325)
        at scala.concurrent.impl.Promise$DefaultPromise.tryAwait(Promise.scala:208)
        at scala.concurrent.impl.Promise$DefaultPromise.ready(Promise.scala:218)
        at scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:223)
        at scala.concurrent.Await$$anonfun$result$1.apply(package.scala:190)
        at scala.concurrent.BlockContext$DefaultBlockContext$.blockOn(BlockContext.scala:53)
        at scala.concurrent.Await$.result(package.scala:190)15/07/29 15:17:09 INFO YarnClientSchedulerBackend: Asking each executor to shut down

        at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:102)
        at org.apache.spark.rpc.RpcEndpointRef.askWithRetry(RpcEndpointRef.scala:78)
        at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend.stopExecutors(CoarseGrainedSchedulerBackend.scala:257)
        ... 6 more
{code}
Effect of the above exception is that a stopped SparkContext is returned to user since SparkContext.clearActiveContext() is not called.


---

* [SPARK-9440](https://issues.apache.org/jira/browse/SPARK-9440) | *Critical* | **LocalLDAModel should save docConcentration, topicConcentration, and gammaShape**

LocalLDAModel needs to save these parameters in order for {{logPerplexity}} and {{bound}} (see SPARK-6793) to work correctly.


---

* [SPARK-9437](https://issues.apache.org/jira/browse/SPARK-9437) | *Minor* | **SizeEstimator overflows for primitive arrays**

{{SizeEstimator}} can overflow when dealing w/ large primitive arrays eg if you have an {{Array[Double]}} of size 1 \<\< 28.  This means that when you try to broadcast a large primitive array, you get:

{noformat}
java.lang.IllegalArgumentException: requirement failed: sizeInBytes was negative: -2147483608
   at scala.Predef$.require(Predef.scala:233)
   at org.apache.spark.storage.BlockInfo.markReady(BlockInfo.scala:55)
   at org.apache.spark.storage.BlockManager.doPut(BlockManager.scala:815)
   at org.apache.spark.storage.BlockManager.putIterator(BlockManager.scala:638)
...
{noformat}


---

* [SPARK-9436](https://issues.apache.org/jira/browse/SPARK-9436) | *Minor* | **Simplify Pregel by merging joins**

Pregel code contains two consecutive joins: 
```
g.vertices.innerJoin(messages)(vprog)
...
g = g.outerJoinVertices(newVerts) { (vid, old, newOpt) =\> newOpt.getOrElse(old) }
```
They can be replaced by one join. Ankur Dave proposed a patch based on our discussion in mailing list: https://www.mail-archive.com/dev@spark.apache.org/msg10316.html


---

* [SPARK-9433](https://issues.apache.org/jira/browse/SPARK-9433) | *Blocker* | **Audit expression unit tests to test for non-foldable codegen path**

Some expressions behave differently if the input is foldable. In our existing expression unit tests, all inputs that are passed in are literals that are foldable.

We can use both NonFoldableLiteral and Literal to test expressions that behave differently for foldable inputs.


---

* [SPARK-9432](https://issues.apache.org/jira/browse/SPARK-9432) | *Blocker* | **Audit expression unit tests to make sure we pass the proper numeric ranges**

For example, if an expression accepts Int and Long, we should make sure we have one test case for Int that uses a numeric input larger than the max value of a Short, and one test case for Long that uses a numeric input larger than the max value of an Int.


---

* [SPARK-9430](https://issues.apache.org/jira/browse/SPARK-9430) | *Major* | **Rename IntervalType to CalendarInterval**

Based on offline discussion with [~marmbrus].

In 1.6, I think we should just create a TimeInterval type, which stores only the interval in terms of number of microseconds. TimeInterval can then be comparable.

In 1.5, we should rename the existing IntervalType to CalendarInterval, so we won't have name clashes in 1.6.


---

* [SPARK-9428](https://issues.apache.org/jira/browse/SPARK-9428) | *Blocker* | **Add test cases for null inputs for expression unit tests**

We need to audit expression unit tests to make sure we pass in null inputs to test null behavior.


---

* [SPARK-9426](https://issues.apache.org/jira/browse/SPARK-9426) | *Blocker* | **Job page DAG visualization is not shown**

If you go to the job page you see the visualization for the first stage of the job, but the job-level visualization is nowhere to be found.


---

* [SPARK-9425](https://issues.apache.org/jira/browse/SPARK-9425) | *Blocker* | **Support DecimalType in UnsafeRow**

The DecimalType has a precision up to 38 digits, so it's possible to serialize a Decimal as bounded byte array.

We could have a fast path for DecimalType with precision under 18 digits, which could fit in a single long.


---

* [SPARK-9422](https://issues.apache.org/jira/browse/SPARK-9422) | *Major* | **Remove the placeholder attributes used in the aggregation buffers**

Originally, to merge two aggregation buffers, we create a mutable buffer with extra placeholder attributes to represent grouping expressions (because the input aggregation buffer has grouping keys). This was done to make the first implementation easy. We can remove it now, which can also help us to implement the hash aggregate operator using unsafe row as the buffer.


---

* [SPARK-9421](https://issues.apache.org/jira/browse/SPARK-9421) | *Major* | **Fix null-handling bug in UnsafeRow.getDouble, getFloat(), and get(ordinal, dataType)**

UnsafeRow.getDouble and getFloat() return NaN when called on columns that are null, which is inconsistent with the behavior of other row classes (which is to return 0.0).

In addition, the generic get(ordinal, dataType) method should always return {{null}} for a null literal, but currently it returns the same thing as the more specific getters.


---

* [SPARK-9420](https://issues.apache.org/jira/browse/SPARK-9420) | *Major* | **Move expressions in sql/core package to catalyst.**

Since catalyst package already depends on Spark core, we can move those expressions into catalyst, and simplify function registry.


---

* [SPARK-9419](https://issues.apache.org/jira/browse/SPARK-9419) | *Critical* | **ShuffleMemoryManager and MemoryStore should track memory on a per-task, not per-thread, basis**

Spark's ShuffleMemoryManager and MemoryStore track memory on a per-thread basis, which causes problems in the handful of cases where we have tasks that use multiple threads. In PythonRDD, RRDD, ScriptTransformation, and PipedRDD we consume the input iterator in a separate thread in order to write it to an external process.  As a result, these RDD's input iterators are consumed in a different thread than the thread that created them, which can cause problems in our memory allocation tracking. For example, if allocations are performed in one thread but deallocations are performed in a separate thread then memory may be leaked or we may get errors complaining that more memory was allocated than was freed.

I think that the right way to fix this is to change our accounting to be performed on a per-task instead of per-thread basis.  Note that the current per-thread tracking has caused problems in the past; SPARK-3731 (#2668) fixes a memory leak in PythonRDD that was caused by this issue (that fix is no longer necessary as of this patch).


---

* [SPARK-9418](https://issues.apache.org/jira/browse/SPARK-9418) | *Major* | **Use sort-merge join as the default shuffle join**

Sort-merge join is more robust in Spark since sorting can be made using the Tungsten sort operator.


---

* [SPARK-9415](https://issues.apache.org/jira/browse/SPARK-9415) | *Blocker* | **MapType should not support equality & ordering**

MapType today happened to support equality comparison because we use Scala maps under the hood. However, map type should not support equality, hash, or ordering (i.e. cannot be used as join keys, grouping keys, or in equality/ordering tests).

We should throw the appropriate analysis exception for these cases.


---

* [SPARK-9413](https://issues.apache.org/jira/browse/SPARK-9413) | *Major* | **Support MapType in Tungsten**

This is an umbrella ticket to support MapType.


---

* [SPARK-9412](https://issues.apache.org/jira/browse/SPARK-9412) | *Blocker* | **Support records larger than a page size**

Currently in Tungsten, we don't support records that are larger than a page.


---

* [SPARK-9411](https://issues.apache.org/jira/browse/SPARK-9411) | *Critical* | **Make page size configurable**

We need to make page sizes configurable so we can reduce them in unit tests and increase them in real production workloads.

The following hardcoded page sizes need to be updated:

- Spark Core: UnsafeShuffleExternalSorter.PAGE\_SIZE
- Spark SQL: UnsafeExternalSorter.PAGE\_SIZE
- Unsafe: BytesToBytesMap.PAGE\_SIZE\_BYTES

While updating the page sizes, we should also update certain size calculations which are based on the page size so that they do not assume that all pages are the same size.  This isn't strictly necessary in this patch but should be done eventually as part of supporting overflow pages for large records.

A number of unit tests also need to be updated to account for the new page sizes.


---

* [SPARK-9408](https://issues.apache.org/jira/browse/SPARK-9408) | *Major* | **Refactor mllib/linalg.py to mllib/linalg**

We need to refactor mllib/linalg.py to mllib/linalg so that the project structure is similar to that of Scala.


---

* [SPARK-9407](https://issues.apache.org/jira/browse/SPARK-9407) | *Blocker* | **Parquet shouldn't fail when pushing down predicates over a column whose underlying Parquet type is an ENUM**

Spark SQL doesn't have an equivalent data type to Parquet {{BINARY (ENUM)}}, and always treats it as a UTF-8 encoded {{StringType}}. Thus, predicate over a Parquet {{ENUM}} column may be pushed down. However, Parquet 1.7.0 and prior versions only support filter push-down optimization for [a limited set of data types\|https://github.com/apache/parquet-mr/blob/apache-parquet-1.7.0/parquet-column/src/main/java/org/apache/parquet/filter2/predicate/ValidTypeMap.java#L66-L80], and fails the query.

The simplest solution seems to be upgrading parquet-mr to 1.8.1, which fixes this issue via PARQUET-201


---

* [SPARK-9404](https://issues.apache.org/jira/browse/SPARK-9404) | *Major* | **UnsafeArrayData**

An Unsafe-based ArrayData implementation. To begin with, we can encode data this way:

first 4 bytes is the # elements
then each 4 byte is the start offset of the element, unless it is negative, in which case the element is null.
followed by the elements themselves

For example, [10, 11, 12, 13, null, 14], internally should be represented as (each 4 bytes)

5, 28, 32, 36, 40, -44, 48, 10, 11, 12, 13, 0, 14


---

* [SPARK-9403](https://issues.apache.org/jira/browse/SPARK-9403) | *Major* | **Implement code generation for In / InSet**

In expression doesn't have any code generation. Would be great to code gen those. Note that we should also optimize the generated code for literal types (InSet).


---

* [SPARK-9402](https://issues.apache.org/jira/browse/SPARK-9402) | *Major* | **Remove CodegenFallback from Abs / FormatNumber**

It already implements code gen.


---

* [SPARK-9398](https://issues.apache.org/jira/browse/SPARK-9398) | *Critical* | **Add unit test for null inputs for date functions**

Unit tests in DateExpressionsSuite didn't cover null inputs.

For each expression, we should add test cases where inputs were null to test behavior with null inputs.

Note that I have a related pull request https://github.com/apache/spark/pull/7718 that added null test cases for next\_day.


---

* [SPARK-9397](https://issues.apache.org/jira/browse/SPARK-9397) | *Critical* | **DataFrame should provide an API to find source data files if applicable**

Certain applications would benefit from being able to inspect DataFrames that are straightforwardly produced by data sources that stem from files, and find out their source data. For example, one might want to display to a user the size of the data underlying a table, or to copy or mutate it.

Currently, there is not a good way to get this information in a public API.


---

* [SPARK-9395](https://issues.apache.org/jira/browse/SPARK-9395) | *Major* | **Create a SpecializedGetters interface to track all the specialized getters**

As we are adding more and more specialized getters to more classes, we should have an interface to make sure we always declare all of them in all classes.

{code}
public interface SpecializedGetters {

  boolean isNullAt(int ordinal);

  boolean getBoolean(int ordinal);

  byte getByte(int ordinal);

  short getShort(int ordinal);

  int getInt(int ordinal);

  long getLong(int ordinal);

  float getFloat(int ordinal);

  double getDouble(int ordinal);

  Decimal getDecimal(int ordinal);

  UTF8String getUTF8String(int ordinal);

  byte[] getBinary(int ordinal);

  Interval getInterval(int ordinal);

  InternalRow getStruct(int ordinal, int numFields);

}

{code}


---

* [SPARK-9394](https://issues.apache.org/jira/browse/SPARK-9394) | *Major* | **CodeFormatter should handle parentheses**

Our CodeFormatter currently does not handle parentheses, and as a result in code dump, we see code formatted this way:

{code}
foo(
a,
b,
c)
{code}


---

* [SPARK-9393](https://issues.apache.org/jira/browse/SPARK-9393) | *Critical* | **Fix several error-handling bugs in ScriptTransform operator**

SparkSQL's ScriptTransform operator has several serious bugs which make debugging fairly difficult:

- If exceptions are thrown in the writing thread then the child process will not be killed, leading to a deadlock because the reader thread will block while waiting for input that will never arrive.
- TaskContext is not propagated to the writer thread, which may cause errors in upstream pipelined operators.
- Exceptions which occur in the writer thread are not propagated to the main reader thread, which may cause upstream errors to be silently ignored instead of killing the job.  This can lead to silently incorrect query results.
- The writer thread is not a daemon thread, but it should be.

In addition, the code in this file is extremely messy:

- Lots of fields are nullable but the nullability isn't clearly explained.
- Many confusing variable names: for instance, there are variables named {{iter}} and {{iterator}} that are defined in the same scope.
- Lots of code was misindented.
- The {{\*serdeClass}} variables are actually expected to be single-quoted strings, which is really confusing: I feel that this parsing / extraction should be performed in the analyzer, not in the operator itself.
- There were no unit tests for the operator itself, only end-to-end tests.

I have a pull request that fixes all of these issues.


---

* [SPARK-9391](https://issues.apache.org/jira/browse/SPARK-9391) | *Major* | **Support minus, dot, and intercept operators in SparkR RFormula**

The RFormula parser should be extended to support the '.', '-', and intercept operators.

See umbrella design doc https://docs.google.com/document/d/10NZNSEurN2EdWM31uFYsgayIPfCFHiuIu3pCWrUmP\_c/edit?usp=sharing


---

* [SPARK-9390](https://issues.apache.org/jira/browse/SPARK-9390) | *Major* | **Create an array abstract class ArrayData and a default implementation backed by Array[Object]**

{code}
interface ArrayData implements SpecializedGetters {
  int numElements();
  int sizeInBytes();
}
{code}


We should also add to SpecializedGetters a method to get array, i.e.

{code}
interface SpecializedGetters {
  ...
  ArrayData getArray(int ordinal);
  ...
}
{code}


---

* [SPARK-9389](https://issues.apache.org/jira/browse/SPARK-9389) | *Major* | **Support ArrayType in Tungsten**

This is an umbrella ticket to support array type.


---

* [SPARK-9388](https://issues.apache.org/jira/browse/SPARK-9388) | *Trivial* | **Make log messages in ExecutorRunnable more readable**

There's a couple of debug messages printed in ExecutorRunnable containing information about the container being started. They're printed all in one line, which makes them - especially the one containing the process's environment - hard to read.

We should make them nicer (like the similar one printed by Client.scala).


---

* [SPARK-9386](https://issues.apache.org/jira/browse/SPARK-9386) | *Blocker* | **Feature flag for metastore partitioning**

There are a lot of test failures related to metastore partition pruning.  We should put this behind a flag in case there are other regressions after the release.


---

* [SPARK-9378](https://issues.apache.org/jira/browse/SPARK-9378) | *Major* | **Test case "CTAS with serde" fails occasionally**

The reason why "CTAS with serde" fails is that the {{MetastoreRelation}} gets converted to a Parquet data source relation by default.

However, this behavior can be reproduced pretty steadily. I'm pretty puzzled why this test case only fails occasionally. A possible explanation is that, some test cases may set {{spark.sql.hive.convertMetastoreParquet}} to false without properly restoring the original value. When such a test case is executed before "CTAS with serde", no failure occurs.


---

* [SPARK-9373](https://issues.apache.org/jira/browse/SPARK-9373) | *Major* | **Support StructType in Tungsten style Projection**

GenerateUnsafeProjection currently doesn't support StructType.


---

* [SPARK-9370](https://issues.apache.org/jira/browse/SPARK-9370) | *Major* | **Support DecimalType in UnsafeRow**

We should be able to represent the Decimal data using 2 longs (16 byte) given we no longer support unlimited precision.

Once we figure out how to convert Decimal into 2 longs, we can add support for it similar to the way we add support for IntervalType (SPARK-9369).


---

* [SPARK-9369](https://issues.apache.org/jira/browse/SPARK-9369) | *Major* | **Support IntervalType in UnsafeRow**

Should be pretty straightforward to support:

1. Add getInterval accessor to InternalRow.

2. Use two longs in the variable length portion of UnsafeRow to store the interval data.

3. Add IntervalWriter to UnsafeRowWriters.

4. In GenerateUnsafeProjection, add support for IntervalType, using the newly added UnsafeRowWriters.IntervalWriter.

5. Change BoundReference to use the specialized getter for IntervalType.

6. Update the accessor for codegen in CodeGenerator.


---

* [SPARK-9366](https://issues.apache.org/jira/browse/SPARK-9366) | *Major* | **TaskEnd event emitted for task has different stage attempt ID than TaskStart for same task**

During a simple job I ran yesterday, I observed the following in the event log:

{code}
{"Event":"SparkListenerTaskStart","Stage ID":0,"Stage Attempt ID":1,"Task Info":{"Task ID":10244,"Index":55,"Attempt":1,"Launch Time":1437767843724,"Executor ID":"8","Host":"demeter-csmaz10-6.demeter.hpc.mssm.edu","Locality":"PROCESS\_LOCAL","Speculative":true,"Getting Result Time":0,"Finish Time":1437767844387,"Failed":false,"Accumulables":[]}}

{"Event":"SparkListenerTaskEnd","Stage ID":0,"Stage Attempt ID":2,"Task Type":"ShuffleMapTask","Task End Reason":{"Reason":"Success"},"Task Info":{"Task ID":10244,"Index":55,"Attempt":1,"Launch Time":1437767843724,"Executor ID":"8","Host":"demeter-csmaz10-6.demeter.hpc.mssm.edu","Locality":"PROCESS\_LOCAL","Speculative":true,"Getting Result Time":0,"Finish Time":1437767844387,"Failed":false,"Accumulables":[]},"Task Metrics":{"Host Name":"demeter-csmaz10-6.demeter.hpc.mssm.edu","Executor Deserialize Time":63,"Executor Run Time":579,"Result Size":2235,"JVM GC Time":0,"Result Serialization Time":1,"Memory Bytes Spilled":0,"Disk Bytes Spilled":0,"Shuffle Write Metrics":{"Shuffle Bytes Written":2736,"Shuffle Write Time":1388809,"Shuffle Records Written":100},"Input Metrics":{"Data Read Method":"Network","Bytes Read":636000,"Records Read":100000}}}
{code}

The {{TaskStart}} event for task 10244 listed it (correctly) as coming from stage 0, attempt 1, but the {{TaskEnd}} shows it as part of stage 0, attempt 2.

I'm pretty sure this is due to [this line\|https://github.com/apache/spark/blob/1efe97dc9ed31e3b8727b81be633b7e96dd3cd34/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L930] in the DAGScheduler, which fills in the latest attempt ID for the task's stage, instead of the attempt that the task actually belongs to.

I know there's a lot of flux right now around concurrent stage attempts and attempt-id-tracking, but this seems trivial to fix independent of that so I'll send a PR momentarily.


---

* [SPARK-9364](https://issues.apache.org/jira/browse/SPARK-9364) | *Major* | **Fix array out of bounds and use-after-free bugs in UnsafeExternalSorter**

UnsafeExternalSorter does not properly update freeSpaceInCurrentPage, which can cause it to write past the end of memory pages and trigger segfaults.

UnsafeExternalRowSorter has a use-after-free bug when returning the last row from an iterator.


---

* [SPARK-9363](https://issues.apache.org/jira/browse/SPARK-9363) | *Major* | **SortMergeJoin operator should support UnsafeRow**

The SortMergeJoin operator should implement the suppotsUnsafeRow and outputsUnsafeRow settings when appropriate.


---

* [SPARK-9362](https://issues.apache.org/jira/browse/SPARK-9362) | *Major* | **Exception when using DataFrame groupby().sum on Decimal type in Python**

Please see below.  It works with integer, but not with decimal.

bash-4.1# pyspark
Python 2.6.6 (r266:84292, Jan 22 2014, 09:42:36) 
[GCC 4.4.7 20120313 (Red Hat 4.4.7-4)] on linux2
Type "help", "copyright", "credits" or "license" for more information.
Welcome to
      \_\_\_\_              \_\_
     / \_\_/\_\_  \_\_\_ \_\_\_\_\_/ /\_\_
    \_\ \/ \_ \/ \_ `/ \_\_/  '\_/
   /\_\_ / .\_\_/\\_,\_/\_/ /\_/\\_\   version 1.4.0
      /\_/

Using Python version 2.6.6 (r266:84292, Jan 22 2014 09:42:36)
SparkContext available as sc, HiveContext available as sqlContext.
\>\>\> from decimal import \*
\>\>\> from pyspark.sql import SQLContext
\>\>\> sqlContext = SQLContext(sc)
\>\>\> l1 = [('a', 1), ('b', 2), ('a', 3)]
\>\>\> rdd1 = sc.parallelize(l1)
\>\>\> df1 = sqlContext.createDataFrame(rdd1, ['key', 'value'])
\>\>\> df1.collect()
[Row(key=u'a', value=1), Row(key=u'b', value=2), Row(key=u'a', value=3)]
\>\>\> df1.groupBy("key").sum("value").collect()
[Row(key=u'a', SUM(value)=4), Row(key=u'b', SUM(value)=2)]                      
\>\>\> l2 = [('a', Decimal('1.1')), ('b', Decimal('2.3')), ('a', Decimal('3.4'))]
\>\>\> rdd2 = sc.parallelize(l2)
\>\>\> df2 = sqlContext.createDataFrame(rdd2, ['key', 'value'])
\>\>\> df2.collect()
[Row(key=u'a', value=Decimal('1.1')), Row(key=u'b', value=Decimal('2.3')), Row(key=u'a', value=Decimal('3.4'))]
\>\>\> df2.groupBy("key").sum("value").collect()
15/07/26 14:58:50 ERROR Executor: Exception in task 1.0 in stage 12.0 (TID 215)
java.lang.ClassCastException: java.math.BigDecimal cannot be cast to org.apache.spark.sql.types.Decimal
	at org.apache.spark.sql.types.Decimal$DecimalIsFractional$.plus(Decimal.scala:330)
	at org.apache.spark.sql.catalyst.expressions.Add.eval(arithmetic.scala:125)
	at org.apache.spark.sql.catalyst.expressions.Coalesce.eval(nullFunctions.scala:51)
	at org.apache.spark.sql.catalyst.expressions.MutableLiteral.update(literals.scala:91)
	at org.apache.spark.sql.catalyst.expressions.SumFunction.update(aggregates.scala:625)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$7.apply(Aggregate.scala:165)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$7.apply(Aggregate.scala:149)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
15/07/26 14:58:50 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 214)
java.lang.ClassCastException: java.math.BigDecimal cannot be cast to org.apache.spark.sql.types.Decimal
	at org.apache.spark.sql.types.Decimal$DecimalIsFractional$.plus(Decimal.scala:330)
	at org.apache.spark.sql.catalyst.expressions.Add.eval(arithmetic.scala:125)
	at org.apache.spark.sql.catalyst.expressions.Coalesce.eval(nullFunctions.scala:51)
	at org.apache.spark.sql.catalyst.expressions.MutableLiteral.update(literals.scala:91)
	at org.apache.spark.sql.catalyst.expressions.SumFunction.update(aggregates.scala:625)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$7.apply(Aggregate.scala:165)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$7.apply(Aggregate.scala:149)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
15/07/26 14:58:50 ERROR TaskSetManager: Task 1 in stage 12.0 failed 1 times; aborting job
Traceback (most recent call last):
  File "\<stdin\>", line 1, in \<module\>
  File "/usr/local/spark/python/pyspark/sql/dataframe.py", line 314, in collect
    port = self.\_sc.\_jvm.PythonRDD.collectAndServe(self.\_jdf.javaToPython().rdd())
  File "/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/java\_gateway.py", line 538, in \_\_call\_\_
  File "/usr/local/spark/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get\_return\_value
py4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 12.0 failed 1 times, most recent failure: Lost task 1.0 in stage 12.0 (TID 215, localhost): java.lang.ClassCastException: java.math.BigDecimal cannot be cast to org.apache.spark.sql.types.Decimal
	at org.apache.spark.sql.types.Decimal$DecimalIsFractional$.plus(Decimal.scala:330)
	at org.apache.spark.sql.catalyst.expressions.Add.eval(arithmetic.scala:125)
	at org.apache.spark.sql.catalyst.expressions.Coalesce.eval(nullFunctions.scala:51)
	at org.apache.spark.sql.catalyst.expressions.MutableLiteral.update(literals.scala:91)
	at org.apache.spark.sql.catalyst.expressions.SumFunction.update(aggregates.scala:625)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$7.apply(Aggregate.scala:165)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$7.apply(Aggregate.scala:149)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

\>\>\>


---

* [SPARK-9361](https://issues.apache.org/jira/browse/SPARK-9361) | *Major* | **Refactor new aggregation code to reduce the times of checking compatibility**

Currently, we call aggregate.Utils.tryConvert in many places to check it the logical.aggregate can be run with new aggregation. But looks like aggregate.Utils.tryConvert costs much time to run. We should only call tryConvert once and keep it value in logical.aggregate and reuse it.


---

* [SPARK-9360](https://issues.apache.org/jira/browse/SPARK-9360) | *Major* | **Support BinaryType in PrefixComparators for UnsafeExternalSort**

The current implementation of UnsafeExternalSort uses NoOpPrefixComparator for binary-typed data.
So, we need to add BinaryPrefixComparator in PrefixComparators.


---

* [SPARK-9358](https://issues.apache.org/jira/browse/SPARK-9358) | *Major* | **GenerateUnsafeRowJoiner**

A class that can be used to concatenate UnsafeRows together.

{code}
class UnsafeRowConcat(leftSchema: StructType, rightSchema: StructType) {
  def concat(left: UnsafeRow, right: UnsafeRow): UnsafeRow = {
    // ...
  }
}
{code}

concat should reuse the same buffer space, to avoid constant allocation.

UnsafeRow has 3 parts: bitset, fixed-length portion, and variable length portion.

We need to concat the bitsets, the fixed-length portions, append the variable length portions,
and then update the fields with variable length portion to change the new offsets.


---

* [SPARK-9356](https://issues.apache.org/jira/browse/SPARK-9356) | *Major* | **Remove the internal use of DecimalType.Unlimited**

We still have a few places that reference DecimalType.Unlimited, which is deprecated. We should remove those cases (or replace them with fixed precision).

{code}
\> git grep Unlimited

sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/Cast.scala:      case DecimalType.Unlimited =\>
sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/NullFunctionsSuite.scala:      Literal.create(null, DecimalType.Unlimited),
sql/core/src/main/scala/org/apache/spark/sql/execution/datasources/PartitioningUtils.scala:   \*   DoubleType -\> DecimalType.Unlimited -\>
{code}


---

* [SPARK-9353](https://issues.apache.org/jira/browse/SPARK-9353) | *Major* | **Standalone scheduling memory requirement incorrect if cores per executor is not set**

I tried to come up with a more succinct title.

The issue only happens if `spark.executor.cores` is not set. Right now if we have a worker with 8G, and we set `spark.executor.memory` to 1G, then the executor launched on the worker can have at most 8 cores, even if the worker has more cores available.

This is caused by the fix in SPARK-8881.


---

* [SPARK-9352](https://issues.apache.org/jira/browse/SPARK-9352) | *Critical* | **Add tests for standalone scheduling code**

There are no tests for the standalone Master scheduling code! This has caused issues like SPARK-8881 and SPARK-9260 in the past. It is crucial that we have some level of confidence that this code actually works...


---

* [SPARK-9350](https://issues.apache.org/jira/browse/SPARK-9350) | *Major* | **Introduce an InternalRow generic getter that requires a DataType**

We can use that to support generic getters in UnsafeRow.


---

* [SPARK-9349](https://issues.apache.org/jira/browse/SPARK-9349) | *Blocker* | **UDAF cleanup for 1.5**

1. Remove the public o.a.s.sql.expressions.aggregate package

2. Move all private classes into execution or catalyst. Don't expose them.

3. Define an interface for AggregationBuffer, and only expose the interface to users. Internally we can do whatever we want, but don't expose the specific implementations directly.

4. Label it as experimental.


---

* [SPARK-9348](https://issues.apache.org/jira/browse/SPARK-9348) | *Major* | **Remove apply method on InternalRow**

apply incurs another virtual function call. This just removes the extra method internally and converge on the generic getter "get".


---

* [SPARK-9340](https://issues.apache.org/jira/browse/SPARK-9340) | *Major* | **CatalystSchemaConverter and CatalystRowConverter don't handle unannotated repeated fields correctly**

SPARK-6776 and SPARK-6777 followed {{parquet-avro}} to implement backwards-compatibility rules defined in {{parquet-format}} spec. However, both Spark SQL and {{parquet-avro}} neglected the following statement in {{parquet-format}}:
{quote}
This does not affect repeated fields that are not annotated: A repeated field that is neither contained by a {{LIST}}- or {{MAP}}-annotated group nor annotated by {{LIST}} or {{MAP}} should be interpreted as a required list of required elements where the element type is the type of the field.
{quote}
One of the consequences is that, Parquet files generated by {{parquet-protobuf}} containing unannotated repeated fields are not correctly converted to Catalyst arrays.

For example, the following Parquet schema
{noformat}
message root {
  repeated int32 f1
}
{noformat}
 should be converted to
{noformat}
StructType(StructField("f1", ArrayType(IntegerType, containsNull = false), nullable = false) :: Nil)
{noformat}
But now it triggers an {{AnalysisException}}.


---

* [SPARK-9337](https://issues.apache.org/jira/browse/SPARK-9337) | *Trivial* | **Add an ut for Word2Vec to verify the empty vocabulary check**

Word2Vec should throw exception when vocabulary is empty


---

* [SPARK-9336](https://issues.apache.org/jira/browse/SPARK-9336) | *Major* | **Remove all extra JoinedRows**

They were added to improve performance (so JIT can inline the JoinedRow calls). However, we can also just improve it by projecting output out to UnsafeRow in Tungsten variant of the operators.


---

* [SPARK-9335](https://issues.apache.org/jira/browse/SPARK-9335) | *Critical* | **Kinesis test hits rate limit**

This test is failing many pull request builds because of rate limits:

https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/38396/testReport/org.apache.spark.streaming.kinesis/KinesisBackedBlockRDDSuite/\_It\_is\_not\_a\_test\_/

I disabled the test. I wonder if it's better to not have this test run by default since it's a bit brittle to depend on an external system like this (what if Kinesis goes down, for instance, it will block all development).


---

* [SPARK-9332](https://issues.apache.org/jira/browse/SPARK-9332) | *Critical* | **CatalystTypeConverters.toScala does not work on UnsafeRows**

When CatalystTypeConverters.toScala is applied to an UnsafeRow it may lead to an UnsupportedOperationException when it tries to use a generic getter to retrieve primitive columns.

For example:

{code}
== Parsed Logical Plan ==
Sort [c183#208 ASC,c183#208 ASC], true
 Project [c183#208,c184#209]
  LogicalRDD [c183#208,c184#209], MapPartitionsRDD[367] at DataFrameFuzzingSuite at NativeConstructorAccessorImpl.java:-2

== Analyzed Logical Plan ==
c183: binary, c184: int
Sort [c183#208 ASC,c183#208 ASC], true
 Project [c183#208,c184#209]
  LogicalRDD [c183#208,c184#209], MapPartitionsRDD[367] at DataFrameFuzzingSuite at NativeConstructorAccessorImpl.java:-2

== Optimized Logical Plan ==
Sort [c183#208 ASC,c183#208 ASC], true
 LogicalRDD [c183#208,c184#209], MapPartitionsRDD[367] at DataFrameFuzzingSuite at NativeConstructorAccessorImpl.java:-2

== Physical Plan ==
UnsafeExternalSort [c183#208 ASC,c183#208 ASC], true, 0
 Exchange rangepartitioning(c183#208 ASC,c183#208 ASC)
  PhysicalRDD [c183#208,c184#209], MapPartitionsRDD[367] at DataFrameFuzzingSuite at NativeConstructorAccessorImpl.java:-2
{code}

{code}
java.lang.UnsupportedOperationException
	at org.apache.spark.sql.catalyst.expressions.UnsafeRow.get(UnsafeRow.java:223)
	at org.apache.spark.sql.catalyst.CatalystTypeConverters$IdentityConverter$.toScalaImpl(CatalystTypeConverters.scala:143)
	at org.apache.spark.sql.catalyst.CatalystTypeConverters$CatalystTypeConverter.toScala(CatalystTypeConverters.scala:118)
	at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toScala(CatalystTypeConverters.scala:266)
	at org.apache.spark.sql.catalyst.CatalystTypeConverters$StructConverter.toScala(CatalystTypeConverters.scala:233)
	at org.apache.spark.sql.catalyst.CatalystTypeConverters$$anonfun$createToScalaConverter$2.apply(CatalystTypeConverters.scala:386)
	at org.apache.spark.sql.DataFrame$$anonfun$rdd$1$$anonfun$apply$9.apply(DataFrame.scala:1480)
	at org.apache.spark.sql.DataFrame$$anonfun$rdd$1$$anonfun$apply$9.apply(DataFrame.scala:1480)
{code}


---

* [SPARK-9331](https://issues.apache.org/jira/browse/SPARK-9331) | *Major* | **Indent generated code properly when dumping them in debug code or exception mode**

Our generated code isn't well indented. It would be great to format the code when we dump it in debug mode or when we see an exception.


---

* [SPARK-9329](https://issues.apache.org/jira/browse/SPARK-9329) | *Major* | **Bring UnsafeRow up to feature parity with other InternalRow implementations**

This is an umbrella ticket to track various tasks needed to bring UnsafeRow up to feature parity with other InternalRows.


---

* [SPARK-9327](https://issues.apache.org/jira/browse/SPARK-9327) | *Major* | **Docs incorrectly state that "spark.\*.extraClassPath" append entries**

The docs state that those config options append entries to the driver / executor classpath, when they actually prepend.


---

* [SPARK-9326](https://issues.apache.org/jira/browse/SPARK-9326) | *Minor* | **Spark never closes the lock file used to prevent concurrent downloads**

A lock file is used to ensure multiple executors running on the same machine don't download the same file concurrently. Spark never closes these lock files (we release the lock, but releasing the lock does not close the  underlying file). In theory, if an executor fetched a large number of files, this could eventually lead to an issue with too many open files.


---

* [SPARK-9324](https://issues.apache.org/jira/browse/SPARK-9324) | *Major* | **Add `unique` as a synonym for `distinct`**

In R unique returns a new data.frame with duplicate rows removed.

cc [~rxin] is there some different meaning for `unique` in Spark ?


---

* [SPARK-9323](https://issues.apache.org/jira/browse/SPARK-9323) | *Major* | **DataFrame.orderBy gives confusing analysis errors when ordering based on nested columns**

The following two queries should be equivalent, but the second crashes:

{code}
sqlContext.read.json(sqlContext.sparkContext.makeRDD(
    """{"a": {"b": 1, "a": {"a": 1}}, "c": [{"d": 1}]}""" :: Nil))
  .registerTempTable("nestedOrder")
   checkAnswer(sql("SELECT a.b FROM nestedOrder ORDER BY a.b"), Row(1))
   checkAnswer(sql("select \* from nestedOrder").select("a.b").orderBy("a.b"), Row(1))
{code}

Here's the stacktrace:

{code}
Cannot resolve column name "a.b" among (b);
org.apache.spark.sql.AnalysisException: Cannot resolve column name "a.b" among (b);
	at org.apache.spark.sql.DataFrame$$anonfun$resolve$1.apply(DataFrame.scala:159)
	at org.apache.spark.sql.DataFrame$$anonfun$resolve$1.apply(DataFrame.scala:159)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.DataFrame.resolve(DataFrame.scala:158)
	at org.apache.spark.sql.DataFrame.col(DataFrame.scala:651)
	at org.apache.spark.sql.DataFrame.apply(DataFrame.scala:640)
	at org.apache.spark.sql.DataFrame$$anonfun$sort$1.apply(DataFrame.scala:593)
	at org.apache.spark.sql.DataFrame$$anonfun$sort$1.apply(DataFrame.scala:593)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.DataFrame.sort(DataFrame.scala:593)
	at org.apache.spark.sql.DataFrame.orderBy(DataFrame.scala:624)
	at org.apache.spark.sql.SQLQuerySuite$$anonfun$96.apply$mcV$sp(SQLQuerySuite.scala:1389)
{code}

Per [~marmbrus], the problem may be that {{DataFrame.resolve}} calls {{resolveQuoted}}, causing the nested field to be treated as a single field named {{a.b}}.

UPDATE: here's a shorter one-liner reproduction:

{code}
    val df = sqlContext.read.json(sqlContext.sparkContext.makeRDD("""{"a": {"b": 1}}""" :: Nil))
    checkAnswer(df.select("a.b").filter("a.b = a.b"), Row(1))
{code}


---

* [SPARK-9321](https://issues.apache.org/jira/browse/SPARK-9321) | *Major* | **Add nrow, ncol, dim for SparkR data frames**

`nrow` will be a synonym for `count` and `ncol` can be implemented using `columns()` or `dtypes`


---

* [SPARK-9320](https://issues.apache.org/jira/browse/SPARK-9320) | *Major* | **Add `summary` as a synonym for `describe`**

`summary` is used to provide similar functionality in R data frames.


---

* [SPARK-9316](https://issues.apache.org/jira/browse/SPARK-9316) | *Major* | **Add support for filtering using `[` (synonym for filter / select)**

Will help us support queries of the form 

{code}
air[air$UniqueCarrier %in% c("UA", "HA"), c(1,2,3,5:9)]
{code}


---

* [SPARK-9308](https://issues.apache.org/jira/browse/SPARK-9308) | *Minor* | **ml.NaiveBayesModel support predicting class probabilities**

ml.LogisticRegressionModel inherit from ProbabilisticClassificationModel and can predict class probabilities.
Other models such as Decision Trees and Random Forest also will support predicting probabilities which is working under SPARK-3727.
This JIRA make NaiveBayesModel support predicting class probabilities.


---

* [SPARK-9306](https://issues.apache.org/jira/browse/SPARK-9306) | *Major* | **SortMergeJoin crashes when trying to join on unsortable columns**

When sort merge join is enabled, it's possible to crash at runtime when trying to join on two StructFields: the problem is that we don't support ordering by struct fields or arrays. When the join columns are unsortable then we should not choose the SMJ join strategy.


---

* [SPARK-9305](https://issues.apache.org/jira/browse/SPARK-9305) | *Major* | **Rename org.apache.spark.Row to Item**

It's a thing used in test cases, but named Row. Pretty annoying because everytime I search for Row, it shows up before the Spark SQL Row, which is what a developer wants most of the time.


---

* [SPARK-9304](https://issues.apache.org/jira/browse/SPARK-9304) | *Critical* | **Improve backwards compatibility of SPARK-8401**

In SPARK-8401 a backwards incompatible change was made to the scala 2.11 build process. It would be good to add scripts with the older names to avoid breaking compatibility for harnesses or other automated builds that build for Scala 2.11. The can just be a one line shell script with a comment explaining it is for backwards compatibility purposes.

/cc [~srowen]


---

* [SPARK-9295](https://issues.apache.org/jira/browse/SPARK-9295) | *Major* | **Analysis should detect sorting on unsupported column types**

The SQL analyzer should report errors for queries that try to sort on columns of unsupported types, such as ArrayType.


---

* [SPARK-9293](https://issues.apache.org/jira/browse/SPARK-9293) | *Major* | **Analysis should detect when set operations are performed on tables with different numbers of columns**

Our SQL analyzer doesn't always enforce that set operations are only performed on relations with the same number of columns.


---

* [SPARK-9292](https://issues.apache.org/jira/browse/SPARK-9292) | *Major* | **Analysis should check that join conditions' data types are booleans**

The following data frame query should fail analysis but instead fails at runtime:

{code}
    val df = Seq((1, 1)).toDF("a", "b")
    df.join(df, df.col("a"))
{code}

This should fail with an AnalysisException because the column "A" is not a boolean and thus cannot be used as a join condition.

This can be fixed by adding a new analysis rule which checks that the join condition has BooleanType.


---

* [SPARK-9291](https://issues.apache.org/jira/browse/SPARK-9291) | *Blocker* | **Conversion is applied twice on partitioned data sources**

We currently apply conversion twice: once in DataSourceStrategy (search for toCatalystRDD), and another in HadoopFsRelation.buildScan (search for rowToRowRdd).


---

* [SPARK-9290](https://issues.apache.org/jira/browse/SPARK-9290) | *Major* | **DateExpressionsSuite is slow to run**

We are running way too many test cases in here.

{code}
[info] - DayOfYear (16 seconds, 998 milliseconds)
{code}


---

* [SPARK-9287](https://issues.apache.org/jira/browse/SPARK-9287) | *Major* | **Speedup unit test of Date expressions**

It tried hard to cover many corner cases, but slow down unit tests a lot (take 30 seconds on my Macbook). 

We could ignore most of them now.


---

* [SPARK-9286](https://issues.apache.org/jira/browse/SPARK-9286) | *Trivial* | **Methods in Unevaluable should be final**

The {{eval()}} and {{genCode()}} methods in SQL's {{Unevaluable}} trait should be marked as {{final}} and we should fix any cases where they are overridden.


---

* [SPARK-9285](https://issues.apache.org/jira/browse/SPARK-9285) | *Major* | **Remove InternalRow's inheritance from Row**

It is a big change, but it lets us use the type information to prevent accidentally passing internal types to external types.


---

* [SPARK-9281](https://issues.apache.org/jira/browse/SPARK-9281) | *Major* | **Parse literals as decimal in SQL**

Right now, we use double to parse all the float number in SQL. When it's used in expression together with DecimalType, it will turn the decimal into double as well.

Also it will loss some precision when using double.

It's better to parse the float number as decimal (we will know exactly the precision and scale is), it also work well with double.

BTW, this is a break change.


---

* [SPARK-9277](https://issues.apache.org/jira/browse/SPARK-9277) | *Minor* | **SparseVector constructor must throw an error when declared number of elements less than array length**

I found that one can create SparseVector inconsistently and it will lead to an Java error in runtime, for example when training LogisticRegressionWithSGD.

Here is the test case:


In [2]:
sc.version
Out[2]:
u'1.3.1'
In [13]:
from pyspark.mllib.linalg import SparseVector
from pyspark.mllib.regression import LabeledPoint
from pyspark.mllib.classification import LogisticRegressionWithSGD
In [3]:
x =  SparseVector(2, {1:1, 2:2, 3:3, 4:4, 5:5})
In [10]:
l = LabeledPoint(0, x)
In [12]:
r = sc.parallelize([l])
In [14]:
m = LogisticRegressionWithSGD.train(r)

Error:


Py4JJavaError: An error occurred while calling o86.trainLogisticRegressionModelWithSGD.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 7 in stage 11.0 failed 1 times, most recent failure: Lost task 7.0 in stage 11.0 (TID 47, localhost): java.lang.ArrayIndexOutOfBoundsException: 2


Attached is the notebook with the scenario and the full message


---

* [SPARK-9270](https://issues.apache.org/jira/browse/SPARK-9270) | *Minor* | **Allow --name option in pyspark**

Currently, the app name is hardcoded in pyspark as "PySparkShell", and the app name cannot be changed.

SPARK-8650 fixed this issue for spark-sql.

SPARK-9180 introduced a new option {{--name}} for spark-shell.

sparkR is different because {{SparkContext}} is not automatically constructed in sparkR, and the app name can be set when initializing {{SparkContext}}.

In summary-
\|\|shell\|\|able to set app name\|\|
\|pyspark\|no\|
\|spark-shell\|yes via --name\|
\|spark-sql\|yes via --conf spark.app.name\|
\|sparkR\|n/a\|


---

* [SPARK-9268](https://issues.apache.org/jira/browse/SPARK-9268) | *Major* | **Params.setDefault should not keep varargs annotation**

See [SPARK-7498].  We added varargs (again).  Though it is technically correct, it often requires that developers do clean assembly, rather than (not clean) assembly, which is a nuisance during development.

This JIRA will remove it for now, pending a fix to the Scala compiler.


---

* [SPARK-9267](https://issues.apache.org/jira/browse/SPARK-9267) | *Trivial* | **Remove highly unnecessary accumulators stringify methods**

{code}
def stringifyPartialValue(partialValue: Any): String = "%s".format(partialValue)
def stringifyValue(value: Any): String = "%s".format(value)
{code}
These are only used in 1 place (DAGScheduler). The level of indirection actually makes the code harder to read without an editor. We should just inline them...


---

* [SPARK-9266](https://issues.apache.org/jira/browse/SPARK-9266) | *Major* | **Prevent "managed memory leak detected" exception from masking original exception**

When a task fails with an exception and also fails to properly clean up its managed memory, the {{spark.unsafe.exceptionOnMemoryLeak}} memory leak detection exception may mask the original exception cause.  We should throw the memory leak exception only if no other exception occurred.


---

* [SPARK-9264](https://issues.apache.org/jira/browse/SPARK-9264) | *Blocker* | **When dealing with Union/Intersect/Except, we cast FloatType/DoubleType to the wrong DecimalType**

The problem is https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/analysis/HiveTypeCoercion.scala#L361-L362. When we union/intersect/except, for a column, if the data type of it in one table is a fixed precision decimal type and that in another table is a FloatType/DoubleType, we cast FloatType to Decimal(7, 7) and cast DoubleType to Decimal(15, 15), respectively. It is wrong.

I tried the following in 1.4
{code}
sqlContext.sql("select a from (select cast(200.101 as double) as a union all select cast(200.101 as decimal(6,3)) as a) tmp").show
{code}
I got 
{code}
+-------------------+
\|                  a\|
+-------------------+
\|               null\|
\|200.101000000000000\|
+-------------------+
{code}


---

* [SPARK-9263](https://issues.apache.org/jira/browse/SPARK-9263) | *Major* | **Add Spark Submit flag to exclude dependencies when using --packages**

While the functionality is there to exclude packages, there are no flags that allow users to exclude dependencies, in case of dependency conflicts. We should provide users with a flag to add dependency exclusions in case the packages are not resolved properly (or not available due to licensing).


---

* [SPARK-9262](https://issues.apache.org/jira/browse/SPARK-9262) | *Major* | **Treat Scala compiler warnings as errors**

I've seen a few cases in the past few weeks that the compiler is throwing warnings that are caused by legitimate bugs. This patch updates warnings to errors, except deprecation warnings.

Note that ideally we should be able to mark deprecation warnings as errors as well. However, due to the lack of ability to suppress individual warning messages in the Scala compiler, we cannot do that (since we do need to access deprecated APIs in Hadoop).


---

* [SPARK-9261](https://issues.apache.org/jira/browse/SPARK-9261) | *Minor* | **StreamingTab calls public APIs in Spark core that expose shaded classes**

There's a minor issue in {{StreamingTab}} that has hit me a couple of times when building with maven.

It calls methods in {{JettyUtils}} and {{WebUI}} that expose Jetty types (namely {{ServletContextHandler}}). Since Jetty is now shaded, it's not safe to do that, since when running unit tests the spark-core jar will have the shaded version of the APIs while the streaming classes haven't been shaded yet.

This seems, at the lowest level, to be a bug in scalac (I've run into this issue in other modules before), since the code shouldn't compile at all, but we should avoid that kind of thing in the first place.


---

* [SPARK-9260](https://issues.apache.org/jira/browse/SPARK-9260) | *Major* | **Standalone scheduling can overflow a worker with cores**

If the cluster is started with `spark.deploy.spreadOut = false`, then we may allocate more cores than is available on a worker. E.g. a worker has 8 cores, and an application sets `spark.cores.max = 10`, then we end up with the following screenshot:


---

* [SPARK-9257](https://issues.apache.org/jira/browse/SPARK-9257) | *Minor* | **Fix the false negative of Aggregate2Sort and FinalAndCompleteAggregate2Sort's missingInput**

{code}
sqlContext.sql(
   """
  \|SELECT sum(value)
  \|FROM agg1
  \|GROUP BY key
  """.stripMargin).explain()

== Physical Plan ==
Aggregate2Sort Some(List(key#510)), [key#510], [(sum(CAST(value#511, LongType))2,mode=Final,isDistinct=false)], [sum(CAST(value#511, LongType))#1435L], [sum(CAST(value#511, LongType))#1435L AS \_c0#1426L]
 ExternalSort [key#510 ASC], false
  Exchange hashpartitioning(key#510)
   Aggregate2Sort None, [key#510], [(sum(CAST(value#511, LongType))2,mode=Partial,isDistinct=false)], [currentSum#1433L], [key#510,currentSum#1433L]
    ExternalSort [key#510 ASC], false
     PhysicalRDD [key#510,value#511], MapPartitionsRDD[97] at apply at Transformer.scala:22

sqlContext.sql(
  """
  \|SELECT sum(distinct value)
  \|FROM agg1
  \|GROUP BY key
  """.stripMargin).explain()

== Physical Plan ==
!FinalAndCompleteAggregate2Sort [key#510,CAST(value#511, LongType)#1446L], [key#510], [(sum(CAST(value#511, LongType)#1446L)2,mode=Complete,isDistinct=false)], [sum(CAST(value#511, LongType))#1445L], [sum(CAST(value#511, LongType))#1445L AS \_c0#1438L]
 Aggregate2Sort Some(List(key#510)), [key#510,CAST(value#511, LongType)#1446L], [key#510,CAST(value#511, LongType)#1446L]
  ExternalSort [key#510 ASC,CAST(value#511, LongType)#1446L ASC], false
   Exchange hashpartitioning(key#510)
    !Aggregate2Sort None, [key#510,CAST(value#511, LongType) AS CAST(value#511, LongType)#1446L], [key#510,CAST(value#511, LongType)#1446L]
     ExternalSort [key#510 ASC,CAST(value#511, LongType) AS CAST(value#511, LongType)#1446L ASC], false
      PhysicalRDD [key#510,value#511], MapPartitionsRDD[102] at apply at Transformer.scala:22
{code}

For examples shown above, you can see there is a {{!}} at the bingeing of the operator's {{simpleString}}), which indicates that its {{missingInput}} is not empty. Actually, it is a false negative and we need to fix it.

Also, it will be good to make these two operators' {{simpleString}} more reader friendly (people can tell what are grouping expressions, what are aggregate functions, and what is the mode of an aggregate function).


---

* [SPARK-9255](https://issues.apache.org/jira/browse/SPARK-9255) | *Major* | **SQL codegen fails with "value \< is not a member of TimestampType.this.InternalType"**

Updates: This issue is due to the following config: 

spark.sql.codegen   true

If this param is set to be false, the problem does not happen. The bug was introduced in 1.4.0.  Releases 1.3.0 and 1.3.1 have no this issue.


===================================
This is a very strange case involving timestamp  I can run the program on Windows using dev pom.xml (1.4.1) or 1.4.1 or 1.3.0 release downloaded from Apache  without issues , but when I ran it on Spark 1.4.1 release either downloaded from Apache or the version built with scala 2.11 on redhat linux, it has the following error (the code I used is after this stack trace):

15/07/22 12:02:50  ERROR Executor 96: Exception in task 0.0 in stage 0.0 (TID 0)
java.util.concurrent.ExecutionException: scala.tools.reflect.ToolBoxError: reflective compilation has failed:

value \< is not a member of TimestampType.this.InternalType
        at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
        at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
        at org.spark-project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
        at org.spark-project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
        at org.spark-project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
        at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
        at org.spark-project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
        at org.spark-project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
        at org.spark-project.guava.cache.LocalCache.get(LocalCache.java:4000)
        at org.spark-project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
        at org.spark-project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:105)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:102)
        at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:170)
        at org.apache.spark.sql.execution.GeneratedAggregate$$anonfun$9.apply(GeneratedAggregate.scala:261)
        at org.apache.spark.sql.execution.GeneratedAggregate$$anonfun$9.apply(GeneratedAggregate.scala:246)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: scala.tools.reflect.ToolBoxError: reflective compilation has failed:

value \< is not a member of TimestampType.this.InternalType
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$ToolBoxGlobal.throwIfErrors(ToolBoxFactory.scala:316)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$ToolBoxGlobal.wrapInPackageAndCompile(ToolBoxFactory.scala:198)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$ToolBoxGlobal.compile(ToolBoxFactory.scala:252)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$$anonfun$compile$2.apply(ToolBoxFactory.scala:429)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$$anonfun$compile$2.apply(ToolBoxFactory.scala:422)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$withCompilerApi$.liftedTree2$1(ToolBoxFactory.scala:355)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$withCompilerApi$.apply(ToolBoxFactory.scala:355)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl.compile(ToolBoxFactory.scala:422)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl.eval(ToolBoxFactory.scala:444)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:74)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:26)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:92)
        at org.spark-project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
        at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
        ... 25 more
15/07/22 12:02:50  ERROR TaskSetManager 75: Task 0 in stage 0.0 failed 1 times; aborting job
Exception in thread "main" org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.util.concurrent.ExecutionException: scala.tools.reflect.ToolBoxError: reflective compilation has failed:

value \< is not a member of TimestampType.this.InternalType
        at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
        at org.spark-project.guava.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
        at org.spark-project.guava.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
        at org.spark-project.guava.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
        at org.spark-project.guava.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
        at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
        at org.spark-project.guava.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
        at org.spark-project.guava.cache.LocalCache$Segment.get(LocalCache.java:2257)
        at org.spark-project.guava.cache.LocalCache.get(LocalCache.java:4000)
        at org.spark-project.guava.cache.LocalCache.getOrLoad(LocalCache.java:4004)
        at org.spark-project.guava.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:105)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:102)
        at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:170)
        at org.apache.spark.sql.execution.GeneratedAggregate$$anonfun$9.apply(GeneratedAggregate.scala:261)
        at org.apache.spark.sql.execution.GeneratedAggregate$$anonfun$9.apply(GeneratedAggregate.scala:246)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: scala.tools.reflect.ToolBoxError: reflective compilation has failed:

value \< is not a member of TimestampType.this.InternalType
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$ToolBoxGlobal.throwIfErrors(ToolBoxFactory.scala:316)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$ToolBoxGlobal.wrapInPackageAndCompile(ToolBoxFactory.scala:198)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$ToolBoxGlobal.compile(ToolBoxFactory.scala:252)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$$anonfun$compile$2.apply(ToolBoxFactory.scala:429)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$$anonfun$compile$2.apply(ToolBoxFactory.scala:422)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$withCompilerApi$.liftedTree2$1(ToolBoxFactory.scala:355)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl$withCompilerApi$.apply(ToolBoxFactory.scala:355)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl.compile(ToolBoxFactory.scala:422)
        at scala.tools.reflect.ToolBoxFactory$ToolBoxImpl.eval(ToolBoxFactory.scala:444)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:74)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:26)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:92)
        at org.spark-project.guava.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
        at org.spark-project.guava.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
        ... 25 more

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1273)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1264)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1263)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1263)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
        at scala.Option.foreach(Option.scala:257)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1457)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1418)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
================================================
Code: TruncHour.java

/\*
 \* To change this license header, choose License Headers in Project Properties.
 \* To change this template file, choose Tools \| Templates
 \* and open the template in the editor.
 \*/
package zwu.spark.sample;

import java.sql.Timestamp;
import org.apache.spark.sql.api.java.UDF1;


public class TruncHour implements UDF1\<Timestamp, Timestamp\> {

    @Override
    public Timestamp call(Timestamp t1) throws Exception {

        t1.setMinutes(0);
        t1.setSeconds(0);
        t1.setNanos(0);
        return t1;

        //throw new UnsupportedOperationException("Not supported yet."); //To change body of generated methods, choose Tools \| Templates.
    }

}

TimestampSample.java:

package zwu.spark.sample;

import java.io.Serializable;
import java.sql.Timestamp;
import java.util.ArrayList;
import java.util.Date;
import java.util.List;
import org.apache.log4j.Logger;
import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaRDD;
import org.apache.spark.api.java.JavaSparkContext;
import org.apache.spark.sql.DataFrame;
import org.apache.spark.sql.types.DataTypes;

/\*\*
 \*
 \* @author zw251y
 \*/
public class TimestampSample {

    private final static Logger log = Logger.getLogger(TimestampSample.class);

    public static void main(String[] args) {
        SparkConf sparkConf = new SparkConf().setAppName("TimeSample");
        //if (PUtil.isWindows) {
        sparkConf.setMaster("local[1]");
        //}

        JavaSparkContext sc = new JavaSparkContext(sparkConf);
        org.apache.spark.sql.hive.HiveContext sqlContext = new org.apache.spark.sql.hive.HiveContext(sc.sc());

        List\<TimeBean\> tbList = new ArrayList\<\>();
        TimeBean tb = new TimeBean();
        tb.ts = new Timestamp(new Date().getTime());
        tbList.add(tb);
        JavaRDD\<TimeBean\> mytable = sc.parallelize(tbList);

        //Apply a schema to an RDD of JavaBeans and register it as a table.
        //DataFrame schemaPeople = sqlContext.applySchema(people, TimeBean.class);
        DataFrame schemaPeople = sqlContext.createDataFrame(mytable, TimeBean.class);
        schemaPeople.registerTempTable("timetable");
         
        sqlContext.udf().register("truncHour", new TruncHour(), DataTypes.TimestampType);
        // SQL can be run over RDDs that have been registered as tables.

        //org.apache.spark.sql.types.TimestampType p = null;
        //p.
        String test = "select p from (SELECT   min(ts) p , count(ts)  from timetable  group by truncHour(ts)) as mytable group by p ";
        DataFrame df = sqlContext.sql(test);
        df.show();

    }

    public static class TimeBean implements Serializable {

        private String name;

        private Timestamp ts;

        /\*\*
         \* Get the value of ts
         \*
         \* @return the value of ts
         \*/
        public Timestamp getTs() {
            return ts;
        }

        /\*\*
         \* Set the value of ts
         \*
         \* @param d new value of ts
         \*/
        public void setTs(Timestamp d) {
            this.ts = d;
        }

        public String getName() {
            return name;
        }

        public void setName(String name) {
            this.name = name;
        }

    }
}


---

* [SPARK-9254](https://issues.apache.org/jira/browse/SPARK-9254) | *Major* | **sbt-launch-lib.bash should use `curl --location` to support HTTP/HTTPS redirection**

The {{curl}} call in the script should use {{--location}} to support HTTP/HTTPS redirection, since target file(s) can be hosted on CDN nodes.


---

* [SPARK-9250](https://issues.apache.org/jira/browse/SPARK-9250) | *Minor* | **./dev/change-scala-version.sh should offer guidance what versions are accepted, i.e. 2.10 or 2.11**

With the commit f5b6dc5 there's this new way of building Spark with Scala 2.10 and 2.11. The help given is not very helpful and could be improved about the possible versions and their format.

{code}
  spark git:(master) ./dev/change-scala-version.sh
Usage: change-scala-version.sh \<version\>
{code}

I can see inside - that could be part of the help.

{code}
VALID\_VERSIONS=( 2.10 2.11 )
{code}


---

* [SPARK-9249](https://issues.apache.org/jira/browse/SPARK-9249) | *Minor* | **local variable assigned but may not be used**

local variable assigned but may not be used

For example:

{noformat}
R/deserialize.R:105:3: warning: local variable data assigned but may not be used
  data \<- readBin(con, raw(), as.integer(dataLen), endian = "big")
  ^~~~
R/deserialize.R:109:3: warning: local variable data assigned but may not be used
  data \<- readBin(con, raw(), as.integer(dataLen), endian = "big")
  ^~~~
{noformat}


---

* [SPARK-9248](https://issues.apache.org/jira/browse/SPARK-9248) | *Minor* | **Closing curly-braces should always be on their own line**

Closing curly-braces should always be on their own line

For example,
{noformat}
inst/tests/test\_sparkSQL.R:606:3: style: Closing curly-braces should always be on their own line, unless it's followed by an else.
  }, error = function(err) {
  ^
{noformat}


---

* [SPARK-9247](https://issues.apache.org/jira/browse/SPARK-9247) | *Critical* | **Use BytesToBytesMap in unsafe broadcast join**

For better performance (both CPU and memory)


---

* [SPARK-9246](https://issues.apache.org/jira/browse/SPARK-9246) | *Major* | **DistributedLDAModel predict top docs per topic**

For each topic, return top documents based on topicDistributions.

Synopsis:
{code}
/\*\*
 \* @param maxDocuments  Max docs to return for each topic
 \* @return Array over topics of (sorted top docs, corresponding doc-topic weights)
 \*/
def topDocumentsPerTopic(maxDocuments: Int): Array[(Array[Long], Array[Double])]
{code}

Note: We will need to make sure that the above return value format is Java-friendly.


---

* [SPARK-9245](https://issues.apache.org/jira/browse/SPARK-9245) | *Major* | **DistributedLDAModel predict top topic per doc-term instance**

For each (document, term) pair, return top topic.  Note that instances of (doc, term) pairs within a document (a.k.a. "tokens") are exchangeable, so we should provide an estimate per document-term, rather than per token.

Synopsis for DistributedLDAModel:
{code}
/\*\* @return RDD of (doc ID, vector of top topic index for each term) \*/
def topTopicAssignments: RDD[(Long, Vector)]
{code}
Note that using Vector will let us have a sparse encoding which is Java-friendly.


---

* [SPARK-9244](https://issues.apache.org/jira/browse/SPARK-9244) | *Minor* | **Increase some default memory limits**

There are a few memory limits that people hit often and that we could make higher, especially now that memory sizes have grown.

- spark.akka.frameSize: This defaults at 10 but is often hit for map output statuses in large shuffles. AFAIK the memory is not fully allocated up-front, so we can just make this larger and still not affect jobs that never sent a status that large.

- spark.executor.memory: Defaults at 512m, which is really small. We can at least increase it to 1g, though this is something users do need to set on their own.


---

* [SPARK-9243](https://issues.apache.org/jira/browse/SPARK-9243) | *Trivial* | **Update crosstab doc for pairs that have no occurrences**

The crosstab value for pairs that have no occurrences was changed from null to 0 in SPARK-7982. We should update the doc in Scala, Python, and SparkR.


---

* [SPARK-9240](https://issues.apache.org/jira/browse/SPARK-9240) | *Blocker* | **Hybrid aggregate operator using unsafe row**

We need a hybrid aggregate operator, which first tries hash-based aggregations and gracefully switch to sort-based aggregations if the hash map's memory footprint exceeds a given threshold (how to track memory footprint and how to set the threshold?).


---

* [SPARK-9238](https://issues.apache.org/jira/browse/SPARK-9238) | *Trivial* | **two extra useless entries for bytesOfCodePointInUTF8**

Only a trial thing, not sure if I understand correctly or not but I guess only 2 entries in bytesOfCodePointInUTF8 for the case of 6 bytes codepoint(1111110x) is enough.
Details can be found from https://en.wikipedia.org/wiki/UTF-8 in "Description" section.


---

* [SPARK-9236](https://issues.apache.org/jira/browse/SPARK-9236) | *Major* | **Left Outer Join with empty JavaPairRDD returns empty RDD**

When the \*left outer join\* is performed on a non-empty {{JavaPairRDD}} with a {{JavaPairRDD}} which was created with the {{emptyRDD()}} method the resulting RDD is empty. In the following unit test the latest assert fails.

{code}
import static org.assertj.core.api.Assertions.assertThat;

import java.util.Collections;

import lombok.val;

import org.apache.spark.SparkConf;
import org.apache.spark.api.java.JavaSparkContext;
import org.junit.Test;

import scala.Tuple2;

public class SparkTest {

  @Test
  public void joinEmptyRDDTest() {
    val sparkConf = new SparkConf().setAppName("test").setMaster("local");

    try (val sparkContext = new JavaSparkContext(sparkConf)) {
      val oneRdd = sparkContext.parallelize(Collections.singletonList("one"));
      val twoRdd = sparkContext.parallelize(Collections.singletonList("two"));
      val threeRdd = sparkContext.emptyRDD();

      val onePair = oneRdd.mapToPair(t -\> new Tuple2\<Integer, String\>(1, t));
      val twoPair = twoRdd.groupBy(t -\> 1);
      val threePair = threeRdd.groupBy(t -\> 1);

      assertThat(onePair.leftOuterJoin(twoPair).collect()).isNotEmpty();
      assertThat(onePair.leftOuterJoin(threePair).collect()).isNotEmpty();
    }
  }

}
{code}


---

* [SPARK-9233](https://issues.apache.org/jira/browse/SPARK-9233) | *Major* | **Enable code-gen in window function unit tests**

Right now, our {{HiveWindowFunctionQuerySuite.scala}} set code-gen to false, since code-gen is enabled by default, we need to enable code-gen for tests in this file and fix bugs we find.


---

* [SPARK-9232](https://issues.apache.org/jira/browse/SPARK-9232) | *Minor* | **Duplicate code in JSONRelation**

The following block appears identically in two places:

{code}
var success: Boolean = false
try {
  success = fs.delete(filesystemPath, true)
} catch {
  case e: IOException =\>
    throw new IOException(
      s"Unable to clear output directory ${filesystemPath.toString} prior"
        + s" to writing to JSON table:\n${e.toString}")
}
if (!success) {
  throw new IOException(
    s"Unable to clear output directory ${filesystemPath.toString} prior"
      + s" to writing to JSON table.")
  }
}
{code}

https://github.com/apache/spark/blob/e5d2c37c68ac00a57c2542e62d1c5b4ca267c89e/sql/core/src/main/scala/org/apache/spark/sql/json/JSONRelation.scala#L72

https://github.com/apache/spark/blob/e5d2c37c68ac00a57c2542e62d1c5b4ca267c89e/sql/core/src/main/scala/org/apache/spark/sql/json/JSONRelation.scala#L131


---

* [SPARK-9231](https://issues.apache.org/jira/browse/SPARK-9231) | *Minor* | **DistributedLDAModel method for top topics per document**

Helper method in DistributedLDAModel of this form:
{code}
/\*\*
 \* For each document, return the top k weighted topics for that document.
 \* @return RDD of (doc ID, topic indices, topic weights)
 \*/
def topTopicsPerDocument(k: Int): RDD[(Long, Array[Int], Array[Double])]
{code}

I believe the above method signature will be Java-friendly.


---

* [SPARK-9230](https://issues.apache.org/jira/browse/SPARK-9230) | *Major* | **SparkR RFormula should support StringType features**

StringType features will need to be encoded using OneHotEncoder to be used for regression. See umbrella design doc https://docs.google.com/document/d/10NZNSEurN2EdWM31uFYsgayIPfCFHiuIu3pCWrUmP\_c/edit?usp=sharing


---

* [SPARK-9228](https://issues.apache.org/jira/browse/SPARK-9228) | *Blocker* | **Combine unsafe and codegen into a single option**

Before QA, lets flip on features and consolidate unsafe and codegen.


---

* [SPARK-9225](https://issues.apache.org/jira/browse/SPARK-9225) | *Minor* | **LDASuite needs unit tests for empty documents**

We need to add a unit test to {{LDASuite}} which check that empty documents are handled appropriately without crashing. This would require defining an empty corpus within {{LDASuite}} and adding tests for the available LDA optimizers (currently EM and Online). Note that only {{SparseVector}}s can be empty.


---

* [SPARK-9224](https://issues.apache.org/jira/browse/SPARK-9224) | *Major* | **OnlineLDAOptimizer Performance Improvements**

OnlineLDAOptimizer's current implementation can be improved by using in-place updating (instead of reassignment to vars), reducing number of transpositions, and an outer product (instead of looping) to collect stats.


---

* [SPARK-9222](https://issues.apache.org/jira/browse/SPARK-9222) | *Minor* | **Make class instantiation variables in DistributedLDAModel [private] clustering**

This would enable testing the various class variables like docConcentration, topicConcentration etc


---

* [SPARK-9216](https://issues.apache.org/jira/browse/SPARK-9216) | *Major* | **Define KinesisBackedBlockRDDs**

https://docs.google.com/document/d/1k0dl270EnK7uExrsCE7jYw7PYx0YC935uBcxn3p0f58/edit?usp=sharing


---

* [SPARK-9215](https://issues.apache.org/jira/browse/SPARK-9215) | *Major* | **Implement WAL-free Kinesis receiver that give at-least once guarantee**

Currently, the KinesisReceiver can loose some data in the case of certain failures (receiver and driver failures). Using the write ahead logs can mitigate some of the problem, but it is not ideal because WALs dont work with S3 (eventually consistency, etc.) which is the most likely file system to be used in the EC2 environment. Hence, we have to take a different approach to improving reliability for Kinesis.

Detailed design doc - https://docs.google.com/document/d/1k0dl270EnK7uExrsCE7jYw7PYx0YC935uBcxn3p0f58/edit?usp=sharing


---

* [SPARK-9214](https://issues.apache.org/jira/browse/SPARK-9214) | *Major* | **support ml.NaiveBayes for Python**

support ml.NaiveBayes for Python


---

* [SPARK-9212](https://issues.apache.org/jira/browse/SPARK-9212) | *Trivial* | **update Netty version to "4.0.29.Final" for Netty Metrics**

In Netty version 4.0.29.Final, metrics for PooledByteBufAllocator is exposed directly, so that no need to get the memory data info in a hack way.


---

* [SPARK-9211](https://issues.apache.org/jira/browse/SPARK-9211) | *Minor* | **HiveComparisonTest generates incorrect file name for golden answer files on Windows**

The names of the golden answer files for the Hive test cases (test suites based on {{HiveComparisonTest}}) are generated using an MD5 hash of the query text. When the query text contains line breaks then the generated MD5 hash differs between Windows and Linux/OSX ({{\r\n}} vs {{\n}}).

This results in erroneously created golden answer files from just running a Hive comparison test and makes it impossible to modify or add new test cases with correctly named golden answer files on Windows.


---

* [SPARK-9208](https://issues.apache.org/jira/browse/SPARK-9208) | *Blocker* | **Audit DataFrame expression API for 1.5 release**

This ticket makes sure I go through all new APIs added and audit them before 1.5.0 release.


---

* [SPARK-9207](https://issues.apache.org/jira/browse/SPARK-9207) | *Critical* | **Turn on Parquet filter push-down by default**

We turned off Parquet filter push-down by default in Spark 1.4.0 and prior versions because of some Parquet side bugs in Parquet 1.6.0rc3. Now we've upgraded to 1.7.0, which fixed all those bugs. Should turn on Parquet filter push-down by default now.


---

* [SPARK-9206](https://issues.apache.org/jira/browse/SPARK-9206) | *Major* | **ClassCastException using HiveContext with GoogleHadoopFileSystem as fs.defaultFS**

Originally reported on StackOverflow: http://stackoverflow.com/questions/31478955/googlehadoopfilesystem-cannot-be-cast-to-hadoop-filesystem

Google's "bdutil" command-line tool (https://github.com/GoogleCloudPlatform/bdutil) is one of the main supported ways of deploying Hadoop and Spark cluster on Google Cloud Platform, and has default settings which configure fs.defaultFS to use the Google Cloud Storage connector for Hadoop (and performs installation of the connector jarfile on top of tarball-based Hadoop and Spark distributions).

Starting in Spark 1.4.1, taking a default bdutil-based Spark deployment, running "spark-shell", and then trying to read a file with sqlContext like:

{code}
sqlContext.parquetFile("gs://my-bucket/my-file.parquet")
{code}

results in the following:

{noformat}
15/07/20 20:59:14 DEBUG IsolatedClientLoader: shared class: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem
java.lang.RuntimeException: java.lang.ClassCastException: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem cannot be cast to org.apache.hadoop.fs.FileSystem
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:346)
	at org.apache.spark.sql.hive.client.ClientWrapper.\<init\>(ClientWrapper.scala:116)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.liftedTree1$1(IsolatedClientLoader.scala:172)
	at org.apache.spark.sql.hive.client.IsolatedClientLoader.\<init\>(IsolatedClientLoader.scala:168)
	at org.apache.spark.sql.hive.HiveContext.metadataHive$lzycompute(HiveContext.scala:213)
	at org.apache.spark.sql.hive.HiveContext.metadataHive(HiveContext.scala:176)
	at org.apache.spark.sql.hive.HiveContext$$anon$2.\<init\>(HiveContext.scala:371)
	at org.apache.spark.sql.hive.HiveContext.catalog$lzycompute(HiveContext.scala:371)
	at org.apache.spark.sql.hive.HiveContext.catalog(HiveContext.scala:370)
	at org.apache.spark.sql.hive.HiveContext$$anon$1.\<init\>(HiveContext.scala:383)
	at org.apache.spark.sql.hive.HiveContext.analyzer$lzycompute(HiveContext.scala:383)
	at org.apache.spark.sql.hive.HiveContext.analyzer(HiveContext.scala:382)
	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:931)
	at org.apache.spark.sql.DataFrame.\<init\>(DataFrame.scala:131)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.SQLContext.baseRelationToDataFrame(SQLContext.scala:438)
	at org.apache.spark.sql.DataFrameReader.parquet(DataFrameReader.scala:264)
	at org.apache.spark.sql.SQLContext.parquetFile(SQLContext.scala:1099)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\<init\>(\<console\>:19)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\<init\>(\<console\>:24)
	at $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.\<init\>(\<console\>:26)
	at $iwC$$iwC$$iwC$$iwC$$iwC.\<init\>(\<console\>:28)
	at $iwC$$iwC$$iwC$$iwC.\<init\>(\<console\>:30)
	at $iwC$$iwC$$iwC.\<init\>(\<console\>:32)
	at $iwC$$iwC.\<init\>(\<console\>:34)
	at $iwC.\<init\>(\<console\>:36)
	at \<init\>(\<console\>:38)
	at .\<init\>(\<console\>:42)
	at .\<clinit\>(\<console\>)
	at .\<init\>(\<console\>:7)
	at .\<clinit\>(\<console\>)
	at $print(\<console\>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at org.apache.spark.repl.SparkILoop.processLine$1(SparkILoop.scala:657)
	at org.apache.spark.repl.SparkILoop.innerLoop$1(SparkILoop.scala:665)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$loop(SparkILoop.scala:670)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:997)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:665)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Caused by: java.lang.ClassCastException: com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem cannot be cast to org.apache.hadoop.fs.FileSystem
	at org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:2595)
	at org.apache.hadoop.fs.FileSystem.access$200(FileSystem.java:91)
	at org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:2630)
	at org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:2612)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:370)
	at org.apache.hadoop.fs.FileSystem.get(FileSystem.java:169)
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:342)
	... 67 more
{noformat}

This appears to be a combination of https://github.com/apache/spark/commit/9ac8393663d759860c67799e000ec072ced76493 and its related "isolated classloader" changes with the IsolatedClientLoader.isSharedClass method including "com.google.\*" alongside java.lang.\*, java.net.\*, etc., as shared classes, presumably for inclusion of Guava and possibly protobuf and gson libraries.

Unfortunately, this also includes the Hadoop extended libraries like com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem (https://github.com/GoogleCloudPlatform/bigdata-interop) and com.google.cloud.bigtable.\* (https://github.com/GoogleCloudPlatform/cloud-bigtable-client).

This can be reproduced by downloading bdutil from https://github.com/GoogleCloudPlatform/bdutil, modifying bdutil/extensions/spark/spark\_env.sh to set SPARK\_HADOOP2\_TARBALL\_URI to some Spark 1.4.1 tarball URI (http URIs should work as well) and then deploying a cluster with:

{code}
./bdutil -p \<your project\> -b \<your GCS bucket\> -z us-central1-f -e hadoop2 -e spark deploy
./bdutil -p \<your project\> -b \<your GCS bucket\> -z us-central1-f -e hadoop2 -e spark shell
{code}

The last command opens an SSH session; then type:

{code}
spark-shell
\> sqlContext.parquetFile("gs://your-bucket/some/path/to/parquet/file.parquet")
{code}

The ClassCastException should then immediately get thrown.

The simple fix of simply excluding com.google.cloud.\* from being "shared classes" appears to work just fine in an end-to-end bdutil-based deployment.


---

* [SPARK-9204](https://issues.apache.org/jira/browse/SPARK-9204) | *Trivial* | **Add default params test to linear regression**

ML's logisitc regression has tests for the default params, we should have similar tests for linear regression.


---

* [SPARK-9202](https://issues.apache.org/jira/browse/SPARK-9202) | *Blocker* | **Worker should not have data structures which grow without bound**

Originally reported by Richard Marscher on the dev list:

{quote}
we have been experiencing issues in production over the past couple weeks with Spark Standalone Worker JVMs seeming to have memory leaks. They accumulate Old Gen until it reaches max and then reach a failed state that starts critically failing some applications running against the cluster.

I've done some exploration of the Spark code base related to Worker in search of potential sources of this problem and am looking for some commentary on a couple theories I have:

Observation 1: The `finishedExecutors` HashMap seem to only accumulate new entries over time unbounded. It only seems to be appended and never periodically purged or cleaned of older executors in line with something like the worker cleanup scheduler. https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/deploy/worker/Worker.scala#L473
{quote}

It looks like the finishedExecutors map is only read when rendering the Worker Web UI and in constructing REST API responses.  I think that we could address this leak by adding a configuration to cap the maximum number of retained executors, applications, etc.  We already have similar caps in the driver UI.  If we add this configuration, I think that we should pick some sensible default value rather than an unlimited one.  This is technically a user-facing behavior change but I think it's okay since the current behavior is to crash / OOM.

To fix this, we should:

- Add the new configurations to cap how much data we retain.
- Add a stress-tester to spark-perf so that we have a way to reproduce these leaks during QA.
- Add some unit tests to ensure that cleanup is performed at the right places. This test should be modeled after the memory-leak-prevention strategies that we employed in JobProgressListener and in other parts of the Driver UI.


---

* [SPARK-9201](https://issues.apache.org/jira/browse/SPARK-9201) | *Major* | **Integrate MLlib with SparkR using RFormula**

We need to interface R glm() and predict() with mllib R formula support.

Design doc from umbrella task: https://docs.google.com/document/d/10NZNSEurN2EdWM31uFYsgayIPfCFHiuIu3pCWrUmP\_c/edit


---

* [SPARK-9199](https://issues.apache.org/jira/browse/SPARK-9199) | *Major* | **Upgrade Tachyon dependency to 0.7.0**

Update the tachyon-client dependency from 0.6.4 to 0.7.0 in spark core.


---

* [SPARK-9198](https://issues.apache.org/jira/browse/SPARK-9198) | *Minor* | **Typo in PySpark SparseVector docs (bad index)**

Several places in the PySpark SparseVector docs have one defined as:
{code}
SparseVector(4, [2, 4], [1.0, 2.0])
{code}
The index 4 goes out of bounds (but this is not checked).


---

* [SPARK-9193](https://issues.apache.org/jira/browse/SPARK-9193) | *Major* | **Avoid assigning tasks to executors under killing**

Now, when some executors are killed by dynamic-allocation, it leads to some mis-assignment onto lost executors sometimes. Such kind of mis-assignment causes task failure(s) or even job failure if it repeats that errors for 4 times.

The root cause is that killExecutors doesn't remove those executors under killing ASAP. It depends on the OnDisassociated event to refresh the active working list later. The delay time really depends on your cluster status (from several milliseconds to sub-minute). When new tasks to be scheduled during that period of time, it will be assigned to those "active" but "under killing" executors. Then the tasks will be failed due to "executor lost". The better way is to exclude those executors under killing in the makeOffers(). Then all those tasks won't be allocated onto those executors "to be lost" any more.


---

* [SPARK-9192](https://issues.apache.org/jira/browse/SPARK-9192) | *Major* | **add initialization phase for nondeterministic expression**

Currently nondeterministic expression is broken without a explicit initialization phase.

Let me take `MonotonicallyIncreasingID` as an example. This expression need a mutable state to remember how many times it has been evaluated, so we use `@transient var count: Long` there. By being transient, the `count` will be reset to 0 and \*\*only\*\* to 0 when serialize and deserialize it, as deserialize transient variable will result to default value. There is \*no way\* to use another initial value for `count`, until we add the explicit initialization phase.
For now no nondeterministic expression need this feature, but we may add new ones with the need of a different initial value for mutable state in the future.

Another use case is local execution for LocalRelation, there is no serialize and deserialize phase and thus we can't reset mutable states for it.


---

* [SPARK-9191](https://issues.apache.org/jira/browse/SPARK-9191) | *Minor* | **Add ml.PCA user guide and code examples**

Add ml.PCA user guide document and code examples for Scala/Java/Python.


---

* [SPARK-9187](https://issues.apache.org/jira/browse/SPARK-9187) | *Minor* | **Timeline view may show negative value for running tasks**

For running tasks, the executorRunTime metrics is 0 which causes negative executorComputingTime in the timeline. It also causes an incorrect SchedulerDelay time.


---

* [SPARK-9183](https://issues.apache.org/jira/browse/SPARK-9183) | *Blocker* | **NPE / confusing error message when looking up missing function in Spark SQL**

Try running the following query in Spark Shell with Hive enabled:

{code}
sqlContext.sql("""select substr("abc", 0, len("ab") - 1)""")
{code}

This query is malformed since there's no {{len}} UDF.  Unfortunately, though, this gives a really confusing error as of Spark 1.4:

{code}
: java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionInfo(FunctionRegistry.java:643)
	at org.apache.hadoop.hive.ql.exec.FunctionRegistry.getFunctionInfo(FunctionRegistry.java:652)
	at org.apache.spark.sql.hive.HiveFunctionRegistry.lookupFunction(hiveUdfs.scala:54)
	at org.apache.spark.sql.hive.HiveContext$$anon$3.org$apache$spark$sql$catalyst$analysis$OverrideFunctionRegistry$$super$lookupFunction(HiveContext.scala:380)
	at org.apache.spark.sql.catalyst.analysis.OverrideFunctionRegistry$$anonfun$lookupFunction$2.apply(FunctionRegistry.scala:44)
	at org.apache.spark.sql.catalyst.analysis.OverrideFunctionRegistry$$anonfun$lookupFunction$2.apply(FunctionRegistry.scala:44)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.catalyst.analysis.OverrideFunctionRegistry$class.lookupFunction(FunctionRegistry.scala:44)
	at org.apache.spark.sql.hive.HiveContext$$anon$3.lookupFunction(HiveContext.scala:380)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$13$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:465)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$13$$anonfun$applyOrElse$5.applyOrElse(Analyzer.scala:463)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:221)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:242)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
[...]
{code}

In Spark 1.3, on the other hand, this gives a helpful message:

{code}
: java.lang.RuntimeException: Couldn't find function len
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$1.apply(hiveUdfs.scala:55)
	at org.apache.spark.sql.hive.HiveFunctionRegistry$$anonfun$1.apply(hiveUdfs.scala:55)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.hive.HiveFunctionRegistry.lookupFunction(hiveUdfs.scala:54)
	at org.apache.spark.sql.hive.HiveContext$$anon$4.org$apache$spark$sql$catalyst$analysis$OverrideFunctionRegistry$$super$lookupFunction(HiveContext.scala:267)
	at org.apache.spark.sql.catalyst.analysis.OverrideFunctionRegistry$$anonfun$lookupFunction$2.apply(FunctionRegistry.scala:43)
	at org.apache.spark.sql.catalyst.analysis.OverrideFunctionRegistry$$anonfun$lookupFunction$2.apply(FunctionRegistry.scala:43)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.catalyst.analysis.OverrideFunctionRegistry$class.lookupFunction(FunctionRegistry.scala:43)
	at org.apache.spark.sql.hive.HiveContext$$anon$4.lookupFunction(HiveContext.scala:267)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:431)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveFunctions$$anonfun$apply$12$$anonfun$applyOrElse$3.applyOrElse(Analyzer.scala:429)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:188)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:187)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:208)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
[...]
{code}


---

* [SPARK-9180](https://issues.apache.org/jira/browse/SPARK-9180) | *Trivial* | **Accept --name option in spark-submit**

Currently spark-shell command does not accept specifying its application name via {{--name}} option, as it is hard coded as {{"Spark shell"}} in {{org.apache.spark.repl.Main}}.
I think it's better if the app name is user configurable.
The major problem of using the default application name {{"Spark shell"}} is that Monitoring JSON API ({{http://\<spark-shell-host\>:4040/api/v1/applications/app-id/}}) cannot be used with spark-shell, as the application ID contains space character.


---

* [SPARK-9179](https://issues.apache.org/jira/browse/SPARK-9179) | *Minor* | **Allow committers to specify the primary author of the PR to be merged**

It's a common case that some contributor contributes an initial version of a feature/bugfix, and later on some other people (mostly committers) fork and add more improvements. When merging these PRs, we probably want to specify the original author as the primary author. Currently we can only do this by running
{noformat}
git commit --amend --author="name \<email\>"
{noformat}
manually right before the merge script pushes to Apache Git repo. It would be nice if the script accepts user specified primary author information.


---

* [SPARK-9178](https://issues.apache.org/jira/browse/SPARK-9178) | *Trivial* | **UTF8String empty string method**

Create a method in UTF8String that returns an empty string, in order to avoid calls of UTF8String.fromString("")


---

* [SPARK-9177](https://issues.apache.org/jira/browse/SPARK-9177) | *Major* | **Reuse Calendar instance in WeekOfYear**

Right now WeekOfYear creates a new Calendar instance for every record, both in code gen and in interpreted mode. We should just reuse the same Calendar instance (i.e. initialize it as a member variable in interpreted mode, and use mutable state in codegen mode).


---

* [SPARK-9175](https://issues.apache.org/jira/browse/SPARK-9175) | *Critical* | **BLAS.gemm fails to update matrix C when alpha==0 and beta!=1**

In the BLAS wrapper, gemm is supposed to update matrix C to be alpha \* A \* B + beta \* C. However, the current implementation will not update C as long as alpha == 0. This is incorrect when beta is not equal to 1. 

Example:
val p = 3 
val a = DenseMatrix.zeros(p,p)
val b = DenseMatrix.zeros(p,p)
var c = DenseMatrix.eye(p)
BLAS.gemm(0, a, b, 5, c)

c is unchanged in the Spark 1.4 even though it should be multiplied by 5 element-wise.

The bug is caused by the following in BLAS.gemm:
if (alpha == 0.0) {
  logDebug("gemm: alpha is equal to 0. Returning C.")
}

Will submit PR to fix this.


---

* [SPARK-9173](https://issues.apache.org/jira/browse/SPARK-9173) | *Major* | **UnionPushDown should also support Intersect and Except in addition to Union**

UnionPushDown transforms only Union. It should also transform Intersect and Except.


---

* [SPARK-9172](https://issues.apache.org/jira/browse/SPARK-9172) | *Major* | **DecimalPrecision should also support Intersect and Except in addition to Union**

DecimalPrecision transforms only Union. It should also transform Intersect and Except.


---

* [SPARK-9168](https://issues.apache.org/jira/browse/SPARK-9168) | *Major* | **Add nanvl expression**

Similar to Oracle's nanvl:

nanvl(v1, v2)

if v1 is NaN, returns v2; otherwise, returns v1.


---

* [SPARK-9166](https://issues.apache.org/jira/browse/SPARK-9166) | *Major* | **Hide JVM stack trace for IllegalArgumentException in Python**

We now hide stack trace for AnalysisException. We should also hide it for IllegalArgumentException.


See this ticket to see how to fix this problem: https://github.com/apache/spark/pull/7135


---

* [SPARK-9150](https://issues.apache.org/jira/browse/SPARK-9150) | *Critical* | **Create CodegenFallback and Unevaluable trait**

It is very hard to track which expression supports code generation or not. This patch removes the default gencode implementation from Expression, and moves the default fallback implementation into a new trait called CodegenFallback. Each concrete expression needs to either implement code generation, or mix in CodegenFallback. This makes it very easy to track which expressions have code generation implemented already.

Additionally, this patch creates an Unevaluable trait that can be used to track expressions that don't support evaluation (e.g. Star).


---

* [SPARK-9149](https://issues.apache.org/jira/browse/SPARK-9149) | *Minor* | **Add an example of spark.ml KMeans**

Create an example of KMeans API for spark.ml.


---

* [SPARK-9148](https://issues.apache.org/jira/browse/SPARK-9148) | *Critical* | **User-facing documentation for NaN handling semantics**

Once we've finalized our NaN changes for Spark 1.5, we need to create user-facing documentation to explain our chosen semantics.


---

* [SPARK-9147](https://issues.apache.org/jira/browse/SPARK-9147) | *Major* | **UnsafeRow should canonicalize NaN values**

NaN has many different representations in raw bytes.

When we set a double/float value, we should check whether it is NaN, and a binary representation that is canonicalized, so we can do comparison on bytes directly.


---

* [SPARK-9146](https://issues.apache.org/jira/browse/SPARK-9146) | *Critical* | **NaN should be greater than all other values**

Based on the design in SPARK-9079, NaN should be greater than all other non-NaN numeric values.


---

* [SPARK-9145](https://issues.apache.org/jira/browse/SPARK-9145) | *Critical* | **Equality test on NaN = NaN should return true**

Based on the design in SPARK-9079, we want NaN = NaN to return true in SQL/DataFrame.


---

* [SPARK-9144](https://issues.apache.org/jira/browse/SPARK-9144) | *Major* | **Remove DAGScheduler.runLocallyWithinThread and spark.localExecution.enabled**

Spark has an option called {{spark.localExecution.enabled}}; according to the docs:

{quote}
Enables Spark to run certain jobs, such as first() or take() on the driver, without sending tasks to the cluster. This can make certain jobs execute very quickly, but may require shipping a whole partition of data to the driver.
{quote}

This feature ends up adding quite a bit of complexity to DAGScheduler, especially in the {{runLocallyWithinThread}} method, but as far as I know nobody uses this feature (I searched the mailing list and haven't seen any recent mentions of the configuration nor stacktraces including the runLocally method).  As a step towards scheduler complexity reduction, I propose that we remove this feature and all code related to it for Spark 1.5.


---

* [SPARK-9143](https://issues.apache.org/jira/browse/SPARK-9143) | *Major* | **Add planner rule for automatically inserting Unsafe \<-\> Safe row format converters**

Now that we have two different internal row formats, UnsafeRow and the old Java-object-based row format, we end up having to perform conversions between these two formats. These conversions should not be performed by the operators themselves; instead, the planner should be responsible for inserting appropriate format conversions when they are needed.


---

* [SPARK-9142](https://issues.apache.org/jira/browse/SPARK-9142) | *Major* | **Removing unnecessary self types in Catalyst**

A small change, based on code review and offline discussion with [~dragos].


---

* [SPARK-9141](https://issues.apache.org/jira/browse/SPARK-9141) | *Blocker* | **DataFrame recomputed instead of using cached parent.**

As I understand, DataFrame.cache() is supposed to work the same as RDD.cache(), so that repeated operations on it will use the cached results and not recompute the entire lineage. However, it seems that some DataFrame operations (e.g. withColumn) change the underlying RDD lineage so that cache doesn't work as expected.

Below is a Scala example that demonstrates this. First, I define two UDF's that  use println so that it is easy to see when they are being called. Next, I create a simple data frame with one row and two columns. Next, I add a column, cache it, and call count() to force the computation. Lastly, I add another column, cache it, and call count().

I would have expected the last statement to only compute the last column, since everything else was cached. However, because withColumn() changes the lineage, the whole data frame is recomputed.

{code:scala}
    // Examples udf's that println when called 
    val twice = udf { (x: Int) =\> println(s"Computed: twice($x)"); x \* 2 } 
    val triple = udf { (x: Int) =\> println(s"Computed: triple($x)"); x \* 3 } 

    // Initial dataset 
    val df1 = sc.parallelize(Seq(("a", 1))).toDF("name", "value") 

    // Add column by applying twice udf 
    val df2 = df1.withColumn("twice", twice($"value")) 
    df2.cache() 
    df2.count() //prints Computed: twice(1) 

    // Add column by applying triple udf 
    val df3 = df2.withColumn("triple", triple($"value")) 
    df3.cache() 
    df3.count() //prints Computed: twice(1)\nComputed: triple(1) 
{code}

I found a workaround, which helped me understand what was going on behind the scenes, but doesn't seem like an ideal solution. Basically, I convert to RDD then back DataFrame, which seems to freeze the lineage. The code below shows the workaround for creating the second data frame so cache will work as expected.

{code:scala}
    val df2 = {
      val tmp = df1.withColumn("twice", twice($"value"))
      sqlContext.createDataFrame(tmp.rdd, tmp.schema)
    }
{code}


---

* [SPARK-9138](https://issues.apache.org/jira/browse/SPARK-9138) | *Critical* | **Vectors.dense() in Python should accept numbers directly**

We already use this feature in doctests


---

* [SPARK-9133](https://issues.apache.org/jira/browse/SPARK-9133) | *Major* | **Add and Subtract should support date/timestamp and interval type**

Should support

date + interval
interval + date

timestamp + interval
interval + timestamp

The best way to support this is probably to resolve this to a date add/substract expression, rather than making add/subtract support these types.


---

* [SPARK-9132](https://issues.apache.org/jira/browse/SPARK-9132) | *Major* | **Implement code gen for Conv**

Would be great to refactor the thing slightly so we can do code generation. Since conv actually have some internal buffer, I think the best way is to create a new NumberConverter class that contains the internal buffer, and move most of the methods there. And then in Conv expression, we just create a NumberConverter, and call it to do the conversion.


---

* [SPARK-9131](https://issues.apache.org/jira/browse/SPARK-9131) | *Blocker* | **Python UDFs change data values**

I am having some troubles when using a custom udf in dataframes with pyspark 1.4.

I have rewritten the udf to simplify the problem and it gets even weirder. The udfs I am using do absolutely nothing, they just receive some value and output the same value with the same format.

I show you my code below:
{code}
c= a.join(b, a['ID'] == b['ID\_new'], 'inner')

c.filter(c['ID'] == '6000000002698917').show()

udf\_A = UserDefinedFunction(lambda x: x, DateType())
udf\_B = UserDefinedFunction(lambda x: x, DateType())
udf\_C = UserDefinedFunction(lambda x: x, DateType())

d = c.select(c['ID'], c['t1'].alias('ta'), udf\_A(vinc\_muestra['t2']).alias('tb'), udf\_B(vinc\_muestra['t1']).alias('tc'), udf\_C(vinc\_muestra['t2']).alias('td'))

d.filter(d['ID'] == '6000000002698917').show()
{code}

I am showing here the results from the outputs:
{code}
+----------------+----------------+----------+----------+
\|          ID     \|     ID\_new  \|     t1	 \|   t2     \|
+----------------+----------------+----------+----------+
\|6000000002698917\|   6000000002698917\|   2012-02-28\|   2014-02-28\|
\|6000000002698917\|   6000000002698917\|   2012-02-20\|   2013-02-20\|
\|6000000002698917\|   6000000002698917\|   2012-02-28\|   2014-02-28\|
\|6000000002698917\|   6000000002698917\|   2012-02-20\|   2013-02-20\|
\|6000000002698917\|   6000000002698917\|   2012-02-20\|   2013-02-20\|
\|6000000002698917\|   6000000002698917\|   2012-02-28\|   2014-02-28\|
\|6000000002698917\|   6000000002698917\|   2012-02-28\|   2014-02-28\|
\|6000000002698917\|   6000000002698917\|   2012-02-20\|   2013-02-20\|
+----------------+----------------+----------+----------+

+----------------+---------------+---------------+------------+------------+
\|       ID        \|	    ta	   \|	   tb	     \|	 tc	   \|     td	  \|
+----------------+---------------+---------------+------------+------------+
\|6000000002698917\|     2012-02-28\|       2007-03-05\|    2003-03-05\|    2014-02-28\|
\|6000000002698917\|     2012-02-20\|       2007-02-15\|    2002-02-15\|    2013-02-20\|
\|6000000002698917\|     2012-02-28\|       2007-03-10\|    2005-03-10\|    2014-02-28\|
\|6000000002698917\|     2012-02-20\|       2007-03-05\|    2003-03-05\|    2013-02-20\|
\|6000000002698917\|     2012-02-20\|       2013-08-02\|    2013-01-02\|    2013-02-20\|
\|6000000002698917\|     2012-02-28\|       2007-02-15\|    2002-02-15\|    2014-02-28\|
\|6000000002698917\|     2012-02-28\|       2007-02-15\|    2002-02-15\|    2014-02-28\|
\|6000000002698917\|     2012-02-20\|       2014-01-02\|    2013-01-02\|    2013-02-20\|
+----------------+---------------+---------------+------------+------------+
{code}
The problem here is that values at columns 'tb', 'tc' and 'td' in dataframe 'd' are completely different from values 't1' and 't2' in dataframe c even when my udfs are doing nothing. It seems like if values were somehow got from other registers (or just invented). Results are different between executions (apparently random).

Thanks in advance


---

* [SPARK-9128](https://issues.apache.org/jira/browse/SPARK-9128) | *Major* | **Get outerclasses and objects at the same time in ClosureCleaner**

Currently, in ClosureCleaner, the outerclasses and objects are retrieved using two different methods. However, the logic of the two methods is the same, and we can get both the outerclasses and objects with only one method calling.


---

* [SPARK-9127](https://issues.apache.org/jira/browse/SPARK-9127) | *Blocker* | **Rand/Randn codegen fails with long seed**

{code}
ERROR GenerateMutableProjection: failed to compile:

      public Object generate(org.apache.spark.sql.catalyst.expressions.Expression[] expr) {
        return new SpecificProjection(expr);
      }

      class SpecificProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {

        private org.apache.spark.sql.catalyst.expressions.Expression[] expressions = null;
        private org.apache.spark.sql.catalyst.expressions.MutableRow mutableRow = null;
        private org.apache.spark.util.random.XORShiftRandom rng4 = new org.apache.spark.util.random.XORShiftRandom(-5419823303878592871 + org.apache.spark.TaskContext.getPartitionId());

        public SpecificProjection(org.apache.spark.sql.catalyst.expressions.Expression[] expr) {
          expressions = expr;
          mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericMutableRow(2);
        }

        public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(org.apache.spark.sql.catalyst.expressions.MutableRow row) {
          mutableRow = row;
          return this;
        }

        /\* Provide immutable access to the last projected row. \*/
        public InternalRow currentValue() {
          return (InternalRow) mutableRow;
        }

        public Object apply(Object \_i) {
          InternalRow i = (InternalRow) \_i;

        boolean isNull0 = i.isNullAt(0);
        long primitive1 = isNull0 ?
            -1L : (i.getLong(0));

          if(isNull0)
            mutableRow.setNullAt(0);
          else
            mutableRow.setLong(0, primitive1);


      final double primitive3 = rng4.nextDouble();

          if(false)
            mutableRow.setNullAt(1);
          else
            mutableRow.setDouble(1, primitive3);


          return mutableRow;
        }
      }

org.codehaus.commons.compiler.CompileException: Line 10, Column 117: Value of decimal integer literal '5419823303878592871' is out of range
	at org.codehaus.janino.UnitCompiler.compileException(UnitCompiler.java:10473)
	at org.codehaus.janino.UnitCompiler.getConstantValue2(UnitCompiler.java:4696)
	at org.codehaus.janino.UnitCompiler.access$9200(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$11.visitIntegerLiteral(UnitCompiler.java:4402)
	at org.codehaus.janino.Java$IntegerLiteral.accept(Java.java:4321)
	at org.codehaus.janino.UnitCompiler.getConstantValue(UnitCompiler.java:4427)
	at org.codehaus.janino.UnitCompiler.getNegatedConstantValue2(UnitCompiler.java:4856)
	at org.codehaus.janino.UnitCompiler.getNegatedConstantValue2(UnitCompiler.java:4890)
	at org.codehaus.janino.UnitCompiler.access$10400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$12.visitIntegerLiteral(UnitCompiler.java:4823)
	at org.codehaus.janino.Java$IntegerLiteral.accept(Java.java:4321)
	at org.codehaus.janino.UnitCompiler.getNegatedConstantValue(UnitCompiler.java:4848)
	at org.codehaus.janino.UnitCompiler.getConstantValue2(UnitCompiler.java:4451)
	at org.codehaus.janino.UnitCompiler.access$8800(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$11.visitUnaryOperation(UnitCompiler.java:4393)
	at org.codehaus.janino.Java$UnaryOperation.accept(Java.java:3647)
	at org.codehaus.janino.UnitCompiler.getConstantValue(UnitCompiler.java:4427)
	at org.codehaus.janino.UnitCompiler.getConstantValue2(UnitCompiler.java:4498)
	at org.codehaus.janino.UnitCompiler.access$8900(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$11.visitBinaryOperation(UnitCompiler.java:4394)
	at org.codehaus.janino.Java$BinaryOperation.accept(Java.java:3768)
	at org.codehaus.janino.UnitCompiler.getConstantValue(UnitCompiler.java:4427)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4360)
	at org.codehaus.janino.UnitCompiler.invokeConstructor(UnitCompiler.java:6681)
	at org.codehaus.janino.UnitCompiler.compileGet2(UnitCompiler.java:4126)
	at org.codehaus.janino.UnitCompiler.access$7600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$10.visitNewClassInstance(UnitCompiler.java:3275)
	at org.codehaus.janino.Java$NewClassInstance.accept(Java.java:4085)
	at org.codehaus.janino.UnitCompiler.compileGet(UnitCompiler.java:3290)
	at org.codehaus.janino.UnitCompiler.compileGetValue(UnitCompiler.java:4368)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1659)
	at org.codehaus.janino.UnitCompiler.access$800(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$4.visitFieldDeclaration(UnitCompiler.java:933)
	at org.codehaus.janino.Java$FieldDeclaration.accept(Java.java:1818)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
	at org.codehaus.janino.UnitCompiler.initializeInstanceVariablesAndInvokeInstanceInitializers(UnitCompiler.java:6101)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2284)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:518)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
	at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
	at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
	at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
	at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
	at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
	at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
	at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
	at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
	at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
	at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
	at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
	at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
	at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.org$apache$spark$sql$catalyst$expressions$codegen$CodeGenerator$$doCompile(CodeGenerator.scala:268)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:292)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:289)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
	at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:256)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:89)
	at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:29)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:305)
	at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:302)
	at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:186)
	at org.apache.spark.sql.execution.Project.buildProjection$lzycompute(basicOperators.scala:42)
	at org.apache.spark.sql.execution.Project.buildProjection(basicOperators.scala:42)
	at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:45)
	at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:44)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
15/07/16 21:56:16 ERROR Project: Failed to generate mutable projection, fallback to interpreted
{code}


---

* [SPARK-9126](https://issues.apache.org/jira/browse/SPARK-9126) | *Major* | **StopwatchSuite shouldn't use Thread.sleep()**

Thread.sleep() might run overtime if Jenkins is busy.


---

* [SPARK-9122](https://issues.apache.org/jira/browse/SPARK-9122) | *Major* | **spark.mllib regression should support batch predict**

Currently, in spark.mllib, generalized linear regression models like LinearRegressionModel, RidgeRegressionModel and LassoModel support predict() via: LinearRegressionModelBase.predict, which only takes single rows (feature vectors).

It should support batch prediction, taking an RDD.  (See other classes which do this already such as NaiveBayesModel.)


---

* [SPARK-9121](https://issues.apache.org/jira/browse/SPARK-9121) | *Major* | **Get rid of the warnings about `no visible global function definition` in SparkR**

We have a lot of warnings about {{no visible global function definition}} in SparkR. So we should get rid of them.

{noformat}
R/utils.R:513:5: warning: no visible global function definition for processClosure
    processClosure(func.body, oldEnv, defVars, checkedFuncs, newEnv)
    ^~~~~~~~~~~~~~
{noformat}


---

* [SPARK-9119](https://issues.apache.org/jira/browse/SPARK-9119) | *Blocker* | **In some cases, we may save wrong decimal values to parquet**

{code}

\> 

import org.apache.spark.sql.Row
import org.apache.spark.sql.types.{StructType,StructField,StringType,DecimalType}
import org.apache.spark.sql.types.Decimal

val schema = StructType(Array(StructField("name", DecimalType(10, 5), false)))
val rowRDD = sc.parallelize(Array(Row(Decimal("67123.45"))))
val df = sqlContext.createDataFrame(rowRDD, schema)
df.registerTempTable("test")
df.show()

// +--------+
// \|    name\|
// +--------+
// \|67123.45\|
// +--------+

sqlContext.sql("create table testDecimal as select \* from test")
sqlContext.table("testDecimal").show()
// +--------+
// \|    name\|
// +--------+
// \|67.12345\|
// +--------+
{code}

The problem is when we do conversions, we do not use precision/scale info in the schema.


---

* [SPARK-9118](https://issues.apache.org/jira/browse/SPARK-9118) | *Minor* | **Implement integer array parameters for ml.param as IntArrayParam**

ml/param/params.scala lacks integer array parameter. It is needed for some models such as multilayer perceptron to specify the layer sizes. I suggest to implement it as IntArrayParam similarly to DoubleArrayParam.


---

* [SPARK-9116](https://issues.apache.org/jira/browse/SPARK-9116) | *Critical* | **python UDT in \_\_main\_\_ cannot be serialized by PySpark**

It's good that we can support UDT defined in \_\_main\_\_, for example, in pyspark-shell.


---

* [SPARK-9115](https://issues.apache.org/jira/browse/SPARK-9115) | *Major* | **date/time function: dayInYear**

dayInyear(date): Int

Returns the number of the day in the year of a given date.
e.g. dayInYear("2015-01-01") == 1, dayInYear("2015-12-31") == 365


---

* [SPARK-9114](https://issues.apache.org/jira/browse/SPARK-9114) | *Blocker* | **The returned value is not converted into internal type in Python UDF**

The returned value is not converted into internal type in Python UDF


---

* [SPARK-9112](https://issues.apache.org/jira/browse/SPARK-9112) | *Minor* | **Implement LogisticRegressionSummary similar to LinearRegressionSummary**

Since the API for LinearRegressionSummary has been merged, other models should follow suit.


---

* [SPARK-9109](https://issues.apache.org/jira/browse/SPARK-9109) | *Minor* | **Unpersist a graph object does not work properly**

Unpersist a graph object does not work properly.

Here is the code to produce 

{code}
import org.apache.spark.graphx.\_
import org.apache.spark.rdd.RDD
import org.slf4j.LoggerFactory
import org.apache.spark.graphx.util.GraphGenerators

val graph: Graph[Long, Long] =
  GraphGenerators.logNormalGraph(sc, numVertices = 100).mapVertices( (id, \_) =\> id.toLong ).mapEdges( e =\> e.attr.toLong)
  
graph.cache().numEdges
graph.unpersist()
{code}

There should not be any cached RDDs in storage (http://localhost:4040/storage/).


---

* [SPARK-9101](https://issues.apache.org/jira/browse/SPARK-9101) | *Major* | **Can't use null in selectExpr**

In 1.3.1 this worked:

{code:python}
df = sqlContext.createDataFrame([[1]], schema=['col'])
df.selectExpr('null as newCol').collect()
{code}

In 1.4.0 it fails with the following stacktrace:

{code}
Traceback (most recent call last):
  File "\<input\>", line 1, in \<module\>
  File "/opt/boxen/homebrew/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py", line 316, in collect
    cls = \_create\_cls(self.schema)
  File "/opt/boxen/homebrew/opt/apache-spark/libexec/python/pyspark/sql/dataframe.py", line 229, in schema
    self.\_schema = \_parse\_datatype\_json\_string(self.\_jdf.schema().json())
  File "/opt/boxen/homebrew/opt/apache-spark/libexec/python/pyspark/sql/types.py", line 519, in \_parse\_datatype\_json\_string
    return \_parse\_datatype\_json\_value(json.loads(json\_string))
  File "/opt/boxen/homebrew/opt/apache-spark/libexec/python/pyspark/sql/types.py", line 539, in \_parse\_datatype\_json\_value
    return \_all\_complex\_types[tpe].fromJson(json\_value)
  File "/opt/boxen/homebrew/opt/apache-spark/libexec/python/pyspark/sql/types.py", line 386, in fromJson
    return StructType([StructField.fromJson(f) for f in json["fields"]])
  File "/opt/boxen/homebrew/opt/apache-spark/libexec/python/pyspark/sql/types.py", line 347, in fromJson
    \_parse\_datatype\_json\_value(json["type"]),
  File "/opt/boxen/homebrew/opt/apache-spark/libexec/python/pyspark/sql/types.py", line 535, in \_parse\_datatype\_json\_value
    raise ValueError("Could not parse datatype: %s" % json\_value)
ValueError: Could not parse datatype: null
{code}

https://github.com/apache/spark/blob/v1.4.0/python/pyspark/sql/types.py#L461

The cause:\_atomic\_types doesn't contain NullType


---

* [SPARK-9095](https://issues.apache.org/jira/browse/SPARK-9095) | *Major* | **Removes old Parquet support code**

As the new Parquet external data source matures, we should remove the old Parquet support now.


---

* [SPARK-9094](https://issues.apache.org/jira/browse/SPARK-9094) | *Minor* | **Increase io.dropwizard.metrics dependency to 3.1.2**

This change is described in pull request:
https://github.com/apache/spark/pull/7493


---

* [SPARK-9093](https://issues.apache.org/jira/browse/SPARK-9093) | *Major* | **Fix single-quotes strings in SparkR**

We should get rid of the warnings about using single-quotes like that.

{noformat}
inst/tests/test\_sparkSQL.R:60:28: style: Only use double-quotes.
               list(type = 'array', elementType = "integer", containsNull = TRUE))
                           ^~~~~~~
{noformat}


---

* [SPARK-9092](https://issues.apache.org/jira/browse/SPARK-9092) | *Major* | **Make --num-executors compatible with dynamic allocation**

Currently when you enable dynamic allocation, you can't use --num-executors or the property spark.executor.instances. If we are to enable dynamic allocation by default, we should make these work so that existing workloads don't fail


---

* [SPARK-9090](https://issues.apache.org/jira/browse/SPARK-9090) | *Trivial* | **Fix definition of residual in LinearRegressionSummary**

Residual is defined as label - prediction (https://en.wikipedia.org/wiki/Least\_squares); we need to update {{LinearRegressionSummary}} to be consistent.


---

* [SPARK-9085](https://issues.apache.org/jira/browse/SPARK-9085) | *Major* | **Remove LeafNode, UnaryNode, BinaryNode from TreeNode**

They are not very useful, and cause problems with toString due to the order they are mixed in.


---

* [SPARK-9083](https://issues.apache.org/jira/browse/SPARK-9083) | *Major* | **If order by clause has non-deterministic expressions, we should add a project to materialize results of these expressions**

When a order by clause has a non-deterministic expression, we actually evaluate it twice, once in the exchange operator when we try to figure out the range partitioner's boundaries and once in the sort operator. We should use a project to materialize the result first.


---

* [SPARK-9082](https://issues.apache.org/jira/browse/SPARK-9082) | *Major* | **Filter using non-deterministic expressions should not be pushed down**

For example,
{code}
val df = sqlContext.range(1, 10).select($"id", rand(0).as('r))
df.as("a").join(df.filter($"r" \< 0.5).as("b"), $"a.id" === $"b.id").explain(true)
{code}
The plan is 
{code}
== Physical Plan ==
ShuffledHashJoin [id#55323L], [id#55327L], BuildRight
 Exchange (HashPartitioning 200)
  Project [id#55323L,Rand 0 AS r#55324]
   PhysicalRDD [id#55323L], MapPartitionsRDD[42268] at range at \<console\>:37
 Exchange (HashPartitioning 200)
  Project [id#55327L,Rand 0 AS r#55325]
   Filter (LessThan)
    PhysicalRDD [id#55327L], MapPartitionsRDD[42268] at range at \<console\>:37
{code}
The rand get evaluated twice instead of once. 

This is caused by when we push down predicates we replace the attribute reference in the predicate with the actual expression.


---

* [SPARK-9080](https://issues.apache.org/jira/browse/SPARK-9080) | *Critical* | **IsNaN expression**

Add IsNaN expression to return true if the input double/float value is NaN.


---

* [SPARK-9079](https://issues.apache.org/jira/browse/SPARK-9079) | *Major* | **Design NaN semantics**

1. What should NaN = NaN return?

NaN = NaN should return true.

2. If we see NaN in the group by key column, should we group NaN values into one group, or into different groups?

All NaN values should be grouped together.

3. What about NaN in join keys?

NaN should be treated as a normal value in join keys.

4. When aggregating over columns containing NaN, should the result be NaN, or should the result exclude NaN values (treating them like nulls)?

This is TO BE DECIDED. By default, the behavior is to return NaN.


5. Where should NaN go in sorting?

NaN should go last when in ascending order, larger than any other numeric value.


Note that 5 is much more important than the other 4 since right now the sorter throws exceptions on NaN values. See SPARK-8797.


---

* [SPARK-9077](https://issues.apache.org/jira/browse/SPARK-9077) | *Trivial* | **Improve error message for decision trees when numExamples \< maxCategoriesPerFeature**

See [SPARK-9075]'s discussion for details.  We should improve the current error message to recommend that the user remove the high-arity categorical features.


---

* [SPARK-9076](https://issues.apache.org/jira/browse/SPARK-9076) | *Major* | **Improve NaN value handling**

This is an umbrella ticket for handling NaN values.

For general design, please see https://issues.apache.org/jira/browse/SPARK-9079


---

* [SPARK-9074](https://issues.apache.org/jira/browse/SPARK-9074) | *Major* | **Expose more of the YARN commandline API to the SparkLauncher**

It would be helpful if the SparkLauncher was extended to support more of the spark commandline options, including but not limited to:

\* {{--proxy-user}}
\* {{--driver-cores}}
\* {{--queue}}
\* {{--archives}}
\* {{--principal}}
\* {{--keytab}}
\* {{--packages}}
\* {{--repositories}}


---

* [SPARK-9073](https://issues.apache.org/jira/browse/SPARK-9073) | *Minor* | **spark.ml Models copy() should call setParent when there is a parent**

Examples with this mistake include:
\* [https://github.com/apache/spark/blob/9716a727fb2d11380794549039e12e53c771e120/mllib/src/main/scala/org/apache/spark/ml/classification/DecisionTreeClassifier.scala#L119]
\* [https://github.com/apache/spark/blob/9716a727fb2d11380794549039e12e53c771e120/mllib/src/main/scala/org/apache/spark/ml/recommendation/ALS.scala#L220]

Whomever writes a PR for this JIRA should check all spark.ml Model's copy() methods and set copy's {{Model.parent}} when available.  Also verify in unit tests (possibly in a standard method checking Models to share code).


---

* [SPARK-9071](https://issues.apache.org/jira/browse/SPARK-9071) | *Major* | **MonotonicallyIncreasingID and SparkPartitionID should be marked as nondeterministic**

They should override deterministic to return false.


---

* [SPARK-9070](https://issues.apache.org/jira/browse/SPARK-9070) | *Trivial* | **JavaDataFrameSuite teardown NPEs if setup failed**

The hive test {{JavaDataFrameSuite}} NPEs in teardown if setup failed.


---

* [SPARK-9069](https://issues.apache.org/jira/browse/SPARK-9069) | *Blocker* | **Remove DecimalType unlimited precision/scale support**

We should remove DecimalType.Unlimited, because BigDecimal does not really support unlimited precision, especially for division.

We can have maximum precision as 38 (to match with Hive 0.13+). The default precision and scale could be (38, 18).


---

* [SPARK-9068](https://issues.apache.org/jira/browse/SPARK-9068) | *Minor* | **refactor the implicit type cast code**

based on https://github.com/apache/spark/pull/7348


---

* [SPARK-9067](https://issues.apache.org/jira/browse/SPARK-9067) | *Major* | **Memory overflow and open file limit exhaustion for NewParquetRDD+CoalescedRDD**

If coalesce transformation with small number of output partitions (in my case 16) is applied to large Parquet file (in my has about 150Gb with 215k partitions), then it case OutOfMemory exceptions 250Gb is not enough) and open file limit exhaustion (with limit set to 8k).

The source of the problem is in SqlNewHad\oopRDD.compute method:
{quote}
      val reader = format.createRecordReader(
        split.serializableHadoopSplit.value, hadoopAttemptContext)
      reader.initialize(split.serializableHadoopSplit.value, hadoopAttemptContext)

      // Register an on-task-completion callback to close the input stream.
      context.addTaskCompletionListener(context =\> close())
{quote}

Created Parquet file reader is intended to be closed at task completion time. This reader contains a lot of references to  parquet.bytes.BytesInput object which in turn contains reference sot large byte arrays (some of them are several megabytes).
As far as in case of CoalescedRDD task is completed only after processing larger number of parquet files, it cause file handles exhaustion and memory overflow.


---

* [SPARK-9062](https://issues.apache.org/jira/browse/SPARK-9062) | *Minor* | **Change output type of Tokenizer to Array(String, true)**

Currently output type of Tokenizer is Array(String, false), which is not compatible with Word2Vec and Other transformers since their input type is Array(String, true). Seq[String] in udf will be treated as Array(String, true) by default. 

I'm also thinking for Nullable columns, maybe tokenizer should return Array(null) for null value in the input.


---

* [SPARK-9061](https://issues.apache.org/jira/browse/SPARK-9061) | *Blocker* | **Fix and re-enable ignored tests in ExpressionTypeCheckingSuite**

In SPARK-8993 we disabled couple tests in ExpressionTypeCheckingSuite. We should fix them and re-enable them.


---

* [SPARK-9060](https://issues.apache.org/jira/browse/SPARK-9060) | *Major* | **Revert SPARK-8359, SPARK-8800, and SPARK-8677**

Since it is hard to fix these problems without causing new issues, I'd say we should revert these changes and figure out the proper improvement of our decimal type support.


---

* [SPARK-9056](https://issues.apache.org/jira/browse/SPARK-9056) | *Trivial* | **Rename configuration `spark.streaming.minRememberDuration` to `spark.streaming.fileStream.minRememberDuration`**

spark.streaming.minRememberDuration is confusing as it is not immediately evident what this configuration is about. Best to rename it to spark.streaming.fileStream.minRememberDuration.


---

* [SPARK-9055](https://issues.apache.org/jira/browse/SPARK-9055) | *Major* | **WidenTypes should also support Intersect and Except in addition to Union**

HiveTypeCoercion.WidenTypes only supports Union right now. It should also support Intersect and Except.


---

* [SPARK-9054](https://issues.apache.org/jira/browse/SPARK-9054) | *Major* | **Rename RowOrdering to InterpretedOrdering and use newOrdering to build orderings**

There are a few places where we still manually construct RowOrdering instead of using SparkPlan.newOrdering.  We should update these to use newOrdering and should rename RowOrdering to InterpretedOrdering to make its function slightly more obvious.


---

* [SPARK-9053](https://issues.apache.org/jira/browse/SPARK-9053) | *Major* | **Fix spaces around parens, infix operators etc.**

We have a number of style errors which look like 

{code}
Place a space before left parenthesis
...
Put spaces around all infix operators.
{code}

However some of the warnings are spurious (example space around infix operator in
{code}
expect\_equal(collect(select(df, hypot(df$a, df$b)))[4, "HYPOT(a, b)"], sqrt(4^2 + 8^2))
{code}

We should add a ignore rule for these spurious examples


---

* [SPARK-9052](https://issues.apache.org/jira/browse/SPARK-9052) | *Major* | **Fix comments after curly braces**

Right now we have a number of style check errors of the form 

{code}
Opening curly braces should never go on their own line and should always and be followed by a new line.
{code}


---

* [SPARK-9050](https://issues.apache.org/jira/browse/SPARK-9050) | *Major* | **Remove out-of-date code in Exchange that was obsoleted by SPARK-8317**

SPARK-8317 changed the SQL Exchange operator so that it no longer pushed sorting into Spark's shuffle layer, a change which allowed more efficient SQL-specific sorters to be used.

There's still some leftover cleanup from that patch which needs to be performed: Exchange's constructor should no longer accept a {{newOrdering}} since it's no longer used.


---

* [SPARK-9049](https://issues.apache.org/jira/browse/SPARK-9049) | *Major* | **UnsafeExchange operator**

Create a version of Exchange that expects UnsafeRow as inputs.


---

* [SPARK-9048](https://issues.apache.org/jira/browse/SPARK-9048) | *Major* | **UnsafeSqlSerializer**

Create a UnsafeSqlSerializer that serializes UnsafeRows.


---

* [SPARK-9045](https://issues.apache.org/jira/browse/SPARK-9045) | *Blocker* | **Fix Scala 2.11 build break due in UnsafeExternalRowSorter**

{code}
[error] /home/jenkins/workspace/Spark-Master-Scala211-Compile/sql/catalyst/src/main/java/org/apache/spark/sql/execution/UnsafeExternalRowSorter.java:135: error: \<anonymous org.apache.spark.sql.execution.UnsafeExternalRowSorter$1\> is not abstract and does not override abstract method \<B\>minBy(Function1\<InternalRow,B\>,Ordering\<B\>) in TraversableOnce
[error]       return new AbstractScalaRowIterator() {
[error]                                             ^
[error]   where B,A are type-variables:
[error]     B extends Object declared in method \<B\>minBy(Function1\<A,B\>,Ordering\<B\>)
[error]     A extends Object declared in interface TraversableOnce
[error] 1 error
[error] Compile failed at Jul 14, 2015 2:26:25 PM [26.443s]
{code}

It turns out that this can be fixed by making AbstractScalaRowIterator into a concrete class instead of an abstract class.


---

* [SPARK-9036](https://issues.apache.org/jira/browse/SPARK-9036) | *Minor* | **SparkListenerExecutorMetricsUpdate messages not included in JsonProtocol**

The JsonProtocol added in SPARK-3454 [doesn't include\|https://github.com/apache/spark/blob/v1.4.1-rc4/core/src/main/scala/org/apache/spark/util/JsonProtocol.scala#L95-L96] code for ser/de of [{{SparkListenerExecutorMetricsUpdate}}\|https://github.com/apache/spark/blob/v1.4.1-rc4/core/src/main/scala/org/apache/spark/scheduler/SparkListener.scala#L107-L110] messages.

The comment notes that they are "not used", which presumably refers to the fact that the [{{EventLoggingListener}} doesn't write these events\|https://github.com/apache/spark/blob/v1.4.1-rc4/core/src/main/scala/org/apache/spark/scheduler/EventLoggingListener.scala#L200-L201].

However, individual listeners can and should make that determination for themselves; I have recently written custom listeners that would like to consume metrics-update messages as JSON, so it would be nice to round out the JsonProtocol implementation by supporting them.


---

* [SPARK-9031](https://issues.apache.org/jira/browse/SPARK-9031) | *Major* | **Merge BlockObjectWriter and DiskBlockObject writer to remove abstract class**

BlockObjectWriter has only one concrete non-test class, DiskBlockObjectWriter.  In order to simplify the code in preparation for other refactorings, I think that we should remove this base class and have only DiskBlockObjectWriter.


---

* [SPARK-9030](https://issues.apache.org/jira/browse/SPARK-9030) | *Major* | **Add Kinesis.createStream unit tests that actual sends data**

Current Kinesis unit tests do not test createStream by sending data. This JIRA is to add such unit test.


---

* [SPARK-9028](https://issues.apache.org/jira/browse/SPARK-9028) | *Major* | **Add CountVectorizer as an estimator to generate CountVectorizerModel**

Add an estimator for CountVectorizerModel. The estimator will extract a vocabulary from document collections according to the term frequency.


---

* [SPARK-9024](https://issues.apache.org/jira/browse/SPARK-9024) | *Major* | **Unsafe HashJoin**

Create a version of BroadcastJoin that accepts UnsafeRow as inputs, and outputs UnsafeRow as outputs.


---

* [SPARK-9023](https://issues.apache.org/jira/browse/SPARK-9023) | *Major* | **UnsafeExchange**

Create a version of Exchange that accepts UnsafeRow and exploits the fact that data is passed in as bytes.


---

* [SPARK-9022](https://issues.apache.org/jira/browse/SPARK-9022) | *Major* | **UnsafeProject**

Create a version of Project that projects output out directly into serialized UnsafeRow.


---

* [SPARK-9021](https://issues.apache.org/jira/browse/SPARK-9021) | *Major* | ** Change RDD.aggregate() to do reduce(mapPartitions()) instead of mapPartitions.fold()**

Please see pull request for more information.

Currently, PySpark will run an unnecessary comboOp on each partition, combining zeroValue and the results of mapPartitions. Since the zeroValue used in this comboOp is the same reference as the zeroValue used for mapPartitions in each partition, unexpected behavior can happen if zeroValue is a mutable object.

Instead, RDD.aggregate() should do a reduction on the results of each mapPartitions task. This way, we remove the unnecessary initial comboOp on each partition and also correct the unexpected behavior for mutable zeroValues.


---

* [SPARK-9020](https://issues.apache.org/jira/browse/SPARK-9020) | *Major* | **Support mutable state in code gen expressions**

Some expressions have state in them (e.g. Rand, MonotonicallyIncreasingID). We currently don't support code-gen any expressions that have mutable states.


---

* [SPARK-9018](https://issues.apache.org/jira/browse/SPARK-9018) | *Major* | **Implement a generic Stopwatch utility for ML algorithms**

The Timer utility should be based on the one implemented in trees. In particular, we should offer two versions:

1. a global timer that is initialized on the driver and use accumulator to aggregate time
2. a local timer that is initialized on the worker, and only provide per task measurement.

1) needs some performance benchmark and guidance on the granularity.


---

* [SPARK-9016](https://issues.apache.org/jira/browse/SPARK-9016) | *Minor* | **Make random forest classifier extend Classifier abstraction**

This is a blocking issue for https://issues.apache.org/jira/browse/SPARK-8069 . Since we want to add thresholding/cutoff support to RandomForest and we wish to do this in a general way we should move RandomForest over to the Clasisfication trait.


---

* [SPARK-9015](https://issues.apache.org/jira/browse/SPARK-9015) | *Minor* | **Maven cleanup / Clean Project Import in scala-ide**

Cleanup maven for a clean import in scala-ide / eclipse.

The outstanging PR contains things like removal of groovy plugin and some more maven cleanup goes here.


In order to make it a seamless experience two more things have to be merged upstream:
1) ide automatically generate jva sources from idl - https://issues.apache.org/jira/browse/AVRO-1671
2) set scala version in ide based on maven config - https://github.com/sonatype/m2eclipse-scala/issues/30


---

* [SPARK-9012](https://issues.apache.org/jira/browse/SPARK-9012) | *Major* | **Accumulators in the task table should be escaped**

If running the following codes, the task table will be broken because accumulators aren't escaped.
{code}
val a = sc.accumulator(1, "\<table\>")
sc.parallelize(1 to 10).foreach(i =\> a += i)
{code}


---

* [SPARK-9010](https://issues.apache.org/jira/browse/SPARK-9010) | *Trivial* | **Improve the Spark Configuration document about `spark.kryoserializer.buffer`**

The meaning of spark.kryoserializer.buffer should be "Initial size of Kryo's serialization buffer. Note that there will be one buffer per core on each worker. This buffer will grow up to spark.kryoserializer.buffer.max if needed.".

The spark.kryoserializer.buffer.max.mb is out-of-date in spark 1.4.


---

* [SPARK-9006](https://issues.apache.org/jira/browse/SPARK-9006) | *Blocker* | **TimestampType may loss a microsecond after a round trip in Python DataFrame**

This bug causes SQLTests.test\_time\_with\_timezone flaky in Python 3.


---

* [SPARK-9005](https://issues.apache.org/jira/browse/SPARK-9005) | *Major* | **RegressionMetrics computing incorrect explainedVariance**

{{RegressionMetrics}} currently computes explainedVariance using {{summary.variance(1)}} (variance of the residuals) where the [Wikipedia definition\|https://en.wikipedia.org/wiki/Fraction\_of\_variance\_unexplained] uses the residual sum of squares {{math.pow(summary.normL2(1), 2)}}. The two coincide only when the predictor is unbiased (e.g. an intercept term is included in a linear model), but this is not always the case. We should change to be consistent.


---

* [SPARK-9001](https://issues.apache.org/jira/browse/SPARK-9001) | *Minor* | **sbt doc fails due to javadoc errors**

Running `build/sbt doc` on master fails due to errors javadocs. 

This is an issues since `build/sbt publish-local` depends on building the docs.

Example error:

[info] Generating /spark/unsafe/target/scala-2.10/api/org/apache/spark/unsafe/bitset/BitSet.html...
[error] /spark/unsafe/src/main/java/org/apache/spark/unsafe/bitset/BitSet.java:93: error: bad use of '\>'
[error]    \*  for (long i = bs.nextSetBit(0); i \>= 0; i = bs.nextSetBit(i + 1)) {
[error]                                         ^


---

* [SPARK-9000](https://issues.apache.org/jira/browse/SPARK-9000) | *Critical* | **Support generic item type in PrefixSpan**

In SPARK-6487, we only support Int type. It requires users to encode other types into integer to use PrefixSpan. We should be able to do this inside PrefixSpan, similar to FPGrowth. This should be done before 1.5 since it changes APIs.


---

* [SPARK-8999](https://issues.apache.org/jira/browse/SPARK-8999) | *Critical* | **Support non-temporal sequence in PrefixSpan**

In SPARK-6487, we assume that all items are ordered. However, we should support non-temporal sequences in PrefixSpan. This should be done before 1.5 because it changes PrefixSpan APIs.

We can use `Array[Array[Int]]` or follow SPMF to use `Array[Int]` and use -1 to mark itemset boundaries. The latter is more efficient for storage. If we support generic item type, we can use null.


---

* [SPARK-8998](https://issues.apache.org/jira/browse/SPARK-8998) | *Major* | **Collect enough frequent prefixes before projection in PrefixSpan**

The implementation in SPARK-6487 might have scalability issues when the number of frequent items is very small. In this case, we can generate candidate sets of higher orders using Apriori-like algorithms and count them, until we collect enough prefixes.


---

* [SPARK-8997](https://issues.apache.org/jira/browse/SPARK-8997) | *Major* | **Improve LocalPrefixSpan performance**

We can improve the performance by:

1. run should output Iterator instead of Array
2. Local count shouldn't use groupBy, which creates too many arrays. We can use PrimitiveKeyOpenHashMap
3. We can use list to avoid materialize frequent sequences


---

* [SPARK-8996](https://issues.apache.org/jira/browse/SPARK-8996) | *Major* | **Add Python API for Kolmogorov-Smirnov Test**

Add Python API for the Kolmogorov-Smirnov test implemented in SPARK-8598. It should be similar to ChiSqTest in Python.


---

* [SPARK-8995](https://issues.apache.org/jira/browse/SPARK-8995) | *Major* | **Cast date strings with date, date and time and just time information to DateType and TimestampTzpe**

Tests of https://github.com/apache/spark/pull/6981 fail, because we can not cast strings like '13:18:08' to a valid date and extract the hours later. It's not possible to parse strings that contains date and time information to date, like '2015-03-18 12:25:49'


---

* [SPARK-8994](https://issues.apache.org/jira/browse/SPARK-8994) | *Trivial* | **Tiny cleanups to Params, Pipeline**

Small cleanups per remaining comments in [https://github.com/apache/spark/pull/5820] which resolved [SPARK-5956]


---

* [SPARK-8993](https://issues.apache.org/jira/browse/SPARK-8993) | *Major* | **More comprehensive type checking in expressions**

This patch makes the following changes:

1. ExpectsInputTypes only defines expected input types, but does not perform any implicit type casting.
2. ImplicitCastInputTypes is a new trait that defines both expected input types, as well as performs implicit type casting.
3. BinaryOperator has a new abstract function "inputType", which defines the expected input type for both left/right. Concrete BinaryOperator expressions no longer perform any implicit type casting.
4. For BinaryOperators, convert NullType (i.e. null literals) into some accepted type so BinaryOperators don't need to handle NullTypes.


---

* [SPARK-8991](https://issues.apache.org/jira/browse/SPARK-8991) | *Trivial* | **Update SharedParamsCodeGen's Generated Documentation**

We no longer need

Specifically, the [generated documentation in SharedParamsCodeGen\|https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/ml/param/shared/SharedParamsCodeGen.scala#L137] should be modified from

{{code}}
      \|/\*\*
      \| \* (private[ml]) Trait for shared param $name$defaultValueDoc.
      \| \*/
{{code}}

to

{{code}}
      \|/\*\*
      \| \* Trait for shared param $name$defaultValueDoc.
      \| \*/
{{code}}


---

* [SPARK-8990](https://issues.apache.org/jira/browse/SPARK-8990) | *Major* | **DataFrameReader.parquet() ignores user specified data source options**

A bad consequence of this is that {{sqlContext.read.parquet(path)}} always do schema merging. For example:
{code}
import sqlContext.\_
import sqlContext.implicits.\_

val path = "s3n://my-bucket/parquet/tiny"
range(0, 10).coalesce(1).write.mode("overwrite").parquet(path)

// Explicitly disables schema merging
read.option("mergeSchema", "false").format("parquet").load(path)
{code}
However, we still see all files are opened for schema discovery:
{noformat}
15/07/10 14:46:52 INFO s3native.NativeS3FileSystem: Opening 's3n://databricks-lian/parquet/tiny/\_metadata' for reading
15/07/10 14:46:52 INFO s3native.NativeS3FileSystem: Opening key 'parquet/tiny/\_metadata' for reading at position '314'
15/07/10 14:46:52 INFO s3native.NativeS3FileSystem: Opening 's3n://databricks-lian/parquet/tiny/part-r-00000-da490c43-15e2-46b5-95ff-4863e6ab1cc4.gz.parquet' for reading
15/07/10 14:46:52 INFO s3native.NativeS3FileSystem: Opening 's3n://databricks-lian/parquet/tiny/\_common\_metadata' for reading
15/07/10 14:46:52 INFO s3native.NativeS3FileSystem: Opening key 'parquet/tiny/part-r-00000-da490c43-15e2-46b5-95ff-4863e6ab1cc4.gz.parquet' for reading at position '345'
15/07/10 14:46:52 INFO s3native.NativeS3FileSystem: Opening key 'parquet/tiny/\_common\_metadata' for reading at position '191'
15/07/10 14:46:52 INFO s3native.NativeS3FileSystem: Opening key 'parquet/tiny/\_metadata' for reading at position '4'
15/07/10 14:46:52 INFO s3native.NativeS3FileSystem: Opening key 'parquet/tiny/part-r-00000-da490c43-15e2-46b5-95ff-4863e6ab1cc4.gz.parquet' for reading at position '97'
15/07/10 14:46:52 INFO s3native.NativeS3FileSystem: Opening key 'parquet/tiny/\_common\_metadata' for reading at position '4'
{noformat}
To workaround this issue, use the following instead:
{noformat}
sqlContext.read.option("mergeSchema", "false").format("parquet").load(path)
{noformat}


---

* [SPARK-8988](https://issues.apache.org/jira/browse/SPARK-8988) | *Major* | **Driver logs links is missing on secure cluster**

On a secure cluster, the {{NodeReports}} api throws an exception:

{code}
INFO cluster.YarnClusterSchedulerBackend: Node Report API is not available in the version of YARN being used, so AM logs link will not appear in application UI
java.io.IOException: Failed on local exception: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]; Host Details : local host is: "hsp-4.vpc.cloudera.com/172.28.195.51"; destination host is: "hsp-1.vpc.cloudera.com":8032; 
	at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:772)
	at org.apache.hadoop.ipc.Client.call(Client.java:1472)
	at org.apache.hadoop.ipc.Client.call(Client.java:1399)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:232)
	at com.sun.proxy.$Proxy22.getClusterNodes(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.ApplicationClientProtocolPBClientImpl.getClusterNodes(ApplicationClientProtocolPBClientImpl.java:262)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:187)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:102)
	at com.sun.proxy.$Proxy23.getClusterNodes(Unknown Source)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.getNodeReports(YarnClientImpl.java:475)
	at org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend$$anonfun$getDriverLogUrls$1.apply(YarnClusterSchedulerBackend.scala:92)
	at org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend$$anonfun$getDriverLogUrls$1.apply(YarnClusterSchedulerBackend.scala:73)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.cluster.YarnClusterSchedulerBackend.getDriverLogUrls(YarnClusterSchedulerBackend.scala:73)
	at org.apache.spark.SparkContext.postApplicationStart(SparkContext.scala:2015)
	at org.apache.spark.SparkContext.\<init\>(SparkContext.scala:553)
	at org.apache.spark.streaming.StreamingContext$.createNewSparkContext(StreamingContext.scala:842)
	at org.apache.spark.streaming.StreamingContext.\<init\>(StreamingContext.scala:80)
	at org.apache.spark.testing.testing.HdfsWordCount$.main(HdfsWordCount.scala:41)
	at org.apache.spark.testing.testing.HdfsWordCount.main(HdfsWordCount.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$2.run(ApplicationMaster.scala:504)
Caused by: java.io.IOException: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
	at org.apache.hadoop.ipc.Client$Connection$1.run(Client.java:680)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Client$Connection.handleSaslConnectionFailure(Client.java:643)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:730)
	at org.apache.hadoop.ipc.Client$Connection.access$2800(Client.java:368)
	at org.apache.hadoop.ipc.Client.getConnection(Client.java:1521)
	at org.apache.hadoop.ipc.Client.call(Client.java:1438)
	... 27 more
Caused by: org.apache.hadoop.security.AccessControlException: Client cannot authenticate via:[TOKEN, KERBEROS]
	at org.apache.hadoop.security.SaslRpcClient.selectSaslClient(SaslRpcClient.java:172)
	at org.apache.hadoop.security.SaslRpcClient.saslConnect(SaslRpcClient.java:396)
	at org.apache.hadoop.ipc.Client$Connection.setupSaslConnection(Client.java:553)
	at org.apache.hadoop.ipc.Client$Connection.access$1800(Client.java:368)
	at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:722)
	at org.apache.hadoop.ipc.Client$Connection$2.run(Client.java:718)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1671)
	at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:717)
	... 30 more
{code}

So the link to the driver is not available in secure mode.


---

* [SPARK-8979](https://issues.apache.org/jira/browse/SPARK-8979) | *Major* | **Implement a PIDRateEstimator**

Based on this [design doc\|https://docs.google.com/document/d/1ls\_g5fFmfbbSTIfQQpUxH56d0f3OksF567zwA00zK9E/edit?usp=sharing]


---

* [SPARK-8978](https://issues.apache.org/jira/browse/SPARK-8978) | *Major* | **Implement the DirectKafkaRateController**

Based on this [design doc\|https://docs.google.com/document/d/1ls\_g5fFmfbbSTIfQQpUxH56d0f3OksF567zwA00zK9E/edit?usp=sharing].

The DirectKafkaInputDStream should use the rate estimate to control how many records/partition to put in the next batch.


---

* [SPARK-8977](https://issues.apache.org/jira/browse/SPARK-8977) | *Major* | **Define the RateEstimator interface, and implement the ReceiverRateController**

Full [design doc\|https://docs.google.com/document/d/1ls\_g5fFmfbbSTIfQQpUxH56d0f3OksF567zwA00zK9E/edit?usp=sharing]

Implement a rate controller for receiver-based InputDStreams that estimates a maximum rate and sends it to each receiver supervisor.


---

* [SPARK-8976](https://issues.apache.org/jira/browse/SPARK-8976) | *Major* | **Python 3 crash: ValueError: invalid mode 'a+' (only r, w, b allowed)**

See Github report: https://github.com/apache/spark/pull/5173#issuecomment-113410652


---

* [SPARK-8975](https://issues.apache.org/jira/browse/SPARK-8975) | *Major* | **Implement a mechanism to send a new rate from the driver to the block generator**

Full design doc [here\|https://docs.google.com/document/d/1ls\_g5fFmfbbSTIfQQpUxH56d0f3OksF567zwA00zK9E/edit?usp=sharing]

- Add a new message, {{RateUpdate(newRate: Long)}} that ReceiverSupervisor handles in its endpoint 
- Add a new method to ReceiverTracker
{{def sendRateUpdate(streamId: Int, newRate: Long): Unit}}
this method sends an asynchronous RateUpdate message to the receiver supervisor corresponding to streamId 
- update the rate in the corresponding block generator.


---

* [SPARK-8974](https://issues.apache.org/jira/browse/SPARK-8974) | *Minor* | **There is a bug in dynamicAllocation. When there is no running tasks, the number of executor a long time without running tasks, the number of executor does not reduce to the value of "spark.dynamicAllocation.minExecutors".**

In yarn-client mode and config option "spark.dynamicAllocation.enabled " is true, when the state of ApplicationMaster is dead or disconnected, if the tasks are submitted  before new ApplicationMaster start. The thread of spark-dynamic-executor-allocation will throw exception, When ApplicationMaster is running and not tasks are running, the number of executor is not zero. So feture of dynamicAllocation are not  supported.


---

* [SPARK-8972](https://issues.apache.org/jira/browse/SPARK-8972) | *Critical* | **Incorrect result for rollup**

{code:java}
import sqlContext.implicits.\_
case class KeyValue(key: Int, value: String)
val df = sc.parallelize(1 to 5).map(i=\>KeyValue(i, i.toString)).toDF
df.registerTempTable("foo")
sqlContext.sql("select count(\*) as cnt, key % 100,GROUPING\_\_ID from foo group by key%100 with rollup").show(100)
// output
+---+---+------------+
\|cnt\|\_c1\|GROUPING\_\_ID\|
+---+---+------------+
\|  1\|  4\|           0\|
\|  1\|  4\|           1\|
\|  1\|  5\|           0\|
\|  1\|  5\|           1\|
\|  1\|  1\|           0\|
\|  1\|  1\|           1\|
\|  1\|  2\|           0\|
\|  1\|  2\|           1\|
\|  1\|  3\|           0\|
\|  1\|  3\|           1\|
+---+---+------------+
{code}
After checking with the code, seems we does't support the complex expressions (not just simple column names) for GROUP BY keys for rollup, as well as the cube. And it even will not report it if we have complex expression in the rollup keys, hence we get very confusing result as the example above.


---

* [SPARK-8967](https://issues.apache.org/jira/browse/SPARK-8967) | *Major* | **Implement @since as an annotation**

We use @since tag in JavaDoc. There exists one issue. For a overloaded method, it inherits the doc from its parent if no JavaDoc is provided. However, if we want to add @since, we have to add JavaDoc. Then we need to copy the JavaDoc from parent, which makes it hard to keep docs in sync.

A better solution would be implementing @since as an annotation, which is not part of the JavaDoc.


---

* [SPARK-8965](https://issues.apache.org/jira/browse/SPARK-8965) | *Minor* | **Add ml-guide Python Example: Estimator, Transformer, and Param**

Look at: [http://spark.apache.org/docs/latest/ml-guide.html#example-estimator-transformer-and-param]

We need a Python example doing exactly the same thing, but in Python.  It should be tested using the PySpark shell.


---

* [SPARK-8963](https://issues.apache.org/jira/browse/SPARK-8963) | *Trivial* | **Improve Linear Regression tests to use Vectors**

Improve LinearRegressionSuite to use Vector comparison.


---

* [SPARK-8962](https://issues.apache.org/jira/browse/SPARK-8962) | *Major* | **Disallow Class.forName**

We should add a regex rule to Scalastyle which prohibits the use of {{Class.forName}}.  We should not use Class.forName directly because this will load classes from the system's default class loader rather than the appropriate context loader.  Instead, we should be calling Utils.classForName instead.


---

* [SPARK-8961](https://issues.apache.org/jira/browse/SPARK-8961) | *Major* | **Makes BaseWriterContainer.outputWriterForRow accepts InternalRow instead of Row**

This is a follow-up of SPARK-8888, which also aims to optimize writing dynamic partitions.

Three more changes can be made here:

# Using {{InternalRow}} instead of {{Row}} in {{BaseWriterContainer.outputWriterForRow}}
# Using {{Cast}} expressions to convert partition columns to strings, so that we can leverage code generation.
# Replacing the FP-style {{zip}} and {{map}} calls with a faster imperative {{while}} loop.


---

* [SPARK-8959](https://issues.apache.org/jira/browse/SPARK-8959) | *Blocker* | **Parquet-thrift and libthrift introduced as test dependencies in PR #7231 break Maven builds**

These two dependencies somehow crash the Scala compiler.


---

* [SPARK-8958](https://issues.apache.org/jira/browse/SPARK-8958) | *Major* | **Dynamic allocation: change cached executor timeout to infinity**

It would be good to keep it more conservative. Losing cached blocks may be very expensive and we should only allow it if the user knows what he/she is doing.


---

* [SPARK-8954](https://issues.apache.org/jira/browse/SPARK-8954) | *Major* | **Building Docker Images Fails in 1.4 branch**

Docker build on branch 1.4 fails when installing the jdk. It expects tzdata-java as a dependency but adding that to the apt-get install list doesn't help.

~/S/s/d/spark-test git:branch-1.4  docker build -t spark-test-base base/                                                 
Sending build context to Docker daemon 3.072 kB
Sending build context to Docker daemon
Step 0 : FROM ubuntu:precise
 ---\> 78cef618c77e
Step 1 : RUN echo "deb http://archive.ubuntu.com/ubuntu precise main universe" \> /etc/apt/sources.list
 ---\> Using cache
 ---\> 2017472bec85
Step 2 : RUN apt-get update
 ---\> Using cache
 ---\> 86b8911ead16
Step 3 : RUN apt-get install -y less openjdk-7-jre-headless net-tools vim-tiny sudo openssh-server
 ---\> Running in dc8197a0ea31
Reading package lists...
Building dependency tree...
Reading state information...
Some packages could not be installed. This may mean that you have
requested an impossible situation or if you are using the unstable
distribution that some required packages have not yet been created
or been moved out of Incoming.
The following information may help to resolve the situation:

The following packages have unmet dependencies:
 openjdk-7-jre-headless : Depends: tzdata-java but it is not going to be installed
E: Unable to correct problems, you have held broken packages.
INFO[0004] The command [/bin/sh -c apt-get install -y less openjdk-7-jre-headless net-tools vim-tiny sudo openssh-server] returned a non-zero code: 100


---

* [SPARK-8952](https://issues.apache.org/jira/browse/SPARK-8952) | *Major* | **JsonFile() of SQLContext display improper warning message for a S3 path**

This is an issue reported by Ben Spark \<ben\_spark\_1@yahoo.com.au\>.

{quote}
Spark 1.4 deployed on AWS EMR 

"jsonFile" is working though with some warning message
Warning message:
In normalizePath(path) :
  path[1]="s3://rea-consumer-data-dev/cbr/profiler/output/20150618/part-00000": No such file or directory
{quote}


---

* [SPARK-8950](https://issues.apache.org/jira/browse/SPARK-8950) | *Minor* | **Correct the calculation of SchedulerDelayTime in StagePage**

In StagePage, the SchedulerDelay is calculated as totalExecutionTime - executorRunTime - executorOverhead - gettingResultTime.

But the totalExecutionTime is calculated in the way that doesn't include the gettingResultTime.


---

* [SPARK-8949](https://issues.apache.org/jira/browse/SPARK-8949) | *Critical* | **Remove references to preferredNodeLocalityData in javadoc and print warning when used**

The SparkContext constructor that takes preferredNodeLocalityData has not worked since before Spark 1.0. Also, the feature in SPARK-4352 is strictly better than a correct implementation of that feature.

We should remove any documentation references to that feature and print a warning when it is used saying it doesn't work.


---

* [SPARK-8948](https://issues.apache.org/jira/browse/SPARK-8948) | *Major* | **Remove ExtractValueWithOrdinal abstract class**

It is unnecessary and makes the type hierarchy slightly more complicated than needed.


---

* [SPARK-8947](https://issues.apache.org/jira/browse/SPARK-8947) | *Major* | **Improve expression type coercion, casting & checking**

This is an umbrella ticket to improve type casting & checking.


---

* [SPARK-8943](https://issues.apache.org/jira/browse/SPARK-8943) | *Major* | **CalendarIntervalType for time intervals**

This is an umbrella ticket for adding a new CalendarIntervalType, internally stored as two components: number of months and number of microseconds. The CalendarIntervalType is not comparable.


---

* [SPARK-8940](https://issues.apache.org/jira/browse/SPARK-8940) | *Major* | **Don't overwrite given schema if it is not null for createDataFrame in SparkR**

The given schema parameter will be overwritten in createDataFrame now. If it is not null, we shouldn't overwrite it.


---

* [SPARK-8937](https://issues.apache.org/jira/browse/SPARK-8937) | *Minor* | **A setting `spark.unsafe.exceptionOnMemoryLeak ` is missing in ScalaTest config.**

`spark.unsafe.exceptionOnMemoryLeak` is present in the config of surefire.

{code}
        \<!-- Surefire runs all Java tests --\>
        \<plugin\>
          \<groupId\>org.apache.maven.plugins\</groupId\>
          \<artifactId\>maven-surefire-plugin\</artifactId\>
          \<version\>2.18.1\</version\>
          \<!-- Note config is repeated in scalatest config --\>
...
           
\<spark.unsafe.exceptionOnMemoryLeak\>true\</spark.unsafe.exceptionOnMemoryLeak\>
            \</systemProperties\>
...
{code}

 but is absent in the config ScalaTest.


---

* [SPARK-8936](https://issues.apache.org/jira/browse/SPARK-8936) | *Major* | **Hyperparameter estimation in LDA**

LDA currently supports inference of word distributions per topic ({{LDAModel.topicsMatrix}}) but no hyperparameter (e.g. document-topic Dirichlet parameter alpha, document-word Dirichlet parameter beta) point estimates are produced.

Hyperparameter estimation in Online LDA has been described by Blei (https://www.cs.princeton.edu/~blei/papers/HoffmanBleiBach2010b.pdf, equation 9) and should be supported in Spark's implementation.


---

* [SPARK-8935](https://issues.apache.org/jira/browse/SPARK-8935) | *Major* | **Implement code generation for all casts**

Cast expression only supports a subset of type casts. We should just implement all the possible casts so we don't need to fall back to non-codegen mode.


See Cast.scala

{code}
  override def genCode(ctx: CodeGenContext, ev: GeneratedExpressionCode): String = {
    // TODO: Add support for more data types.
    (child.dataType, dataType) match {

      case (BinaryType, StringType) =\>
        defineCodeGen (ctx, ev, c =\>
          s"${ctx.stringType}.fromBytes($c)")

      case (DateType, StringType) =\>
        defineCodeGen(ctx, ev, c =\>
          s"""${ctx.stringType}.fromString(
                org.apache.spark.sql.catalyst.util.DateTimeUtils.dateToString($c))""")

      case (TimestampType, StringType) =\>
        defineCodeGen(ctx, ev, c =\>
          s"""${ctx.stringType}.fromString(
                org.apache.spark.sql.catalyst.util.DateTimeUtils.timestampToString($c))""")

      case (\_, StringType) =\>
        defineCodeGen(ctx, ev, c =\> s"${ctx.stringType}.fromString(String.valueOf($c))")

      // fallback for DecimalType, this must be before other numeric types
      case (\_, dt: DecimalType) =\>
        super.genCode(ctx, ev)

      case (BooleanType, dt: NumericType) =\>
        defineCodeGen(ctx, ev, c =\> s"(${ctx.javaType(dt)})($c ? 1 : 0)")

      case (dt: DecimalType, BooleanType) =\>
        defineCodeGen(ctx, ev, c =\> s"!$c.isZero()")

      case (dt: NumericType, BooleanType) =\>
        defineCodeGen(ctx, ev, c =\> s"$c != 0")

      case (\_: DecimalType, dt: NumericType) =\>
        defineCodeGen(ctx, ev, c =\> s"($c).to${ctx.primitiveTypeName(dt)}()")

      case (\_: NumericType, dt: NumericType) =\>
        defineCodeGen(ctx, ev, c =\> s"(${ctx.javaType(dt)})($c)")

      case other =\>
        super.genCode(ctx, ev)
    }
  }
{code}


---

* [SPARK-8934](https://issues.apache.org/jira/browse/SPARK-8934) | *Major* | **cast from double/float to timestamp should not go through decimal**

We shouldn't convert the floating point number to decimal and then immediately converting it into a timestamp.

{code}
    case DoubleType =\>
      buildCast[Double](\_, d =\> try {
        decimalToTimestamp(Decimal(d))
      } catch {
        case \_: NumberFormatException =\> null
      })
    // TimestampWritable.floatToTimestamp
    case FloatType =\>
      buildCast[Float](\_, f =\> try {
        decimalToTimestamp(Decimal(f))
      } catch {
        case \_: NumberFormatException =\> null
      })
{code}


---

* [SPARK-8933](https://issues.apache.org/jira/browse/SPARK-8933) | *Major* | **Provide a --force flag to build/mvn that always uses downloaded maven**

I noticed the other day that build/mvn will still use the system maven if "mvn" binary is installed. I think this was intentional to support just using zinc and using the system maven (and to match the semantics of sbt/sbt). It would be nice to have a flag that will force it to use the downloaded maven. I was thinking it could have a --force flag, and then it could swallow that flag and not pass it onto maven.

This is useful in some cases like our test runners, where we want to coerce a specific version of maven is used.


---

* [SPARK-8932](https://issues.apache.org/jira/browse/SPARK-8932) | *Major* | **Support copy in UnsafeRow as long as ObjectPool is not used**

We call {{InternalRow.copy()}} in many places throughout SQL but UnsafeRow currently throws UnsupportedOperationException when copy() is called.

Supporting copying when ObjectPool is used may be difficult, since we may need to handle deep-copying of objects in the pool.  In addition, this copy() method needs to produce a self-contained row object which may be passed around / buffered by downstream code which does not understand the UnsafeRow format.

In the long run, we'll need to figure out how to handle the ObjectPool corner cases, but this may be unnecessary if other changes are made. Therefore, in order to unblock my sort patch I propose that we support copy() for the cases where UnsafeRow does not use an ObjectPool and continue to throw UnsupportedOperationException when an ObjectPool is used.


---

* [SPARK-8931](https://issues.apache.org/jira/browse/SPARK-8931) | *Critical* | **Fallback to interpret mode if failed to compile in codegen**

And we should not fallback during testing.


---

* [SPARK-8930](https://issues.apache.org/jira/browse/SPARK-8930) | *Major* | **Throw a AnalysisException with meaningful messages if DataFrame#explode takes a star in expressions**

The current implementation throws an exception with meaningless messages if DataFrame#explode contain a star '\*' (ISTM that explode cannot take a star in expressions) like codes blow;

{code}
    val df = Seq(("1", "1,2"), ("2", "4"), ("3", "7,8,9")).toDF("prefix", "csv")
      df.explode($"\*") { case Row(prefix: String, csv: String) =\>
        csv.split(",").map(v =\> Tuple1(prefix + ":" + v))
      }
{code}

{code}
[info] - explode takes UnresolvedStar \*\*\* FAILED \*\*\* (14 milliseconds)
[info]   org.apache.spark.sql.AnalysisException: cannot resolve '\_1' given input columns prefix, csv;
[info]   at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:55)
[info]   at org.apache.spark.sql.catalyst.analysis.CheckAnalysis$$anonfun$checkAnalysis$1$$anonfun$apply$2.applyOrElse(CheckAnalysis.scala:52)
{code}


---

* [SPARK-8928](https://issues.apache.org/jira/browse/SPARK-8928) | *Major* | **CatalystSchemaConverter doesn't stick to behavior of old versions of Spark SQL when dealing with LISTs**

Spark SQL 1.4.x and prior versions doesn't follow Parquet format spec when dealing with complex types (because the spec wasn't clear about this part at the time Spark SQL Parquet support was authored). SPARK-6777 partly fixes this problem with {{CatalystSchemaConverter}} and introduces a feature flag to indicate whether we should stick to Parquet format spec (standard mode) or older Spark versions behavior (compatible mode). However, when dealing with LISTs (i.e. Spark SQL arrays), {{CatalystSchemaConverter}} doesn't give exactly the same schema as before in compatible mode.

For a nullable int array, 1.4.x- sticks to parquet-avro and gives:
{noformat}
message root {
  \<repetition\> group \<field-name\> (LIST) {
    required int32 array;
  }
}
{noformat}
The inner most field name is {{array}}. However, {{CatalystSchemaConverter}} uses {{element}}.

Similarly, for a nullable int array, 1.4.x- sticks to parquet-hive, and gives:
{noformat}
optional group \<field-name\> (LIST) {
  optional group bag {
    optional int32 array\_element;
  }
}
{noformat}
The inner most field name is {{array\_element}}. However, {{CatalystSchemaConverter}} still uses {{element}}.

This issue doesn't affect Parquet read path since all the schemas above are covered by SPARK-6776. But this issue affects the write path. Ideally, we'd like Spark SQL 1.5.x to write Parquet files sticking to either exactly the same format used in 1.4.x- or the most recent Parquet format spec.


---

* [SPARK-8927](https://issues.apache.org/jira/browse/SPARK-8927) | *Trivial* | **Doc format wrong for some config descriptions**

In the docs, a couple descriptions of configuration (under Network) are not inside \<td\>\</td\> and are being displayed immediately under the section title instead of in their row.


---

* [SPARK-8914](https://issues.apache.org/jira/browse/SPARK-8914) | *Minor* | **Remove RDDApi**

As [~rxin] suggested in the following link, we should consider to remove `RDDApi`.
https://github.com/apache/spark/pull/7298#issuecomment-119747662


---

* [SPARK-8913](https://issues.apache.org/jira/browse/SPARK-8913) | *Major* | **Follow-up on SPARK-8700. Cleanup the test**

Clean up the tests to make weightR a DenseVector then use assert(model1.weights ~== weightsR relTol ...)


---

* [SPARK-8911](https://issues.apache.org/jira/browse/SPARK-8911) | *Major* | **Local mode endless heartbeat warnings**

Every now and then we get WARN Executor: Told to re-register on heartbeat. This is likely caused by recent heartbeat receiver changes.


---

* [SPARK-8910](https://issues.apache.org/jira/browse/SPARK-8910) | *Critical* | **MiMa test is flaky because it starts a SQLContext**

I've seen this many times on GitHub. At the very least we should disable the SparkUI to prevent port contention, which is one of the most common sources of flakiness.
{code}
15/07/08 12:46:08 WARN AbstractLifeCycle: FAILED SelectChannelConnector@0.0.0.0:4040: java.net.BindException: Address already in use
java.net.BindException: Address already in use
	at sun.nio.ch.Net.bind0(Native Method)
	at sun.nio.ch.Net.bind(Net.java:444)
	at sun.nio.ch.Net.bind(Net.java:436)
{code}
https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/36826/consoleFull


---

* [SPARK-8909](https://issues.apache.org/jira/browse/SPARK-8909) | *Trivial* | **Nice to have all the examples in scala, java,python, R to be same in sql-programming-guide**

in the sql-programming-guide. in the section manually specifying the options. Our scala example is not in sync with the  java, python, R examples.

So to be consistent it would be nice to have format as parquest for scala code section


---

* [SPARK-8908](https://issues.apache.org/jira/browse/SPARK-8908) | *Minor* | **Calling distinct() with parentheses throws error in Scala DataFrame**

To reproduce, please call {{distinct()}} on DataFrame in spark-shell. For eg,
{code}
scala\> sqlContext.table("my\_table").distinct()

\<console\>:19: error: not enough arguments for method apply: (colName: String)org.apache.spark.sql.Column in class DataFrame.
Unspecified value parameter colName.
{code}
This is confusing because {{distinct}} in DataFrame is an alias of {{dropDuplicates}}, and both {{dropDuplicates}} and {{dropDuplicates()}} work.

Here is the summary-
\|\|Scala code\|\|Works\|\|
\|DF.distinct\|Y\|
\|DF.distinct()\|N\|
\|DF.dropDuplicates\|Y\|
\|DF.dropDuplicates()\|Y\|

Looking at the definition of {{distinct}}, it's missing {{()}}-
{code}
override def distinct: DataFrame = dropDuplicates()
{code}
As a result, what seems happening is as follows-
{code}
distinct()
=\> dropDuplicates()()
=\> DataFrame() // because dropDuplicates() returns DF
=\> DataFrame.apply() // fails because apply() takes a column parameter
{code}
I can verify that adding {{()}} to the definition makes both {{distinct}} and {{distinct()}} work.


---

* [SPARK-8907](https://issues.apache.org/jira/browse/SPARK-8907) | *Major* | **Speed up path construction in DynamicPartitionWriterContainer.outputWriterForRow**

Don't use zip and scala collection methods to avoid garbage collection

{code}
    val partitionPath = partitionColumns.zip(row.toSeq).map { case (col, rawValue) =\>
      val string = if (rawValue == null) null else String.valueOf(rawValue)
      val valueString = if (string == null \|\| string.isEmpty) {
        defaultPartitionName
      } else {
        PartitioningUtils.escapePathName(string)
      }
      s"/$col=$valueString"
    }.mkString.stripPrefix(Path.SEPARATOR)
{code}

We can probably use catalyst expressions themselves to construct the path, and then we can leverage code generation to do this.


---

* [SPARK-8906](https://issues.apache.org/jira/browse/SPARK-8906) | *Major* | **Move all internal data source related classes out of sources package**

Move all of them into execution package for better private visibility.


---

* [SPARK-8902](https://issues.apache.org/jira/browse/SPARK-8902) | *Trivial* | **Hostname missing in spark-ec2 error message**

When SSH to a machine fails, the following message is printed:

{noformat}
Failed to SSH to remote host {0}.
Please check that you have provided the correct --identity-file and --key-pair parameters and try again.
{noformat}

The intention is to print the host name, but instead {0} is printed.

I have a pull request for this: https://github.com/apache/spark/pull/7288


---

* [SPARK-8900](https://issues.apache.org/jira/browse/SPARK-8900) | *Major* | **sparkPackages flag name is wrong in the documentation**

The SparkR documentation example in http://people.apache.org/~pwendell/spark-releases/latest/sparkr.html is incorrect.

    sc \<- sparkR.init(packages="com.databricks:spark-csv\_2.11:1.0.3")
should be
    sc \<- sparkR.init(sparkPackages="com.databricks:spark-csv\_2.11:1.0.3")


---

* [SPARK-8894](https://issues.apache.org/jira/browse/SPARK-8894) | *Major* | **Example code errors in SparkR documentation**

There are errors in SparkR related documentation.
1. in https://spark.apache.org/docs/latest/sql-programming-guide.html#hive-tables, for R part, 
{code}
results = sqlContext.sql("FROM src SELECT key, value").collect()
{code}
should be
{code}
results \<- collect(sql(sqlContext, "FROM src SELECT key, value"))
{code}

2. in https://spark.apache.org/docs/latest/sparkr.html#from-hive-tables, 
{code}
results \<- hiveContext.sql("FROM src SELECT key, value")
{code}
should be
{code}
results \<- sql(hiveContext, "FROM src SELECT key, value")
{code}


---

* [SPARK-8893](https://issues.apache.org/jira/browse/SPARK-8893) | *Trivial* | **Require positive partition counts in RDD.repartition**

What does {{sc.parallelize(1 to 3).repartition(p).collect}} return? I would expect {{Array(1, 2, 3)}} regardless of {{p}}. But if {{p}} \< 1, it returns {{Array()}}. I think instead it should throw an {{IllegalArgumentException}}.

I think the case is pretty clear for {{p}} \< 0. But the behavior for {{p}} = 0 is also error prone. In fact that's how I found this strange behavior. I used {{rdd.repartition(a/b)}} with positive {{a}} and {{b}}, but {{a/b}} was rounded down to zero and the results surprised me. I'd prefer an exception instead of unexpected (corrupt) results.

I'm happy to send a pull request for this.


---

* [SPARK-8891](https://issues.apache.org/jira/browse/SPARK-8891) | *Blocker* | **Calling aggregation expressions on null literals fails at runtime**

Queries that call aggregate expressions with null literals, such as {{select avg(null)}} or {{select sum(null)}} fail with various errors due to mishandling of the internal NullType type.

For instance, with codegen disabled on a recent 1.5 master:

{code}
scala.MatchError: NullType (of class org.apache.spark.sql.types.NullType$)
	at org.apache.spark.sql.catalyst.expressions.Cast.org$apache$spark$sql$catalyst$expressions$Cast$$cast(Cast.scala:407)
	at org.apache.spark.sql.catalyst.expressions.Cast.cast$lzycompute(Cast.scala:426)
	at org.apache.spark.sql.catalyst.expressions.Cast.cast(Cast.scala:426)
	at org.apache.spark.sql.catalyst.expressions.Cast.nullSafeEval(Cast.scala:428)
	at org.apache.spark.sql.catalyst.expressions.UnaryExpression.eval(Expression.scala:196)
	at org.apache.spark.sql.catalyst.expressions.Coalesce.eval(nullFunctions.scala:48)
	at org.apache.spark.sql.catalyst.expressions.BinaryExpression.eval(Expression.scala:268)
	at org.apache.spark.sql.catalyst.expressions.Coalesce.eval(nullFunctions.scala:48)
	at org.apache.spark.sql.catalyst.expressions.MutableLiteral.update(literals.scala:147)
	at org.apache.spark.sql.catalyst.expressions.SumFunction.update(aggregates.scala:536)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$6.apply(Aggregate.scala:132)
	at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$6.apply(Aggregate.scala:125)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}

When codegen is enabled, the resulting code fails to compile.

The fix for this issue involves changes to Cast and Sum.


---

* [SPARK-8890](https://issues.apache.org/jira/browse/SPARK-8890) | *Critical* | **Reduce memory consumption for dynamic partition insert**

Currently, InsertIntoHadoopFsRelation can run out of memory if the number of table partitions is large. The problem is that we open one output writer for each partition, and when data are randomized and when the number of partitions is large, we open a large number of output writers, leading to OOM.

The solution here is to inject a sorting operation once the number of active partitions is beyond a certain point (e.g. 50?)


---

* [SPARK-8888](https://issues.apache.org/jira/browse/SPARK-8888) | *Major* | **Replace the hash map in DynamicPartitionWriterContainer.outputWriterForRow with java.util.HashMap**

We should use a java.util.HashMap, which is faster than Scala's map, although this one barely matters.


---

* [SPARK-8886](https://issues.apache.org/jira/browse/SPARK-8886) | *Trivial* | **Python style usually don't add space before/after the = in named parameters**

python style usually don't add space before/after the = in named parameters. can you submit a follow up patch to fix that?
thanks.


---

* [SPARK-8883](https://issues.apache.org/jira/browse/SPARK-8883) | *Minor* | **Remove the class OverrideFunctionRegistry**

The class `OverrideFunctionRegistry` is redundant since the `HiveFunctionRegistry` has its own way to the underlying registry.


---

* [SPARK-8882](https://issues.apache.org/jira/browse/SPARK-8882) | *Major* | **A New Receiver Scheduling Mechanism**

The design doc: https://docs.google.com/document/d/1ZsoRvHjpISPrDmSjsGzuSu8UjwgbtmoCTzmhgTurHJw/edit?usp=sharing


---

* [SPARK-8881](https://issues.apache.org/jira/browse/SPARK-8881) | *Critical* | **Standalone mode scheduling fails because cores assignment is not atomic**

Current scheduling algorithm (in Master.scala) has two issues:

1. cores are allocated one at a time instead of spark.executor.cores at a time
2. when spark.cores.max/spark.executor.cores \< num\_workers, executors are not launched and the app hangs (due to 1)

=== Edit by Andrew ===

Here's an example from the PR. Let's say we have 4 workers with 16 cores each. We set `spark.cores.max` to 48 and `spark.executor.cores` to 16. Because in spread out mode, the existing code allocates 1 core at a time, we end up allocating 12 cores on each worker, and no executors can be launched because each one wants at least 16 cores. Instead, we should allocate 16 cores at a time.


---

* [SPARK-8880](https://issues.apache.org/jira/browse/SPARK-8880) | *Minor* | **Fix confusing Stage.attemptId member variable**

This variable very confusingly refers to the \*next\* stageId that should be used, making this code especially hard to understand.


---

* [SPARK-8879](https://issues.apache.org/jira/browse/SPARK-8879) | *Major* | **Remove EmptyRow class**

Right now InternalRow is megamorphic because it has many different implementations. We should work towards having only one or at most two InternalRow implementations.


---

* [SPARK-8877](https://issues.apache.org/jira/browse/SPARK-8877) | *Minor* | **Public API for association rule generation**

Association rule generation was added by SPARK-8559. However, the class is currently private and inaccessible to end users. We should provide a public API for generating association rules directly from the model produced by frequent itemset algorithms (eg {{FPGrowth}})


---

* [SPARK-8876](https://issues.apache.org/jira/browse/SPARK-8876) | *Major* | **Remove InternalRow type alias in expressions package**

The type alias was there because initially when I moved Row around, I didn't want to do massive changes to the expression code. But now it should be pretty easy to just remove it. One less concept to worry about.


---

* [SPARK-8875](https://issues.apache.org/jira/browse/SPARK-8875) | *Minor* | **Shuffle code cleanup: remove BlockStoreShuffleFetcher class**

The shuffle code has gotten increasingly difficult to read as it has evolved, and many classes have evolved significantly since they were originally created. The BlockStoreShuffleFetcher class now serves little purpose other than to make the code more difficult to read; we should move its functionality into the ShuffleBlockFetcherIterator class.


---

* [SPARK-8874](https://issues.apache.org/jira/browse/SPARK-8874) | *Major* | **Add missing methods in Word2Vec ML**

Add getVectors and findSynonyms.


---

* [SPARK-8873](https://issues.apache.org/jira/browse/SPARK-8873) | *Blocker* | **Support cleaning up shuffle files when using shuffle service in Mesos**

With dynamic allocation enabled with Mesos, drivers can launch with shuffle data cached in the external shuffle service.
However, there is no reliable way to let the shuffle service clean up the shuffle data when the driver exits, since it may crash before it notifies the shuffle service and shuffle data will be cached forever.
We need to implement a reliable way to detect driver termination and clean up shuffle data accordingly.


---

* [SPARK-8872](https://issues.apache.org/jira/browse/SPARK-8872) | *Minor* | **Improve FPGrowthSuite with equivalent R code**

In `FPGrowthSuite`, we only tested output with minSupport 0.5, where the expected output is hard-coded. We can add equivalent R code using the arules package to generate the expect output for validation purpose, similar to https://github.com/apache/spark/blob/master/mllib/src/test/scala/org/apache/spark/ml/regression/LinearRegressionSuite.scala#L98 and the test code in https://github.com/apache/spark/pull/7005.


---

* [SPARK-8868](https://issues.apache.org/jira/browse/SPARK-8868) | *Minor* | **SqlSerializer2 can go into infinite loop when row consists only of NullType columns**

The following SQL query will cause an infinite loop in SqlSerializer2 code:

{code}
val df = sqlContext.sql("select null where 1 = 1")
df.unionAll(df).sort("\_c0").collect()
{code}

The same problem occurs if we add more null-literals, but does not occur as long as there is a column of any other type (e.g. {{select 1, null where 1 == 1}}).

I think that what's happening here is that if you have a row that consists only of columns of NullType (not columns of other types which happen to only contain null values, but only columns of null literals), SqlSerializer will end up writing / reading no data for rows of this type.  Since the deserialization stream will never try to read any data but nevertheless will be able to return an empty row, DeserializationStream.asIterator will go into an infinite loop since there will never be a read to trigger an EOF exception.


---

* [SPARK-8867](https://issues.apache.org/jira/browse/SPARK-8867) | *Major* | **Show the UDF usage for user.**

As Hive does, we need to provide the feature for user, to show the usage of a UDF.


---

* [SPARK-8866](https://issues.apache.org/jira/browse/SPARK-8866) | *Major* | **Use 1 microsecond (us) precision for TimestampType**

100ns is slightly weird to compute. Let's use 1us to be more consistent with other systems (e.g. Postgres) and less error prone.


---

* [SPARK-8865](https://issues.apache.org/jira/browse/SPARK-8865) | *Minor* | **Fix bug:  init SimpleConsumerConfig with kafka params**

"zookeeper.connect" and "group.id" aren't necessary for anything in the kafka direct stream.

But they're expected to be present in a kafka consumer config, and overriding that behavior wasn't possible. So as a workaround, we set them to a blank string. That way users don't have to define unnecessary settings in the kafka param map passed to the KafkaUtils constructor. We talked through that during the original development of the direct stream.

The code as it is released today is almost always going to set a blank string, regardless of what users pass in, because contains on a java property object is not the equivalent of containsKey, it is containsValue. The intention was that if the user sets those properties (whatever personal reasons they have), the values should not get overwritten with a blank string.


---

* [SPARK-8864](https://issues.apache.org/jira/browse/SPARK-8864) | *Major* | **Date/time function and data type design**

Please see the attached design doc.


---

* [SPARK-8863](https://issues.apache.org/jira/browse/SPARK-8863) | *Minor* | **'spark\_ec2.py' doesn't check '~/.aws/credentials' even if boto can support '~/.aws/credentials'**

'spark\_ec2.py' use boto to control ec2.
And boto can support '~/.aws/credentials' which is AWS CLI default configuration file.

We can check this information from ref of boto.

"A boto config file is a text file formatted like an .ini configuration file that specifies values for options that control the behavior of the boto library.
In Unix/Linux systems, on startup, the boto library looks for configuration files in the following locations and in the following order:
/etc/boto.cfg - for site-wide settings that all users on this machine will use
(if profile is given) ~/.aws/credentials - for credentials shared between SDKs
(if profile is given) ~/.boto - for user-specific settings
~/.aws/credentials - for credentials shared between SDKs
~/.boto - for user-specific settings"

\* ref of boto: http://boto.readthedocs.org/en/latest/boto\_config\_tut.html
\* ref of aws cli : http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html

However 'spark\_ec2.py' only check boto config & environment variable even if there is '~/.aws/credentials', and 'spark\_ec2.py' is terminated.

So I changed to check '~/.aws/credentials'.


---

* [SPARK-8862](https://issues.apache.org/jira/browse/SPARK-8862) | *Major* | **Add a web UI page that visualizes physical plans (SparkPlan)**

We currently have the ability to visualize part of the query plan using the Spark DAG viz. However, that does NOT work for one of the most important operators: broadcast join. The reason is that broadcast join launches multiple Spark jobs.


---

* [SPARK-8861](https://issues.apache.org/jira/browse/SPARK-8861) | *Major* | **Add basic instrumentation to each SparkPlan operator**

The basic metric can be the number of tuples that is flowing through. We can add more metrics later.

In order for this to work, we can add a new "accumulators" method to SparkPlan that defines the list of accumulators, .e.g.
{code}
  def accumulators: Map[String, Accumulator]
{code}


---

* [SPARK-8859](https://issues.apache.org/jira/browse/SPARK-8859) | *Major* | **Report accumulator updates via heartbeats (internal accumulator only)**

Let's add a flag to Accumulator to allow reporting their values back via heartbeats.

Initially, this flag should be reserved for internal usage, and not user accumulators.


---

* [SPARK-8856](https://issues.apache.org/jira/browse/SPARK-8856) | *Major* | **Better instrumentation and visualization for physical plan (Spark 1.5)**

This is an umbrella ticket to improve physical plan instrumentation and visualization.


---

* [SPARK-8852](https://issues.apache.org/jira/browse/SPARK-8852) | *Major* | **spark-streaming-flume-assembly packages too many dependencies**

This assembly was recently added to make it easier to start pyspark jobs that use the flume backend; but it packages way too many dependencies, several of which are already packaged in the main Spark assembly, making the final file unnecessarily large.

We should clean up the dependency tree for that module and only package what's really needed.


---

* [SPARK-8851](https://issues.apache.org/jira/browse/SPARK-8851) | *Major* | **in Yarn client mode, Client.scala does not login even when credentials are specified**

[#6051\|https://github.com/apache/spark/pull/6051] added support for passing the credentials configuration from SparkConf, so the client mode works fine. This though created an issue where the Client.scala class does not login to the KDC, thus requiring a kinit before running in Client mode.


---

* [SPARK-8850](https://issues.apache.org/jira/browse/SPARK-8850) | *Major* | **Turn unsafe mode on by default**

Let's turn unsafe on and see what bugs we find in preparation for 1.5.


---

* [SPARK-8845](https://issues.apache.org/jira/browse/SPARK-8845) | *Minor* | **ML use of Breeze optimization: use adjustedValue not value?**

In LinearRegression and LogisticRegression, we use Breeze's optimizers (LBFGS and OWLQN).  We check the {{State.value}} to see the current objective.  However, Breeze's documentation makes it sound like {{value}} and {{adjustedValue}} differ for some optimizers, possibly including OWLQN: [https://github.com/scalanlp/breeze/blob/26faf622862e8d7a42a401aef601347aac655f2b/math/src/main/scala/breeze/optimize/FirstOrderMinimizer.scala#L36]

If that is the case, then we should use adjustedValue instead of value.  This is relevant to [SPARK-8538] and [SPARK-8539], where we will provide the objective trace to the user.

CC: [~dbtsai] Could you please take a look?  Thanks!


---

* [SPARK-8844](https://issues.apache.org/jira/browse/SPARK-8844) | *Blocker* | **head/collect is broken in SparkR**

{code}
\> t = tables(sqlContext)
\> showDF(T)
Error in (function (classes, fdef, mtable)  :
  unable to find an inherited method for function showDF for signature "logical"
\> showDF(t)
+---------+-----------+
\|tableName\|isTemporary\|
+---------+-----------+
+---------+-----------+
\> 15/07/06 09:59:10 WARN Executor: Told to re-register on heartbeat

\>
\>
\> head(t)
Error in readTypedObject(con, type) :
  Unsupported type for deserialization

\> collect(t)
Error in readTypedObject(con, type) :
  Unsupported type for deserialization
{code}


---

* [SPARK-8841](https://issues.apache.org/jira/browse/SPARK-8841) | *Trivial* | **Fix partition pruning percentage log message**

The log message printed by DataSourceStrategy when pruning partitions calculates the percentage pruned incorrectly.


---

* [SPARK-8840](https://issues.apache.org/jira/browse/SPARK-8840) | *Major* | **Float type coercion with hiveContext**

Problem with +float+ type coercion on SparkR with hiveContext.

{code}
\> result \<- sql(hiveContext, "SELECT offset, percentage from data limit 100")
\> show(result)
DataFrame[offset:float, percentage:float]
\> head(result)
Error in as.data.frame.default(x[[i]], optional = TRUE) :
    cannot coerce class ""jobj"" to a data.frame
{code}

This trouble looks like already exists (SPARK-2863 - Emulate Hive type
coercion in native reimplementations of Hive functions) with same
reason - not completed "native reimplementations of Hive..." not
"...functions" only.

I used spark 1.4.0 binaries from official site:
http://spark.apache.org/downloads.html
And running it on:
\* Hortonworks HDP 2.2.0.0-2041
\* with Hive 0.14
\* with disabled hooks for Application Timeline Servers (ATSHook) in hive-site.xml, commented:
\*\* hive.exec.failure.hooks,
\*\* hive.exec.post.hooks,
\*\* hive.exec.pre.hooks.


---

* [SPARK-8839](https://issues.apache.org/jira/browse/SPARK-8839) | *Major* | **Thrift Sever will throw `java.util.NoSuchElementException: key not found` exception when  many clients connect it**

If there are about 150+ JDBC clients connectting to the Thrift Server,  some clients will throw such exception:
{code:title=Exception message\|borderStyle=solid}
java.sql.SQLException: java.util.NoSuchElementException: key not found: 90d93e56-7f6d-45bf-b340-e3ee09dd60fc
 at org.apache.hive.jdbc.Utils.verifySuccess(Utils.java:155)
{code}


---

* [SPARK-8838](https://issues.apache.org/jira/browse/SPARK-8838) | *Major* | **Add config to enable/disable merging part-files when merging parquet schema**

Currently all part-files are merged when merging parquet schema. However, in case there are many part-files and we can make sure that all the part-files have the same schema as their summary file. If so, we provide a configuration to disable merging part-files when merging parquet schema.


---

* [SPARK-8831](https://issues.apache.org/jira/browse/SPARK-8831) | *Major* | **Support AbstractDataType in TypeCollection**

Otherwise it is impossible to declare an expression supporting DecimalType.


---

* [SPARK-8830](https://issues.apache.org/jira/browse/SPARK-8830) | *Major* | **levenshtein directly on top of UTF8String**

We currently rely on commons-lang's levenshtein implementation. Ideally, we should have our own implementation to:

1. Reduce external dependency
2. Work directly against UTF8String so we don't need to convert to/from java.lang.String back and forth.


---

* [SPARK-8828](https://issues.apache.org/jira/browse/SPARK-8828) | *Critical* | **Revert the change of SPARK-5680**

SPARK-5680 introduced a bug to sum function. After this change, when all input values are nulls, it returns 0.0 instead of null, which is wrong.


---

* [SPARK-8823](https://issues.apache.org/jira/browse/SPARK-8823) | *Minor* | **Optimizations for sparse vector products in pyspark.mllib.linalg**

Currently we iterate over indices and values of both the sparse vectors that can be vectorized in NumPy.


---

* [SPARK-8822](https://issues.apache.org/jira/browse/SPARK-8822) | *Major* | **clean up type checking in math.scala**

There are various issues w.r.t. type checking in math.scala. Most of the custom implementation of checkInputTypes can be replaced with mixin ExpectsInputTypes.

There are also other minor issues, such as over complicating certain expressions due to the previous lack of implicit type casting among numeric types.


---

* [SPARK-8821](https://issues.apache.org/jira/browse/SPARK-8821) | *Major* | **The ec2 script doesn't run on python 3 with an utf8 env**

Otherwise the script will crash with

 - Downloading boto...
Traceback (most recent call last):
  File "ec2/spark\_ec2.py", line 148, in \<module\>
    setup\_external\_libs(external\_libs)
  File "ec2/spark\_ec2.py", line 128, in setup\_external\_libs
    if hashlib.md5(tar.read()).hexdigest() != lib["md5"]:
  File "/usr/lib/python3.4/codecs.py", line 319, in decode
    (result, consumed) = self.\_buffer\_decode(data, self.errors, final)
UnicodeDecodeError: 'utf-8' codec can't decode byte 0x8b in position 1: invalid start byte

In case of an utf8 env setting.


---

* [SPARK-8820](https://issues.apache.org/jira/browse/SPARK-8820) | *Minor* | **Add a configuration to set the checkpoint directory for convenience.**

Add a configuration named \*spark.streaming.checkpointDir\*  to set the checkpoint directory.
 It will overwrite by user if they also call \*StreamingContext#checkpoint\*.


---

* [SPARK-8819](https://issues.apache.org/jira/browse/SPARK-8819) | *Blocker* | **Spark doesn't compile with maven 3.3.x**

Simple reproduction: Install maven 3.3.3 and run "build/mvn clean package -DskipTests"

This works just fine for maven 3.2.1 but not for 3.3.x. The result is an infinite loop caused by MSHADE-148:
{code}
[INFO] Replacing /Users/andrew/Documents/dev/spark/andrew-spark/bagel/target/spark-bagel\_2.10-1.5.0-SNAPSHOT.jar with /Users/andrew/Documents/dev/spark/andrew-spark/bagel/target/spark-bagel\_2.10-1.5.0-SNAPSHOT-shaded.jar
[INFO] Dependency-reduced POM written at: /Users/andrew/Documents/dev/spark/andrew-spark/bagel/dependency-reduced-pom.xml
[INFO] Dependency-reduced POM written at: /Users/andrew/Documents/dev/spark/andrew-spark/bagel/dependency-reduced-pom.xml
...
{code}

This is ultimately caused by SPARK-7558 (master 9eb222c13991c2b4a22db485710dc2e27ccf06dd) but is recently revealed through SPARK-8781 (master 82cf3315e690f4ac15b50edea6a3d673aa5be4c0).


---

* [SPARK-8810](https://issues.apache.org/jira/browse/SPARK-8810) | *Major* | **Gaps in SQL UDF test coverage**

SQL UDFs are untested in GROUP BY, WHERE and HAVING clauses, and in combination.


---

* [SPARK-8809](https://issues.apache.org/jira/browse/SPARK-8809) | *Major* | **Remove ConvertNaNs analyzer rule**

Cast already handles "NaN" when casting from string to double/float. I don't think this rule is necessary anymore.


---

* [SPARK-8808](https://issues.apache.org/jira/browse/SPARK-8808) | *Major* | **Fix assignments in SparkR**

{noformat}
inst/tests/test\_binary\_function.R:79:12: style: Use \<-, not =, for assignment.
  mockFile = c("Spark is pretty.", "Spark is awesome.")
{noformat}


---

* [SPARK-8807](https://issues.apache.org/jira/browse/SPARK-8807) | *Major* | **Add between operator in SparkR**

Add between operator in SparkR

```
df$age between c(1, 2)
```


---

* [SPARK-8804](https://issues.apache.org/jira/browse/SPARK-8804) | *Blocker* | ** order of UTF8String is wrong if there is any non-ascii character in it**

We compare the UTF8String byte by byte, but byte in JVM is signed, it should be compared as unsigned.


---

* [SPARK-8803](https://issues.apache.org/jira/browse/SPARK-8803) | *Major* | **Crosstab element's can't contain null's and back ticks**

Having back ticks or null as elements causes problems. 

Since elements become column names, we have to drop them from the element as back ticks are special characters.

Having null throws exceptions, we could replace them with empty strings.


---

* [SPARK-8801](https://issues.apache.org/jira/browse/SPARK-8801) | *Major* | **Support TypeCollection in ExpectsInputTypes**

Some functions support more than one input types for each parameter. For example, length supports binary and string, and maybe array/struct in the future.

This ticket proposes a TypeCollection AbstractDataType that supports multiple data types.


---

* [SPARK-8797](https://issues.apache.org/jira/browse/SPARK-8797) | *Critical* | **Sorting float/double column containing NaNs can lead to "Comparison method violates its general contract!" errors**

When sorting a float or double column that contains NaN (not a number) values, TimSort may throw a ""Comparison method violates its general contract!" error.


---

* [SPARK-8794](https://issues.apache.org/jira/browse/SPARK-8794) | *Major* | **Column pruning isn't applied beneath sample**

I observe that certain transformations (e.g. sample) on DataFrame cause the underlying relation's support for column pruning to be disregarded in subsequent queries.

I encountered this issue while using an ML pipeline with a typical dataset of (label, features).   For my particular data source (which implements PrunedScan), the 'features' column is expensive to compute while the 'label' column is cheap.  The first stage of the pipeline - StringIndexer - operates only on the label and so should be quick.   Yet I found that the 'features' column would be materialized.   Upon investigation,  the issue occurs when the dataset is split into train/test with sampling.   The sampling transformation causes the pruning optimization to be lost.

See this gist for a sample program demonstrating the issue:
[https://gist.github.com/EronWright/cb5fb9af46fd810194f8]


---

* [SPARK-8792](https://issues.apache.org/jira/browse/SPARK-8792) | *Major* | **Add Python API for PCA transformer**

Add Python API for PCA transformer


---

* [SPARK-8788](https://issues.apache.org/jira/browse/SPARK-8788) | *Minor* | **Java unit test for PCA transformer**

Add Java unit test for PCA transformer


---

* [SPARK-8787](https://issues.apache.org/jira/browse/SPARK-8787) | *Trivial* | **Change the parameter  order of @deprecated in package object sql**

Parameter order of @deprecated annotation  in package object sql is wrong 
deprecated("1.3.0", "use DataFrame") .

This has to be changed to  deprecated("use DataFrame", "1.3.0")


---

* [SPARK-8785](https://issues.apache.org/jira/browse/SPARK-8785) | *Major* | **Improve Parquet schema merging**

Currently, the parquet schema merging (ParquetRelation2.readSchema) may spend much time to merge duplicate schema. We can select only non duplicate schema and merge them later.


---

* [SPARK-8783](https://issues.apache.org/jira/browse/SPARK-8783) | *Minor* | **CTAS with WITH clause does not work**

Following CTAS with WITH clause query 
{code}
CREATE TABLE with\_table1 AS
WITH T AS (
  SELECT \*
  FROM table1
)
SELECT \*
FROM T
{code}
induces following error
{code}
no such table T; line 7 pos 5
org.apache.spark.sql.AnalysisException: no such table T; line 7 pos 5
...
{code}

I think that WITH clause within CTAS is not handled properly.


---

* [SPARK-8782](https://issues.apache.org/jira/browse/SPARK-8782) | *Blocker* | **GenerateOrdering fails for NullType (i.e. ORDER BY NULL crashes)**

Queries containing ORDER BY NULL currently result in a code generation exception:

{code}
      public SpecificOrdering generate(org.apache.spark.sql.catalyst.expressions.Expression[] expr) {
        return new SpecificOrdering(expr);
      }

      class SpecificOrdering extends org.apache.spark.sql.catalyst.expressions.codegen.BaseOrdering {

        private org.apache.spark.sql.catalyst.expressions.Expression[] expressions = null;

        public SpecificOrdering(org.apache.spark.sql.catalyst.expressions.Expression[] expr) {
          expressions = expr;
        }

        @Override
        public int compare(InternalRow a, InternalRow b) {
          InternalRow i = null;  // Holds current row being evaluated.
          
          i = a;
          final Object primitive1 = null;
          i = b;
          final Object primitive3 = null;
          if (true && true) {
            // Nothing
          } else if (true) {
            return -1;
          } else if (true) {
            return 1;
          } else {
            int comp = primitive1.compare(primitive3);
            if (comp != 0) {
              return comp;
            }
          }
      
          return 0;
        }
      }
org.codehaus.commons.compiler.CompileException: Line 29, Column 43: A method named "compare" is not declared in any enclosing class nor any supertype, nor through a static import
	at org.codehaus.janino.UnitCompiler.compileError(UnitCompiler.java:10174)
{code}


---

* [SPARK-8781](https://issues.apache.org/jira/browse/SPARK-8781) | *Blocker* | **Published POMs are no longer effective POMs**

Published to maven repository POMs are no longer effective POMs. E.g. 

In https://repository.apache.org/content/repositories/snapshots/org/apache/spark/spark-core\_2.11/1.4.2-SNAPSHOT/spark-core\_2.11-1.4.2-20150702.043114-52.pom:

{noformat}
...
\<dependency\>
\<groupId\>org.apache.spark\</groupId\>
\<artifactId\>spark-launcher\_${scala.binary.version}\</artifactId\>
\<version\>${project.version}\</version\>
\</dependency\>
...
{noformat}

while it should be

{noformat}
...
\<dependency\>
\<groupId\>org.apache.spark\</groupId\>
\<artifactId\>spark-launcher\_2.11\</artifactId\>
\<version\>${project.version}\</version\>
\</dependency\>
...
{noformat}


The following commits are most likely the cause of it:
- for branch-1.3: https://github.com/apache/spark/commit/ce137b8ed3b240b7516046699ac96daa55ddc129
- for branch-1.4: https://github.com/apache/spark/commit/84da653192a2d9edb82d0dbe50f577c4dc6a0c78
- for master: https://github.com/apache/spark/commit/984ad60147c933f2d5a2040c87ae687c14eb1724

On branch-1.4 reverting the commit fixed the issue.

See SPARK-3812 for additional details


---

* [SPARK-8777](https://issues.apache.org/jira/browse/SPARK-8777) | *Major* | **Add random data generation test utilities to Spark SQL**

We should add utility functions for generating data that conforms to a given SparkSQL DataType or Schema. This would make it significantly easier to write certain types of tests.


---

* [SPARK-8776](https://issues.apache.org/jira/browse/SPARK-8776) | *Major* | **Increase the default MaxPermSize**

Since 1.4.0, Spark SQL has isolated class loaders for seperating hive dependencies on metastore and execution, which increases the memory consumption of PermGen. How about we increase the default size from 128m to 256m? Seems the change we need to make is https://github.com/apache/spark/blob/3c0156899dc1ec1f7dfe6d7c8af47fa6dc7d00bf/launcher/src/main/java/org/apache/spark/launcher/AbstractCommandBuilder.java#L139.


---

* [SPARK-8774](https://issues.apache.org/jira/browse/SPARK-8774) | *Critical* | **Add R model formula with basic support as a transformer**

To have better integration with SparkR, we can add a feature transformer to support R formula. A list of operators R supports can be find here: http://ww2.coastal.edu/kingw/statistics/R-tutorials/formulae.html

The initial version should support "~", "+", and "." on numeric columns and we can expand it in the future.

{code}
val formula = new RModelFormula()
  .setFormula("y ~ x + z")
{code}

The output should append two new columns: features and label.

Design doc is posted at https://docs.google.com/document/d/10NZNSEurN2EdWM31uFYsgayIPfCFHiuIu3pCWrUmP\_c/edit?usp=sharing, as part of SPARK-6805.


---

* [SPARK-8772](https://issues.apache.org/jira/browse/SPARK-8772) | *Major* | **Implement implicit type cast for expressions that define expected input types**

We should have a engine-wide implicit cast rule defined.


---

* [SPARK-8771](https://issues.apache.org/jira/browse/SPARK-8771) | *Trivial* | **Actor system deprecation tag uses deprecated deprecation tag**

The deprecation of the actor system adds a spurious build warning:
{quote}
@deprecated now takes two arguments; see the scaladoc.
[warn]   @deprecated("Actor system is no longer supported as of 1.4")
{quote}


---

* [SPARK-8770](https://issues.apache.org/jira/browse/SPARK-8770) | *Major* | **BinaryOperator expression**

Our current BinaryExpression abstract class is not for generic binary expressions, i.e. it requires left/right children to have the same type. However, due to its name, contributors build new binary expressions that don't have that assumption (e.g. Sha) and still extend BinaryExpression.

We should create a new BinaryOperator abstract class with this assumption, and update the analyzer to only apply type casting rule there.


---

* [SPARK-8769](https://issues.apache.org/jira/browse/SPARK-8769) | *Trivial* | **toLocalIterator should mention it results in many jobs**

toLocalIterator on RDDs should mention that it results in mutliple jobs, and that to avoid re-computing, if the input was the result of a wide-transformation, the input should be cached.


---

* [SPARK-8765](https://issues.apache.org/jira/browse/SPARK-8765) | *Critical* | **Flaky PySpark PowerIterationClustering test**

See failure: [https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/36133/console]

{code}
\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*
File "/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/mllib/clustering.py", line 291, in \_\_main\_\_.PowerIterationClusteringModel
Failed example:
    sorted(model.assignments().collect())
Expected:
    [Assignment(id=0, cluster=1), Assignment(id=1, cluster=0), ...
Got:
    [Assignment(id=0, cluster=1), Assignment(id=1, cluster=1), Assignment(id=2, cluster=1), Assignment(id=3, cluster=1), Assignment(id=4, cluster=0)]
\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*
File "/home/jenkins/workspace/SparkPullRequestBuilder/python/pyspark/mllib/clustering.py", line 299, in \_\_main\_\_.PowerIterationClusteringModel
Failed example:
    sorted(sameModel.assignments().collect())
Expected:
    [Assignment(id=0, cluster=1), Assignment(id=1, cluster=0), ...
Got:
    [Assignment(id=0, cluster=1), Assignment(id=1, cluster=1), Assignment(id=2, cluster=1), Assignment(id=3, cluster=1), Assignment(id=4, cluster=0)]
\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*
   2 of  13 in \_\_main\_\_.PowerIterationClusteringModel
\*\*\*Test Failed\*\*\* 2 failures.

Had test failures in pyspark.mllib.clustering with python2.6; see logs.
{code}

CC: [~mengxr] [~yanboliang]


---

* [SPARK-8763](https://issues.apache.org/jira/browse/SPARK-8763) | *Major* | **executing run-tests.py with Python 2.6 fails with absence of subprocess.check\_output function**

Running run-tests.py with Python 2.6 cause following error:

{noformat}
Running PySpark tests. Output is in python//Users/tomohiko/.jenkins/jobs/pyspark\_test/workspace/python/unit-tests.log
Will test against the following Python executables: ['python2.6', 'python3.4', 'pypy']
Will test the following Python modules: ['pyspark-core', 'pyspark-ml', 'pyspark-mllib', 'pyspark-sql', 'pyspark-streaming']
Traceback (most recent call last):
  File "./python/run-tests.py", line 196, in \<module\>
    main()
  File "./python/run-tests.py", line 159, in main
    python\_implementation = subprocess.check\_output(
AttributeError: 'module' object has no attribute 'check\_output'
...
{noformat}

The cause of this error is using subprocess.check\_output function, which exists since Python 2.7.
(ref. https://docs.python.org/2.7/library/subprocess.html#subprocess.check\_output)


---

* [SPARK-8758](https://issues.apache.org/jira/browse/SPARK-8758) | *Major* | **Add Python user guide for PowerIterationClustering**

Add Python user guide for PowerIterationClustering


---

* [SPARK-8756](https://issues.apache.org/jira/browse/SPARK-8756) | *Major* | **Keep cached information and avoid re-calculating footers in ParquetRelation2**

Currently, in ParquetRelation2, footers are re-read every time refresh() is called. But we can check if it is possibly changed before we do the reading because reading all footers will be expensive when there are too many partitions.


---

* [SPARK-8754](https://issues.apache.org/jira/browse/SPARK-8754) | *Minor* | **YarnClientSchedulerBackend doesn't stop gracefully in failure conditions**

{code:xml}
java.lang.NullPointerException
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.stop(YarnClientSchedulerBackend.scala:151)
        at org.apache.spark.scheduler.TaskSchedulerImpl.stop(TaskSchedulerImpl.scala:421)
        at org.apache.spark.scheduler.DAGScheduler.stop(DAGScheduler.scala:1447)
        at org.apache.spark.SparkContext.stop(SparkContext.scala:1651)
        at org.apache.spark.SparkContext.\<init\>(SparkContext.scala:572)
        at org.apache.spark.examples.SparkPi$.main(SparkPi.scala:28)
        at org.apache.spark.examples.SparkPi.main(SparkPi.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:621)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:170)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:193)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:112)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{code}

If the application has FINISHED/FAILED/KILLED or failed to launch application master, monitorThread is not getting initialized but monitorThread.interrupt() is getting invoked as part of stop() without any check and It is causing to throw NPE and also it is preventing to stop the client.


---

* [SPARK-8753](https://issues.apache.org/jira/browse/SPARK-8753) | *Major* | **Create an IntervalType data type**

We should create an IntervalType data type that represents time intervals. Internally, we can use a long value to store it, similar to Timestamp (i.e. 100ns precision). This data type initially cannot be stored externally, but only used for expressions.

1. Add IntervalType data type.

2. Add parser support in our SQL expression, in the form of

{code}
INTERVAL [number] [unit] 
{code}

unit can be YEAR[S], MONTH[S], WEEK[S], DAY[S], HOUR[S], MINUTE[S], SECOND[S], MILLISECOND[S], MICROSECOND[S], or NANOSECOND[S].

3. Add in the analyzer to make sure we throw some exception to prevent saving a dataframe/table with IntervalType out to external systems.

Related Hive ticket: https://issues.apache.org/jira/browse/HIVE-9792


---

* [SPARK-8750](https://issues.apache.org/jira/browse/SPARK-8750) | *Major* | **Remove the closure in functions.callUdf**

{code}
[warn] /Users/yhuai/Projects/Spark/yin-spark-1/sql/core/src/main/scala/org/apache/spark/sql/functions.scala:1829: Class org.apache.spark.sql.functions$$anonfun$callUDF$1 differs only in case from org.apache.spark.sql.functions$$anonfun$callUdf$1. Such classes will overwrite one another on case-insensitive filesystems.
{code}


---

* [SPARK-8749](https://issues.apache.org/jira/browse/SPARK-8749) | *Major* | **Remove HiveTypeCoercion trait**

It is easier to test rules if they are in the companion object.


---

* [SPARK-8748](https://issues.apache.org/jira/browse/SPARK-8748) | *Major* | **Move castability test out from Cast case class into Cast object**

So we can use it as static methods in the analyzer.


---

* [SPARK-8746](https://issues.apache.org/jira/browse/SPARK-8746) | *Trivial* | **Need to update download link for Hive 0.13.1 jars (HiveComparisonTest)**

The Spark SQL documentation (https://github.com/apache/spark/tree/master/sql) describes how to generate golden answer files for new hive comparison test cases. However the download link for the Hive 0.13.1 jars points to https://hive.apache.org/downloads.html but none of the linked mirror sites still has the 0.13.1 version.

We need to update the link to https://archive.apache.org/dist/hive/hive-0.13.1/


---

* [SPARK-8744](https://issues.apache.org/jira/browse/SPARK-8744) | *Trivial* | **StringIndexerModel should have public constructor**

It would be helpful to allow users to pass a pre-computed index to create an indexer, rather than always going through StringIndexer to create the model.


---

* [SPARK-8743](https://issues.apache.org/jira/browse/SPARK-8743) | *Major* | **Deregister Codahale metrics for streaming when StreamingContext is closed**

Currently, when the StreamingContext is closed, the registered metrics are not deregistered. If another streaming context is started, it throws a warning saying that the metrics are already registered. 

The solution is to deregister the metrics when streamingcontext is stopped.


---

* [SPARK-8742](https://issues.apache.org/jira/browse/SPARK-8742) | *Blocker* | **Improve SparkR error messages for DataFrame API**

Currently all DataFrame API errors result in following generic error:

{code}
Error: returnStatus == 0 is not TRUE
{code}

This is because invokeJava in backend.R does not inspect error messages. For most use cases it is critical to return better error messages. Initially, we can return the stack trace from the JVM. In future we can inspect the errors and translate them to human-readable error messages.


---

* [SPARK-8741](https://issues.apache.org/jira/browse/SPARK-8741) | *Major* | **Remove e and pi from DataFrame functions**

It is not really useful to have dataframe functions that return numeric constants available already in all programming languages. We should keep the expression for SQL, but nothing else.


---

* [SPARK-8740](https://issues.apache.org/jira/browse/SPARK-8740) | *Minor* | **Support GitHub OAuth tokens in dev/merge\_spark\_pr.py**

We should allow dev/merge\_spark\_pr.py to use personal GitHub OAuth tokens in order to make authenticated requests. This is necessary to work around per-IP rate limiting issues.


---

* [SPARK-8739](https://issues.apache.org/jira/browse/SPARK-8739) | *Major* | **Illegal character `\r` can be contained in StagePage.**

There is a following code in StagePage.scala.

{code}
                   \|width="$serializationTimeProportion%"\>\</rect\>
                 \|\<rect class="getting-result-time-proportion"
                   \|x="$gettingResultTimeProportionPos%" y="0px" height="26px"
                   \|width="$gettingResultTimeProportion%"\>\</rect\>\</svg\>',
               \|'start': new Date($launchTime),
               \|'end': new Date($finishTime)
             \|}
           \|""".stripMargin.replaceAll("\n", " ")
{code}

The last `replaceAll("\n", "") doesn't work when we checkout and build source code on Windows and deploy on Linux.
It's because when we checkout the source code on Windows, new-line-code is replaced with "\r\n" and replaceAll("\n", "") replaces only "\n".


---

* [SPARK-8738](https://issues.apache.org/jira/browse/SPARK-8738) | *Major* | **Generate better error message in Python for AnalysisException**

The long Java stack trace is hard to read.


---

* [SPARK-8736](https://issues.apache.org/jira/browse/SPARK-8736) | *Critical* | **GBTRegressionModel thresholds prediction but should not**

It outputs 0/1 as though it is doing classification.  It should not.


---

* [SPARK-8735](https://issues.apache.org/jira/browse/SPARK-8735) | *Blocker* | **Expose metrics for runtime memory usage**

Spark has many uses of memory: caching, shuffle, metadata etc. It is useful for users to be able to drill down on the internal memory allocation for memory-intensive operations like aggregations and joins. The goal is to do this for both tungsten and non-tungsten applications.


---

* [SPARK-8727](https://issues.apache.org/jira/browse/SPARK-8727) | *Major* | **Add missing python api**

Add the python api that is missing for

https://issues.apache.org/jira/browse/SPARK-8248
https://issues.apache.org/jira/browse/SPARK-8234
https://issues.apache.org/jira/browse/SPARK-8217
https://issues.apache.org/jira/browse/SPARK-8215
https://issues.apache.org/jira/browse/SPARK-8212


---

* [SPARK-8721](https://issues.apache.org/jira/browse/SPARK-8721) | *Major* | **Rename ExpectsInputTypes =\> AutoCastInputTypes**

ExpectsInputTypes is confusingly named because it does auto type cast.


---

* [SPARK-8718](https://issues.apache.org/jira/browse/SPARK-8718) | *Minor* | **Improve EdgePartition2D for non perfect square number of partitions**

The current implementation of EdgePartition2D has a major limitation:

bq. One of the limitations of this approach is that the number of machines must either be a perfect square. We partially address this limitation by computing the machine assignment to the next largest perfect square and then mapping back down to the actual number of machines. Unfortunately, this can also lead to work imbalance and so it is suggested that a perfect square is used.

To remove this limitation I'm proposing the following code change. It allows us to partition into any number of evenly sized bins while maintaining the property that any vertex will only need to be replicated at most 2 \* sqrt(numParts) times. To maintain current behavior for perfect squares we use the old algorithm in that case, although this could be removed if we dont care about producing the exact same result.

See this IPython notebook for a visualization of what is being proposed [https://github.com/aray/e2d/blob/master/EdgePartition2D.ipynb] and download it to interactively change the number of partitions.


---

* [SPARK-8715](https://issues.apache.org/jira/browse/SPARK-8715) | *Major* | **ArrayOutOfBoundsException for DataFrameStatSuite.crosstab**

columnNames may turn out to be less than 9 elements. Use columnNames.length in the for loop instead.


---

* [SPARK-8714](https://issues.apache.org/jira/browse/SPARK-8714) | *Major* | **Refresh table not working with non-default databases**

I created an external table with sqlContext.createExternalTable from parquet data in S3. After that, I placed a new partition (directory with some parquet files) in S3. Then I did a sql refresh table. It worked great.

I created a separate cluster and changed the database from default to my database. I repeated the above process for the new database. Now the createExternalTable works fine but the refresh table doesn't work. The first time I got an exception stack trace but the next time I don't see any error but when I do a count on the # of rows, the new data is not reflected.


---

* [SPARK-8713](https://issues.apache.org/jira/browse/SPARK-8713) | *Major* | **Support codegen for not thread-safe expressions**

Currently, we disable codegen if any expression is not thread safe. We should support that, but disable caching the compiled expresssions.


---

* [SPARK-8710](https://issues.apache.org/jira/browse/SPARK-8710) | *Major* | **ScalaReflection.mirror should be a def**

Right now, it is a val (https://github.com/apache/spark/blob/master/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/ScalaReflection.scala#L31). This introduces problems when we trigger the creation of ScalaReflection in one thread and use it in another thread.


---

* [SPARK-8709](https://issues.apache.org/jira/browse/SPARK-8709) | *Major* | **Exclude hadoop-client's mockito-all dependency to fix test compilation break for Hadoop 2**

{{build/sbt -Phadoop-1 -Dhadoop.version=2.0.0-mr1-cdh4.1.1 -Phive -Pkinesis-asl -Phive-thriftserver core/test:compile}} currently fails to compile:

{code}
[error] /Users/joshrosen/Documents/spark/core/src/test/java/org/apache/spark/shuffle/unsafe/UnsafeShuffleWriterSuite.java:117: error: cannot find symbol
[error]     when(shuffleMemoryManager.tryToAcquire(anyLong())).then(returnsFirstArg());
[error]                                                       ^
[error]   symbol:   method then(Answer\<Object\>)
[error]   location: interface OngoingStubbing\<Long\>
[error] /Users/joshrosen/Documents/spark/core/src/test/java/org/apache/spark/shuffle/unsafe/UnsafeShuffleWriterSuite.java:408: error: cannot find symbol
[error]       .then(returnsFirstArg()) // Allocate initial sort buffer
[error]       ^
[error]   symbol:   method then(Answer\<Object\>)
[error]   location: interface OngoingStubbing\<Long\>
[error] /Users/joshrosen/Documents/spark/core/src/test/java/org/apache/spark/shuffle/unsafe/UnsafeShuffleWriterSuite.java:435: error: cannot find symbol
[error]       .then(returnsFirstArg()) // Allocate initial sort buffer
[error]       ^
[error]   symbol:   method then(Answer\<Object\>)
[error]   location: interface OngoingStubbing\<Long\>
[error] 3 errors
[error] (core/test:compile) javac returned nonzero exit code
[error] Total time: 60 s, completed Jun 29, 2015 11:03:13 AM
{code}

This is because {{hadoop-client}} pulls in a dependency on {{mockito-all}}, but I recently changed Spark to depend on {{mockito-core}} instead, which caused Hadoop's earlier Mockito version to take precedence over our newer version.


---

* [SPARK-8708](https://issues.apache.org/jira/browse/SPARK-8708) | *Major* | **MatrixFactorizationModel.predictAll() populates single partition only**

When using mllib.recommendation.ALS the RDD returned by .predictAll() has all values pushed into single partition despite using quite high parallelism.

This degrades performance of further processing (I can obviously run .partitionBy()) to balance it but that's still too costly (ie if running .predictAll() in loop for thousands of products) and should be possible to do it rather somehow on the model (automatically)).

Bellow is an example on tiny sample (same on large dataset):

{code:title=pyspark}
\>\>\> r1 = (1, 1, 1.0)
\>\>\> r2 = (1, 2, 2.0)
\>\>\> r3 = (2, 1, 2.0)
\>\>\> r4 = (2, 2, 2.0)
\>\>\> r5 = (3, 1, 1.0)
\>\>\> ratings = sc.parallelize([r1, r2, r3, r4, r5], 5)
\>\>\> ratings.getNumPartitions()
5
\>\>\> users = ratings.map(itemgetter(0)).distinct()
\>\>\> model = ALS.trainImplicit(ratings, 1, seed=10)
\>\>\> predictions\_for\_2 = model.predictAll(users.map(lambda u: (u, 2)))
\>\>\> predictions\_for\_2.glom().map(len).collect()
[0, 0, 3, 0, 0]
{code}


---

* [SPARK-8706](https://issues.apache.org/jira/browse/SPARK-8706) | *Major* | **Implement Pylint / Prospector checks for PySpark**

It would be nice to implement Pylint / Prospector (https://github.com/landscapeio/prospector) checks for PySpark. As with the style checker rules, I'll imagine that we'll want to roll out new rules gradually in order to avoid a mass refactoring commit.

For starters, we should create a pull request that introduces the harness for running the linters, add a configuration file which enables only the lint checks that currently pass, and install the required dependencies on Jenkins. Once we've done this, we can open a series of smaller followup PRs to gradually enable more linting checks and to fix existing violations.


---

* [SPARK-8705](https://issues.apache.org/jira/browse/SPARK-8705) | *Major* | **Javascript error in the web console when `totalExecutionTime` of a task is 0**

Because System.currentTimeMillis() is not accurate for tasks that only need several milliseconds, sometimes totalExecutionTime in makeTimeline will be 0. If totalExecutionTime is 0, there will the following error in the console.

!https://cloud.githubusercontent.com/assets/1000778/8406776/5cd38e04-1e92-11e5-89f2-0c5134fe4b6b.png!


---

* [SPARK-8704](https://issues.apache.org/jira/browse/SPARK-8704) | *Major* | **Add missing methods in StandardScaler (ML and PySpark)**

Add std, mean to StandardScalerModel


---

* [SPARK-8703](https://issues.apache.org/jira/browse/SPARK-8703) | *Major* | **Add CountVectorizer as a ml transformer to convert document to words count vector**

Converts a text document to a sparse vector of token counts. Similar to http://scikit-learn.org/stable/modules/generated/sklearn.feature\_extraction.text.CountVectorizer.html

I can further add an estimator to extract vocabulary from corpus if that's appropriate.


---

* [SPARK-8702](https://issues.apache.org/jira/browse/SPARK-8702) | *Major* | **Avoid massive concating strings in Javascript**

When there are massive tasks, such as {{sc.parallelize(1 to 100000, 10000).count()}}, the generated JS codes have a lot of string concatenations in the stage page, nearly 40 string concatenations for one task.

We can generate the whole string for a task instead of execution string concatenations in the browser.


---

* [SPARK-8695](https://issues.apache.org/jira/browse/SPARK-8695) | *Minor* | **TreeAggregation shouldn't be triggered when it doesn't save wall-clock time**

If an RDD has 5 partitions, tree aggregation doesn't reduce the wall-clock time. Instead, it introduces scheduling and shuffling overhead. We should update the condition to use tree aggregation (code attached):

{code}
      while (numPartitions \> scale + numPartitions / scale) {
{code}


---

* [SPARK-8693](https://issues.apache.org/jira/browse/SPARK-8693) | *Minor* | **profiles and goals are not printed in a nice way**

In our master build, I see
{code}
-Phadoop-1[info] Building Spark (w/Hive 0.13.1) using SBT with these arguments:  -Dhadoop.version=1.0.4[info] Building Spark (w/Hive 0.13.1) using SBT with these arguments:  -Pkinesis-asl[info] Building Spark (w/Hive 0.13.1) using SBT with these arguments:  -Phive-thriftserver[info] Building Spark (w/Hive 0.13.1) using SBT with these arguments:  -Phive[info] Building Spark (w/Hive 0.13.1) using SBT with these arguments:  package[info] Building Spark (w/Hive 0.13.1) using SBT with these arguments:  assembly/assembly[info] Building Spark (w/Hive 0.13.1) using SBT with these arguments:  streaming-kafka-assembly/assembly
{code}
Seems we format the string in a wrong way?


---

* [SPARK-8690](https://issues.apache.org/jira/browse/SPARK-8690) | *Minor* | **Add a setting to disable SparkSQL parquet schema merge by using datasource API**

We need a general config to disable the parquet schema merge feature. 

Our sparkSQL application requirement is 

# In spark 1.1, 1.2, sparkSQL read parquet time is around 1~5 sec. We don't want increase too much read parquet time. Around 2000 parquet file,  the schema is the same. So we don't need  schema merge feature
# We need to use datasource API's feature like partition discovery. So we cannot use Spark 1.2 or pervious version 
# We have a lot of SparkSQL product. We use \*sqlContext.parquetFile(filename)\* to read the parquet file. We don't want to change the application code. One setting to disable this feature is what we want 


In  1.4, we have serval method. But both of them cannot perfect match our use case 

# Set spark.sql.parquet.useDataSourceApi to false. It will match requirement 1,3. But it will use old parquet API and fail in requirement 2 
# Use sqlContext.load("parquet" , Map( "path" -\> "..." , "mergeSchema" -\> "false" ))  will meet requirement 1,2. But it need to change a lot of code we use in parquet load. 
# Spark 1.4 improve a lot on schema merge than 1.3. But directly use default version of parquet will increase the load time from 1~5 sec to 100 sec. It will fail requirement 1. 
# Try PR 5231 config. But it  cannot disable schema merge. 

I think it is better to use a config to disable datasource API's schema merge feature. A PR will be provide later


---

* [SPARK-8688](https://issues.apache.org/jira/browse/SPARK-8688) | *Major* | **Hadoop Configuration has to disable client cache when writing or reading delegation tokens.**

In class \*AMDelegationTokenRenewer\* and \*ExecutorDelegationTokenUpdater\*, Spark will write and read the credentials.
But if we don't disable the \*fs.hdfs.impl.disable.cache\*, Spark will use cached  FileSystem (which will use old token ) to  upload or download file.
Then when the old token is expired, it can't gain the auth to get/put the hdfs.

(I only tested in a very short time with the configuration:
dfs.namenode.delegation.token.renew-interval=3min
dfs.namenode.delegation.token.max-lifetime=10min
I'm not sure whatever it matters.
 )


---

* [SPARK-8687](https://issues.apache.org/jira/browse/SPARK-8687) | *Major* | **Spark on yarn-client mode can't send `spark.yarn.credentials.file` to executor.**

Yarn will set +spark.yarn.credentials.file+ after \*DriverEndpoint\* initialized. So executor will fetch the old configuration and will cause the problem.


---

* [SPARK-8686](https://issues.apache.org/jira/browse/SPARK-8686) | *Minor* | **DataFrame should support `where` with expression represented by String**

DataFrame supports `filter` function with two types of argument, `Column` and `String`. But `where` doesn't.


---

* [SPARK-8685](https://issues.apache.org/jira/browse/SPARK-8685) | *Major* | **dataframe left joins are not working as expected in pyspark**

I have the following code:

{code}
from pyspark import SQLContext

d1 = [{'name':'bob', 'country': 'usa', 'age': 1},
{'name':'alice', 'country': 'jpn', 'age': 2}, 
{'name':'carol', 'country': 'ire', 'age': 3}]

d2 = [{'name':'bob', 'country': 'usa', 'colour':'red'},
{'name':'carol', 'country': 'ire', 'colour':'green'}]

r1 = sc.parallelize(d1)
r2 = sc.parallelize(d2)

sqlContext = SQLContext(sc)
df1 = sqlContext.createDataFrame(d1)
df2 = sqlContext.createDataFrame(d2)
df1.join(df2, (df1.name == df2.name) & (df1.country == df2.country), 'left\_outer').collect()
{code}

When I run it I get the following, (notice in the first row, all join keys are take from the right-side and so are blanked out):

{code}
[Row(age=2, country=None, name=None, colour=None, country=None, name=None),
Row(age=1, country=u'usa', name=u'bob', colour=u'red', country=u'usa', name=u'bob'),
Row(age=3, country=u'ire', name=u'carol', colour=u'green', country=u'ire', name=u'alice')]
{code}

I would expect to get (though ideally without duplicate columns):
{code}
[Row(age=2, country=u'ire', name=u'alice', colour=None, country=None, name=None),
Row(age=1, country=u'usa', name=u'bob', colour=u'red', country=u'usa', name=u'bob'),
Row(age=3, country=u'ire', name=u'carol', colour=u'green', country=u'ire', name=u'alice')]
{code}

The workaround for now is this rather clunky piece of code:
{code}
df2 = sqlContext.createDataFrame(d2).withColumnRenamed('name', 'name2').withColumnRenamed('country', 'country2')
df1.join(df2, (df1.name == df2.name2) & (df1.country == df2.country2), 'left\_outer').collect()
{code}

Also, {{.show()}} works
{code}
sqlContext = SQLContext(sc)
df1 = sqlContext.createDataFrame(d1)
df2 = sqlContext.createDataFrame(d2)
df1.join(df2, (df1.name == df2.name) & (df1.country == df2.country), 'left\_outer').show()
+---+-------+-----+------+-------+-----+
\|age\|country\| name\|colour\|country\| name\|
+---+-------+-----+------+-------+-----+
\|  3\|    ire\|carol\| green\|    ire\|carol\|
\|  2\|    jpn\|alice\|  null\|   null\| null\|
\|  1\|    usa\|  bob\|   red\|    usa\|  bob\|
+---+-------+-----+------+-------+-----+
{code}


---

* [SPARK-8683](https://issues.apache.org/jira/browse/SPARK-8683) | *Major* | **Depend on mockito-core instead of mockito-all**

Spark's tests currently depend on {{mockito-all}}, which bundles Hamcrest and Objenesis classes. Instead, it should depend on {{mockito-core}}, which declares those libraries as Maven dependencies. This is necessary in order to fix a dependency conflict that leads to a NoSuchMethodError when using certain Hamcrest matchers.

See https://github.com/mockito/mockito/wiki/Declaring-mockito-dependency for more details.


---

* [SPARK-8680](https://issues.apache.org/jira/browse/SPARK-8680) | *Major* | **PropagateTypes is very slow when there are lots of columns**

The time for PropagateTypes is O(N\*N), N is the number of columns, which is very slow if there many columns (\>1000)

There easiest optimization could be put `q.inputSet` outside of  transformExpressions which could have about 4 times improvement for N=3000


---

* [SPARK-8678](https://issues.apache.org/jira/browse/SPARK-8678) | *Major* | **Default values in Pipeline API should be immutable**

If the default params are mutable, then if the function or method is called again without any value for the default params, then the changed values are used.


---

* [SPARK-8675](https://issues.apache.org/jira/browse/SPARK-8675) | *Major* | **Executors created by LocalBackend won't get the same classpath as other executor backends**

AFAIK, some spark application always use LocalBackend to do some local initiatives, spark sql is an example.  Starting a LocalPoint won't add user classpath into executor. 

{noformat}
  override def start() {
    localEndpoint = SparkEnv.get.rpcEnv.setupEndpoint(
      "LocalBackendEndpoint", new LocalEndpoint(SparkEnv.get.rpcEnv, scheduler, this, totalCores))
  }
{noformat}

Thus will cause local executor fail with these scenarios,  loading hadoop built-in native libraries,  loading other user defined native libraries, loading user jars,  reading s3 config from a site.xml file, etc


---

* [SPARK-8671](https://issues.apache.org/jira/browse/SPARK-8671) | *Major* | **Add isotonic regression to the pipeline API**

It is useful to have IsotonicRegression under the pipeline API for score calibration. The parameters should be the same as the implementation in spark.mllib package.


---

* [SPARK-8670](https://issues.apache.org/jira/browse/SPARK-8670) | *Blocker* | **Nested columns can't be referenced (but they can be selected)**

This is strange and looks like a regression from 1.3.

{code}
import json

daterz = [
  {
    'name': 'Nick',
    'stats': {
      'age': 28
    }
  },
  {
    'name': 'George',
    'stats': {
      'age': 31
    }
  }
]

df = sqlContext.jsonRDD(sc.parallelize(daterz).map(lambda x: json.dumps(x)))

df.select('stats.age').show()
df['stats.age']  # 1.4 fails on this line
{code}

On 1.3 this works and yields:

{code}
age
28 
31 
Out[1]: Column\<stats.age AS age#2958L\>
{code}

On 1.4, however, this gives an error on the last line:

{code}
+---+
\|age\|
+---+
\| 28\|
\| 31\|
+---+

---------------------------------------------------------------------------
IndexError                                Traceback (most recent call last)
\<ipython-input-1-04bd990e94c6\> in \<module\>()
     19 
     20 df.select('stats.age').show()
---\> 21 df['stats.age']

/path/to/spark/python/pyspark/sql/dataframe.pyc in \_\_getitem\_\_(self, item)
    678         if isinstance(item, basestring):
    679             if item not in self.columns:
--\> 680                 raise IndexError("no such column: %s" % item)
    681             jc = self.\_jdf.apply(item)
    682             return Column(jc)

IndexError: no such column: stats.age
{code}

This means, among other things, that you can't join DataFrames on nested columns.


---

* [SPARK-8669](https://issues.apache.org/jira/browse/SPARK-8669) | *Major* | **Parquet 1.7 files that store binary enums crash when inferring schema**

Loading a Parquet 1.7 file that contains a binary ENUM field in Spark 1.5-SNAPSHOT crashes with the following exception:

{noformat}
  org.apache.spark.sql.AnalysisException: Illegal Parquet type: BINARY (ENUM);
  at org.apache.spark.sql.parquet.CatalystSchemaConverter.illegalType$1(CatalystSchemaConverter.scala:129)
  at org.apache.spark.sql.parquet.CatalystSchemaConverter.convertPrimitiveField(CatalystSchemaConverter.scala:184)
  at org.apache.spark.sql.parquet.CatalystSchemaConverter.convertField(CatalystSchemaConverter.scala:114)
...
{noformat}


---

* [SPARK-8668](https://issues.apache.org/jira/browse/SPARK-8668) | *Major* | **expr function to convert SQL expression into a Column**

selectExpr uses the expression parser to parse a string expressions. would be great to create an "expr" function in functions.scala/functions.py that converts a string into an expression (or a list of expressions separated by comma).


---

* [SPARK-8664](https://issues.apache.org/jira/browse/SPARK-8664) | *Major* | **Add PCA transformer**

Add PCA transformer for ML pipeline


---

* [SPARK-8662](https://issues.apache.org/jira/browse/SPARK-8662) | *Major* | **[SparkR] SparkSQL tests fail in R 3.2**

SparkR tests for equality using `all.equal` on environments fail in R 3.2.

This is due to a change in how equality between environments is handled in the new version of R.

This should most likely not be a huge problem, we'll just have to rewrite some of the tests to be more fine-grained instead of testing equality on entire environments.


---

* [SPARK-8661](https://issues.apache.org/jira/browse/SPARK-8661) | *Major* | **Update comments that contain R statements in ml.LinearRegressionSuite**

Similar to SPARK-8660, but for ml.LinearRegressionSuite: https://github.com/apache/spark/blob/master/mllib/src/test/scala/org/apache/spark/ml/regression/LinearRegressionSuite.scala.


---

* [SPARK-8660](https://issues.apache.org/jira/browse/SPARK-8660) | *Trivial* | **Update comments that contain R statements in ml.logisticRegressionSuite**

We put R statements as comments in unit test. However, there are two issues:

1. JavaDoc style "/\*\* ... \*/" is used instead of normal multiline comment "/\* ... \*/".
2. We put a leading "\*" on each line. It is hard to copy & paste the commands to/from R and verify the result.

For example, in https://github.com/apache/spark/blob/master/mllib/src/test/scala/org/apache/spark/ml/classification/LogisticRegressionSuite.scala#L504

{code}
    /\*\*
     \* Using the following R code to load the data and train the model using glmnet package.
     \*
     \* \> library("glmnet")
     \* \> data \<- read.csv("path", header=FALSE)
     \* \> label = factor(data$V1)
     \* \> features = as.matrix(data.frame(data$V2, data$V3, data$V4, data$V5))
     \* \> weights = coef(glmnet(features,label, family="binomial", alpha = 1.0, lambda = 6.0))
     \* \> weights
     \* 5 x 1 sparse Matrix of class "dgCMatrix"
     \*                      s0
     \* (Intercept) -0.2480643
     \* data.V2      0.0000000
     \* data.V3       .
     \* data.V4       .
     \* data.V5       .
     \*/
{code}

should change to

{code}
    /\*
      Using the following R code to load the data and train the model using glmnet package.
     
      library("glmnet")
      data \<- read.csv("path", header=FALSE)
      label = factor(data$V1)
      features = as.matrix(data.frame(data$V2, data$V3, data$V4, data$V5))
      weights = coef(glmnet(features,label, family="binomial", alpha = 1.0, lambda = 6.0))
      weights

      5 x 1 sparse Matrix of class "dgCMatrix"
                           s0
      (Intercept) -0.2480643
      data.V2      0.0000000
      data.V3       .
      data.V4       .
      data.V5       .
    \*/
{code}


---

* [SPARK-8657](https://issues.apache.org/jira/browse/SPARK-8657) | *Minor* | **Fail to upload conf archive to viewfs**

When I run in spark-1.4 yarn-client mode, I throws the following Exception when trying to upload conf archive to viewfs:

15/06/26 17:56:37 INFO yarn.Client: Uploading resource file:/tmp/spark-095ec3d2-5dad-468c-8d46-2c813457404d/\_\_hadoop\_conf\_\_8436284925771788661
.zip -\> viewfs://nsX/user/ultraman/.sparkStaging/application\_1434370929997\_191242/\_\_hadoop\_conf\_\_8436284925771788661.zip
15/06/26 17:56:38 INFO yarn.Client: Deleting staging directory .sparkStaging/application\_1434370929997\_191242
15/06/26 17:56:38 ERROR spark.SparkContext: Error initializing SparkContext.
java.lang.IllegalArgumentException: Wrong FS: hdfs://SunshineNameNode2:8020/user/ultraman/.sparkStaging/application\_1434370929997\_191242/\_\_had
oop\_conf\_\_8436284925771788661.zip, expected: viewfs://nsX/
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:645)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.getUriPath(ViewFileSystem.java:117)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.getFileStatus(ViewFileSystem.java:346)
        at org.apache.spark.deploy.yarn.ClientDistributedCacheManager.addResource(ClientDistributedCacheManager.scala:67)
        at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$5.apply(Client.scala:341)
        at org.apache.spark.deploy.yarn.Client$$anonfun$prepareLocalResources$5.apply(Client.scala:338)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.deploy.yarn.Client.prepareLocalResources(Client.scala:338)
        at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:559)
        at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:115)
        at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:58)
        at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:141)
        at org.apache.spark.SparkContext.\<init\>(SparkContext.scala:497)
        at org.apache.spark.repl.SparkILoop.createSparkContext(SparkILoop.scala:1017)
        at $line3.$read$$iwC$$iwC.\<init\>(\<console\>:9)
        at $line3.$read$$iwC.\<init\>(\<console\>:18)
        at $line3.$read.\<init\>(\<console\>:20)
        at $line3.$read$.\<init\>(\<console\>:24)
        at $line3.$read$.\<clinit\>(\<console\>)
        at $line3.$eval$.\<init\>(\<console\>:7)
        at $line3.$eval$.\<clinit\>(\<console\>)
        at $line3.$eval.$print(\<console\>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
        at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
        at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)

The bug is easy to fix, we should pass the correct file system object to addResource. The similar issure is: https://github.com/apache/spark/pull/1483. I will attach my bug fix PR very soon.


---

* [SPARK-8656](https://issues.apache.org/jira/browse/SPARK-8656) | *Minor* | **Spark Standalone master json API's worker number is not match web UI number**

Spark standalone master web UI show  "Alive workers" worker number, "Alive Workers" total core, total used cores and "Alive workers" total memory, memory used. 
But the JSON API page "http://MASTERURL:8088/json" shows "all workers" worker number, core, memory number. 
This webUI data is not sync with the JSON API. 

The proper way is to sync the number with webUI and JSON API.


---

* [SPARK-8652](https://issues.apache.org/jira/browse/SPARK-8652) | *Blocker* | **PySpark tests sometimes forget to check return status of doctest.testmod(), masking failing tests**

Several PySpark files call {{doctest.testmod()}} in order to run doctests, but forget to check its return status. As a result, failures will not be automatically detected by our test runner script, creating the potential for bugs to slip through.


---

* [SPARK-8647](https://issues.apache.org/jira/browse/SPARK-8647) | *Minor* | **Potential issues with the constant hashCode**

Hi,

This may be potential bug or performance issue or just the code docs.

The issue is wrt to MatrixUDT class.
 If we decide to put instance of MatrixUDT into the hash based collection.
The hashCode function is returning constant and even though equals method is consistant with hashCode. I don't see the reason why hashCode() = 1994 (i.e constant) has been used.

I was expecting it to be similar to the other matrix class or the vector class .

If there is the reason why we have this code, we should document it properly in the code so that others reading it is fine.

regards,
Alok

Details
=====
a)
In reference to the file 
https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/linalg/Matrices.scala

line 188-197 ie

 override def equals(o: Any): Boolean = {
o match {
case v: MatrixUDT =\> true
case \_ =\> false
}
}
override def hashCode(): Int = 1994

b) the commit is 
https://github.com/apache/spark/commit/11e025956be3818c00effef0d650734f8feeb436
on March 20.


---

* [SPARK-8646](https://issues.apache.org/jira/browse/SPARK-8646) | *Major* | **PySpark does not run on YARN if master not provided in command line**

Running pyspark jobs result in a "no module named pyspark" when run in yarn-client mode in spark 1.4.

[I believe this JIRA represents the change that introduced this error.\| https://issues.apache.org/jira/browse/SPARK-6869 ]

This does not represent a binary compatible change to spark. Scripts that worked on previous spark versions (ie comands the use spark-submit) should continue to work without modification between minor versions.


---

* [SPARK-8644](https://issues.apache.org/jira/browse/SPARK-8644) | *Major* | **SparkException thrown due to Executor exceptions should include caller site in stack trace**

Currently when a job fails due to executor (or other) issues, the exception thrown by Spark has a stack trace which stops at the DAGScheduler EventLoop, which makes it hard to trace back to the user code which submitted the job. It should try to include the user submission stack trace.

Example exception today:

{code}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.RuntimeException: uh-oh!
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33$$anonfun$34$$anonfun$apply$mcJ$sp$1.apply(DAGSchedulerSuite.scala:851)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33$$anonfun$34$$anonfun$apply$mcJ$sp$1.apply(DAGSchedulerSuite.scala:851)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1637)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1095)
	at org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1095)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1765)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1765)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1285)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1276)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1275)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1275)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:749)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:749)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:749)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1486)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1447)
{code}

Here is the part I want to include:

{code}
	at org.apache.spark.rdd.RDD.count(RDD.scala:1095)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33$$anonfun$34.apply$mcJ$sp(DAGSchedulerSuite.scala:851)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33$$anonfun$34.apply(DAGSchedulerSuite.scala:851)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33$$anonfun$34.apply(DAGSchedulerSuite.scala:851)
	at org.scalatest.Assertions$class.intercept(Assertions.scala:997)
	at org.scalatest.FunSuite.intercept(FunSuite.scala:1555)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33.apply$mcV$sp(DAGSchedulerSuite.scala:850)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33.apply(DAGSchedulerSuite.scala:849)
	at org.apache.spark.scheduler.DAGSchedulerSuite$$anonfun$33.apply(DAGSchedulerSuite.scala:849)
	at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
	at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
	at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
	at org.scalatest.Transformer.apply(Transformer.scala:22)
	at org.scalatest.Transformer.apply(Transformer.scala:20)
	at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
	at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
	at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
	at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
{code}

Observe how much more useful the second one is for knowing what started the job.


---

* [SPARK-8640](https://issues.apache.org/jira/browse/SPARK-8640) | *Major* | **Window Function Multiple Frame Processing in Single Processing Step**

The new Window operator is capable of processing different frames for the same Window. In order to enable this the Catalyst Analyzer needs to be modified.

PR will follow as soon as SPARK-8638 gets in.


---

* [SPARK-8639](https://issues.apache.org/jira/browse/SPARK-8639) | *Trivial* | **Instructions for executing jekyll in docs/README.md could be slightly more clear, typo in docs/api.md**

In docs/README.md, the text states around line 31

Execute 'jekyll' from the 'docs/' directory. Compiling the site with Jekyll will create a directory called '\_site' containing index.html as well as the rest of the compiled files.

It might be more clear if we said

Execute 'jekyll build' from the 'docs/' directory to compile the site. Compiling the site with Jekyll will create a directory called '\_site' containing index.html as well as the rest of the compiled files.



In docs/api.md: "Here you can API docs for Spark and its submodules."
should be something like: "Here you can read API docs for Spark and its submodules."


---

* [SPARK-8638](https://issues.apache.org/jira/browse/SPARK-8638) | *Major* | **Window Function Performance Improvements**

Improve the performance of Spark Window Functions in the following cases:
# Much better performance (10x) in the running case (e.g. BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW). The current implementation in spark uses a sliding window approach in these cases. This means that an aggregate is maintained for every row, so space usage is N (N being the number of rows). This also means that all these aggregates all need to be updated separately, this takes N\*(N-1)/2 updates. The running case differs from the Sliding case because we are only adding data to an aggregate function (no reset is required), we only need to maintain one aggregate (like in the UNBOUNDED PRECEDING AND UNBOUNDED case), update the aggregate for each row, and get the aggregate value after each update. This is what the new implementation does. This approach only uses 1 buffer, and only requires N updates; I am currently working on data with window sizes of 500-1000 doing running sums and this saves a lot of time.
#. Fewer comparisons in the sliding case. The current implementation determines frame boundaries for every input row. The new implementation makes more use of the fact that the window is sorted, maintains the boundaries, and only moves them when the current row order changes. This is a minor improvement.
# A single Window node is able to process all types of Frames for the same Partitioning/Ordering. This saves a little time/memory spent buffering and managing partitions. This will be enabled in a follow-up PR.
# A lot of the staging code is moved from the execution phase to the initialization phase. Minor performance improvement, and improves readability of the execution code.

The attached perf\_test.scala file contains s number of queries which can be used to measure the differences between the current and the proposed window function implementation. In the tests the new implementation outperforms the current implementation by a factor 7x in sliding window cases, and by a factor 14x in the running window cases.


---

* [SPARK-8637](https://issues.apache.org/jira/browse/SPARK-8637) | *Blocker* | **Packages argument is wrong in sparkR.init**

This was a bug introduced in https://github.com/apache/spark/pull/6928 and affects branch-1.4 and master branch


---

* [SPARK-8636](https://issues.apache.org/jira/browse/SPARK-8636) | *Major* | **CaseKeyWhen has incorrect NULL handling**

CaseKeyWhen implementation in Spark uses the following equals implementation:

{code}
  private def equalNullSafe(l: Any, r: Any) = {
    if (l == null && r == null) {
      true
    } else if (l == null \|\| r == null) {
      false
    } else {
      l == r
    }
  }
{code}

Which is not correct, since in SQL, NULL is never equal to NULL (actually, it is not unequal either). In this case, a NULL value in a CASE WHEN expression should never match.

For example, you can execute this in MySQL:

{code}
SELECT CASE NULL WHEN NULL THEN "NULL MATCHES" ELSE "NULL DOES NOT MATCH" END FROM DUAL;
{code}

And the result will be "NULL DOES NOT MATCH".


---

* [SPARK-8634](https://issues.apache.org/jira/browse/SPARK-8634) | *Critical* | **Fix flaky test StreamingListenerSuite "receiver info reporting"**

As per the unit test log in https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/35754/

{code}
15/06/24 23:09:10.210 Thread-3495 INFO ReceiverTracker: Starting 1 receivers
15/06/24 23:09:10.270 Thread-3495 INFO SparkContext: Starting job: apply at Transformer.scala:22
...
15/06/24 23:09:14.259 ForkJoinPool-4-worker-29 INFO StreamingListenerSuiteReceiver: Started receiver and sleeping
15/06/24 23:09:14.270 ForkJoinPool-4-worker-29 INFO StreamingListenerSuiteReceiver: Reporting error and sleeping
{code}

it needs at least 4 seconds to receive all receiver events in this slow machine, but `timeout` for `eventually` is only 2 seconds.

We can increase `timeout` to make this test stable.


---

* [SPARK-8633](https://issues.apache.org/jira/browse/SPARK-8633) | *Major* | **List missing model methods in Python Pipeline API**

Most Python models under the pipeline API are implemented as JavaModel wrappers. However, we didn't provide methods to extract information from model. In SPARK-7647, we added weights and intercept to linear models. This JIRA is to list all missing model methods, create JIRAs for each, and link them here.


---

* [SPARK-8630](https://issues.apache.org/jira/browse/SPARK-8630) | *Major* | **Prevent from checkpointing QueueInputDStream**

It's better to prevent from checkpointing QueueInputDStream rather than failing the application when recovering `QueueInputDStream`, so that people can find the issue as soon as possible. See SPARK-8553 for example.


---

* [SPARK-8628](https://issues.apache.org/jira/browse/SPARK-8628) | *Critical* | **Race condition in AbstractSparkSQLParser.parse**

SPARK-5009 introduced the following code in AbstractSparkSQLParser:

{code}
def parse(input: String): LogicalPlan = {
    // Initialize the Keywords.
    lexical.initialize(reservedWords)
    phrase(start)(new lexical.Scanner(input)) match {
      case Success(plan, \_) =\> plan
      case failureOrError =\> sys.error(failureOrError.toString)
    }
  }
{code}

The corresponding initialize method in SqlLexical is not thread-safe:

{code}
  /\* This is a work around to support the lazy setting \*/
  def initialize(keywords: Seq[String]): Unit = {
    reserved.clear()
    reserved ++= keywords
  }
{code}

I'm hitting this when parsing multiple SQL queries concurrently. When one query parsing starts, it empties the reserved keyword list, then a race-condition occurs and other queries fail to parse because they recognize keywords as identifiers.


---

* [SPARK-8625](https://issues.apache.org/jira/browse/SPARK-8625) | *Major* | **Propagate user exceptions in tasks back to driver**

Runtime exceptions that are thrown by user code in Spark are presented to the user as strings (message and stacktrace), rather than the exception object itself. If the exception stores information about the error in fields then these cannot be retrieved.

Exceptions are Serializable, so it would be feasible to return the original object back to the driver as the cause field in SparkException. This would allow the client to retrieve information from the original exception.


---

* [SPARK-8623](https://issues.apache.org/jira/browse/SPARK-8623) | *Major* | **Hadoop RDDs fail to properly serialize configuration**

The following query was executed using "spark-sql --master yarn-client" on 1.5.0-SNAPSHOT:

select \* from wcs.geolite\_city limit 10;

This lead to the following error:

15/06/25 09:38:37 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0, lxhnl008.ad.ing.net): java.lang.NullPointerException
	at org.apache.hadoop.conf.Configuration.\<init\>(Configuration.java:693)
	at org.apache.hadoop.mapred.JobConf.\<init\>(JobConf.java:442)
	at org.apache.hadoop.mapreduce.Job.\<init\>(Job.java:131)
	at org.apache.spark.sql.sources.SqlNewHadoopRDD.getJob(SqlNewHadoopRDD.scala:83)
	at org.apache.spark.sql.sources.SqlNewHadoopRDD.getConf(SqlNewHadoopRDD.scala:89)
	at org.apache.spark.sql.sources.SqlNewHadoopRDD$$anon$1.\<init\>(SqlNewHadoopRDD.scala:127)
	at org.apache.spark.sql.sources.SqlNewHadoopRDD.compute(SqlNewHadoopRDD.scala:124)
	at org.apache.spark.sql.sources.SqlNewHadoopRDD.compute(SqlNewHadoopRDD.scala:66)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)

This does not happen in every case, ie. some queries execute fine, and it is unclear why.

Using just "spark-sql" the query executes fine as well and thus the issue seems to rely in the communication with Yarn. Also the query executes fine (with yarn) in spark-shell.


---

* [SPARK-8621](https://issues.apache.org/jira/browse/SPARK-8621) | *Critical* | **crosstab exception when one of the value is empty**

I think this happened because some value is empty.

{code}
scala\> df1.stat.crosstab("role", "lang")
org.apache.spark.sql.AnalysisException: syntax error in attribute name: ;
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.parseAttributeName(LogicalPlan.scala:145)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveQuoted(LogicalPlan.scala:135)
	at org.apache.spark.sql.DataFrame.resolve(DataFrame.scala:157)
	at org.apache.spark.sql.DataFrame.col(DataFrame.scala:603)
	at org.apache.spark.sql.DataFrameNaFunctions.org$apache$spark$sql$DataFrameNaFunctions$$fillCol(DataFrameNaFunctions.scala:394)
	at org.apache.spark.sql.DataFrameNaFunctions$$anonfun$2.apply(DataFrameNaFunctions.scala:160)
	at org.apache.spark.sql.DataFrameNaFunctions$$anonfun$2.apply(DataFrameNaFunctions.scala:157)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:108)
	at org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:157)
	at org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:147)
	at org.apache.spark.sql.DataFrameNaFunctions.fill(DataFrameNaFunctions.scala:132)
	at org.apache.spark.sql.execution.stat.StatFunctions$.crossTabulate(StatFunctions.scala:132)
	at org.apache.spark.sql.DataFrameStatFunctions.crosstab(DataFrameStatFunctions.scala:91)
{code}


---

* [SPARK-8619](https://issues.apache.org/jira/browse/SPARK-8619) | *Major* | **Can't find the keytab file when recovering the streaming application.**

In a streaming application, I use \*--keytab /root/spark.keytab\* to get the token.
But when the streaming application failed and I wanted to recover it from checkpoint file, there was an error:
{quote}
java.io.IOException: Login failure for spark/hadoop.hadoop.com@HADOOP.COM from keytab spark.keytab-1fd8f7bb-0d3c-4f65-990a-9ae09055cc8d: javax.security.auth.login.LoginException: Unable to obtain password from user
{quote}

Spark had changed the configuration, so the checkpoint can't find the file:
{code:title=Client.java @ Function: setupCredentials \|borderStyle=solid}
      val keytabFileName = f.getName + "-" + UUID.randomUUID().toString
      UserGroupInformation.loginUserFromKeytab(args.principal, args.keytab)
      loginFromKeytab = true
      sparkConf.set("spark.yarn.keytab", keytabFileName)
{code}

So when recovering the application, we should ignore this configurations.


---

* [SPARK-8615](https://issues.apache.org/jira/browse/SPARK-8615) | *Minor* | **sql programming guide recommends deprecated code**

The Spark 1.4 sql programming guide has an example code on how to use JDBC tables:
https://spark.apache.org/docs/latest/sql-programming-guide.html#jdbc-to-other-databases

sqlContext.load("jdbc", Map(...))

However this code complies with a warning, and recommends to do this:

 sqlContext.read.format("jdbc").options(Map(...)).load()


---

* [SPARK-8613](https://issues.apache.org/jira/browse/SPARK-8613) | *Major* | **Add a param for disabling of feature scaling, default to true**

Add a param to disable feature scaling. Do this distinct from disabling scaling in any particular alg incase someone wants to work on logistic while work in linear is in progress.


---

* [SPARK-8610](https://issues.apache.org/jira/browse/SPARK-8610) | *Major* | **Separate Row and InternalRow (part 2)**

Currently, we use GenericRow both for Row and InternalRow, which is confusing because it could contain Scala type also Catalyst types.

We should have different implementation for them, to avoid some potential bugs.


---

* [SPARK-8607](https://issues.apache.org/jira/browse/SPARK-8607) | *Critical* | **SparkR - Third party jars are not being added to classpath in SparkRBackend**

Getting a ClassNotFound exception when using the --jars flag in the SparkR shell, as well as when creating a sparkContext with sparkR.init.

Related to https://issues.apache.org/jira/browse/SPARK-5185


---

* [SPARK-8606](https://issues.apache.org/jira/browse/SPARK-8606) | *Critical* | **Exceptions in RDD.getPreferredLocations() and getPartitions() should not be able to crash DAGScheduler**

RDD.getPreferredLocations() and RDD.getPartitions() may throw exceptions but the DAGScheduler does not guard against this, leaving it vulnerable to crashing and stopping the SparkContext if exceptions occur there.

We should fix this by adding more try blocks around these calls in DAGScheduler.


---

* [SPARK-8604](https://issues.apache.org/jira/browse/SPARK-8604) | *Major* | **Parquet data source doesn't write summary file while doing appending**

Currently, Parquet and ORC data sources don't set their output format class, as we override the output committer in Spark SQL. However, SPARK-8678 ignores user defined output committer class while doing appending to avoid potential issues brought by direct output committers (e.g. {{DirectParquetOutputCommitter}}). This makes both of these data sources fallback to the default output committer retrieved from {{TextOutputFormat}}, which is {{FileOutputCommitter}}. For ORC, it's totally fine since ORC itself just uses {{FileOutputCommitter}}. But for Parquet, {{ParquetOutputCommitter}} also writes the summary files while committing the job.


---

* [SPARK-8601](https://issues.apache.org/jira/browse/SPARK-8601) | *Major* | **Add an option to disable feature scaling in Linear Regression**

See parent task for more details.


---

* [SPARK-8600](https://issues.apache.org/jira/browse/SPARK-8600) | *Major* | **Naive Bayes API for spark.ml Pipelines**

Create a NaiveBayes API for the spark.ml Pipelines API. This should wrap the existing NaiveBayes implementation under spark.mllib package. Should also keep the parameter names consistent. The output columns could include both the prediction and confidence scores.


---

* [SPARK-8599](https://issues.apache.org/jira/browse/SPARK-8599) | *Critical* | **Improve non-deterministic expression handling**

Right now, we are using expressions for Random distribution generating expressions. But, we have to track them in lots of places in the optimizer to handle them carefully. Otherwise, these expressions will be treated as stateless expressions and have unexpected behaviors (e.g. SPARK-8023).


---

* [SPARK-8598](https://issues.apache.org/jira/browse/SPARK-8598) | *Minor* | **Implementation of 1-sample, two-sided, Kolmogorov Smirnov Test for RDDs**

We have implemented a 1-sample, two-sided version of the Kolmogorov Smirnov test, which tests the null hypothesis that the sample comes from a given continuous distribution. We provide various functions to access the functionality: namely, a function that takes an RDD[Double] of the data and a lambda to calculate the CDF, a function that takes an RDD[Double] and an Iterator[(Double,Double,Double)] =\> Iterator[Double] which uses mapPartition to provide an optimized way to perform the calculation when the CDF calculation requires a non-serializable object (e.g. the apache math commons real distributions), and finally a function that takes an RDD[Double] and a String name of the theoretical distribution to be used. The appropriate result class has been added, as well as tests to the HypothesisTestSuite


---

* [SPARK-8596](https://issues.apache.org/jira/browse/SPARK-8596) | *Major* | **Install and configure RStudio server on Spark EC2**

This will make it convenient for R users to use SparkR from their browsers


---

* [SPARK-8593](https://issues.apache.org/jira/browse/SPARK-8593) | *Major* | **History Server doesn't show complete application when one attempt inprogress**

The Spark history server doesn't show an application if the first attempt of the application is still inprogress.  

Here are the files in hdfs:
-rwxrwx---   3 tgraves hdfs        234 2015-06-24 15:49 sparkhistory/application\_1433751980223\_18926\_1.inprogress
-rwxrwx---   3 tgraves hdfs    9609450 2015-06-24 15:51 sparkhistory/application\_1433751980223\_18926\_2


The UI shows them if I set the showIncomplete=true.

Removing the inprogress file allows it to show up when showIncomplete is false.

It should be smart enough to atleast show the second successful attempt.


---

* [SPARK-8592](https://issues.apache.org/jira/browse/SPARK-8592) | *Minor* | **CoarseGrainedExecutorBackend: Cannot register with driver =\> NPE**

I cannot reproduce this consistently but when submitting jobs just after another finished it will not come up:

{code}
15/06/24 14:57:24 INFO WorkerWatcher: Connecting to worker akka.tcp://sparkWorker@10.0.7.171:39135/user/Worker
15/06/24 14:57:24 INFO WorkerWatcher: Successfully connected to akka.tcp://sparkWorker@10.0.7.171:39135/user/Worker
15/06/24 14:57:24 ERROR CoarseGrainedExecutorBackend: Cannot register with driver: akka.tcp://sparkDriver@172.17.0.109:47462/user/CoarseGrainedScheduler
java.lang.NullPointerException
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.actorRef$lzycompute(AkkaRpcEnv.scala:273)
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.actorRef(AkkaRpcEnv.scala:273)
	at org.apache.spark.rpc.akka.AkkaRpcEndpointRef.toString(AkkaRpcEnv.scala:313)
	at java.lang.String.valueOf(String.java:2982)
	at scala.collection.mutable.StringBuilder.append(StringBuilder.scala:200)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3.apply(CoarseGrainedSchedulerBackend.scala:125)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1$$anonfun$applyOrElse$3.apply(CoarseGrainedSchedulerBackend.scala:125)
	at org.apache.spark.Logging$class.logInfo(Logging.scala:59)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint.logInfo(CoarseGrainedSchedulerBackend.scala:69)
	at org.apache.spark.scheduler.cluster.CoarseGrainedSchedulerBackend$DriverEndpoint$$anonfun$receiveAndReply$1.applyOrElse(CoarseGrainedSchedulerBackend.scala:125)
	at org.apache.spark.rpc.akka.AkkaRpcEnv.org$apache$spark$rpc$akka$AkkaRpcEnv$$processMessage(AkkaRpcEnv.scala:178)
	at org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1$$anonfun$receiveWithLogging$1$$anonfun$applyOrElse$4.apply$mcV$sp(AkkaRpcEnv.scala:127)
	at org.apache.spark.rpc.akka.AkkaRpcEnv.org$apache$spark$rpc$akka$AkkaRpcEnv$$safelyCall(AkkaRpcEnv.scala:198)
	at org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1$$anonfun$receiveWithLogging$1.applyOrElse(AkkaRpcEnv.scala:126)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:36)
	at org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:59)
	at org.apache.spark.util.ActorLogReceive$$anon$1.apply(ActorLogReceive.scala:42)
	at scala.PartialFunction$class.applyOrElse(PartialFunction.scala:123)
	at org.apache.spark.util.ActorLogReceive$$anon$1.applyOrElse(ActorLogReceive.scala:42)
	at akka.actor.Actor$class.aroundReceive(Actor.scala:465)
	at org.apache.spark.rpc.akka.AkkaRpcEnv$$anonfun$actorRef$lzycompute$1$1$$anon$1.aroundReceive(AkkaRpcEnv.scala:93)
	at akka.actor.ActorCell.receiveMessage(ActorCell.scala:516)
	at akka.actor.ActorCell.invoke(ActorCell.scala:487)
	at akka.dispatch.Mailbox.processMailbox(Mailbox.scala:238)
	at akka.dispatch.Mailbox.run(Mailbox.scala:220)
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)
{code}


---

* [SPARK-8584](https://issues.apache.org/jira/browse/SPARK-8584) | *Major* | **Better exception message if invalid checkpoint dir is specified**

If we're running Spark on a cluster, the checkpoint dir must be a non-local path. Otherwise, the attempt to read from a checkpoint will fail because the checkpoint files are written on the executors, not on the driver.

Currently, the error message that you get looks something like the following, which is not super intuitive:
{code}
Checkpoint RDD 3 (0) has different number of partitions than original RDD 2 (100)
{code}


---

* [SPARK-8583](https://issues.apache.org/jira/browse/SPARK-8583) | *Major* | **Refactor python/run-tests to integrate with dev/run-test's module system**

We should refactor the {{python/run-tests}} script to be written in Python and integrate with the recent {{dev/run-tests}} module system so that we can more granularly skip Python tests in the pull request builder.


---

* [SPARK-8581](https://issues.apache.org/jira/browse/SPARK-8581) | *Minor* | **Simplify and clean up the checkpointing code**

It is an old piece of code and a little overly complex at the moment. We can rewrite this to improve the readability and preserve exactly the same semantics.


---

* [SPARK-8580](https://issues.apache.org/jira/browse/SPARK-8580) | *Major* | **Test Parquet interoperability and compatibility with other libraries/systems**

As we are implementing Parquet backwards-compatibility rules for Spark 1.5.0 to improve interoperability with other systems (reading non-standard Parquet files they generate, and generating standard Parquet files), it would be good to have a set of standard test Parquet files generated by various systems/tools (parquet-thrift, parquet-avro, parquet-hive, Impala, and old versions of Spark SQL) to ensure compatibility.


---

* [SPARK-8579](https://issues.apache.org/jira/browse/SPARK-8579) | *Major* | **Support arbitrary object in UnsafeRow**

It's common to run count(distinct xxx) in SQL, the data type will be UDT of OpenHashSet, it's good that we could use UnsafeRow to reducing the memory usage during aggregation.

Also for DecimalType, which could be used inside the grouping key.


---

* [SPARK-8578](https://issues.apache.org/jira/browse/SPARK-8578) | *Major* | **Should ignore user defined output committer when appending data**

When appending data to a file system via Hadoop API, it's safer to ignore user defined output committer classes like {{DirectParquetOutputCommitter}}. Because it's relatively hard to handle task failure in this case.  For example, {{DirectParquetOutputCommitter}} directly writes to the output directory to boost write performance when working with S3. However, there's no general way to determine task output file path of a specific task in Hadoop API, thus we don't know to revert a failed append job. (When doing overwrite, we can just remove the whole output directory.)


---

* [SPARK-8576](https://issues.apache.org/jira/browse/SPARK-8576) | *Minor* | **Add spark-ec2 options to assign launched instances into IAM roles and to set instance-initiated shutdown behavior**

There are 2 EC2 options that would be useful to add.

\* One is the ability to assign IAM roles to launched instances.
\* The other is the ability to configure instances to self-terminate when they initiate a shutdown.

Both of these options are useful when spark-ec2 is being used as part of an automated pipeline and the engineers want to minimize the need to pass around AWS keys for access (replaced by IAM role) and to be able to launch a cluster that can terminate itself cleanly.


---

* [SPARK-8575](https://issues.apache.org/jira/browse/SPARK-8575) | *Minor* | **Deprecate callUDF in favor of udf**

Follow-up of [SPARK-8356\|https://issues.apache.org/jira/browse/SPARK-8356] to use {{callUDF}} in favor of {{udf}} wherever possible.


---

* [SPARK-8572](https://issues.apache.org/jira/browse/SPARK-8572) | *Critical* | **Type coercion for ScalaUDFs**

Seems we do not do type coercion for ScalaUDFs. The following code will hit a runtime exception.
{code}
import org.apache.spark.sql.functions.\_
val myUDF = udf((x: Int) =\> x + 1)
val df = sqlContext.range(1, 10).toDF("i").select(myUDF($"i"))
df.explain(true)
df.show
{code}
It is also good to check if we do type coercion for PythonUDFs.


---

* [SPARK-8570](https://issues.apache.org/jira/browse/SPARK-8570) | *Minor* | **Improve MLlib Local Matrix Documentation.**

Update the MLlib Data Types Local Matrix documentation as follows:

-Include information on sparse matrices.
-Add sparse matrix examples to the existing Scala and Java examples.
-Add Python examples for both dense and sparse matrices (currently no Python examples exist for the Local Matrix section).


---

* [SPARK-8568](https://issues.apache.org/jira/browse/SPARK-8568) | *Critical* | **Prevent accidental use of "and" and "or" to build invalid expressions in Python**

In Spark DataFrames (and in Pandas as well), the correct way to construct a conjunctive expression is to use the bitwise and operator, i.e.: "(x \> 5) & (y \> 6)". 

However, a lot of users assume that they should be using the Python "and" keyword, i.e. doing "x \> 5 and y \> 6". Python's boolean evaluation logic converts "x \> 5 and y \> 6" into just "y \> 6" (since "x \> 5" is not None). This is super confusing & error prone.

We should override \_\_bool\_\_ and \_\_nonzero\_\_ for Column to throw an exception if users call "and" and "or" on Column expressions.

Background: see this blog post http://www.nodalpoint.com/unexpected-behavior-of-spark-dataframe-filter-method/


---

* [SPARK-8567](https://issues.apache.org/jira/browse/SPARK-8567) | *Critical* | **Flaky test: o.a.s.sql.hive.HiveSparkSubmitSuite --jars**

Seems tests in HiveSparkSubmitSuite fail with timeout pretty frequently.


---

* [SPARK-8563](https://issues.apache.org/jira/browse/SPARK-8563) | *Major* | **Bug that IndexedRowMatrix.computeSVD() yields the U with wrong numCols**

IndexedRowMatrix.computeSVD() yields a wrong U which \*U.numCols() = self.nCols\*.

It should have been \*U.numCols() = k = svd.U.numCols()\*

{code}
self = U \* sigma \* V.transpose
(m x n) = (m x n) \* (k x k) \* (k x n)
--\>
(m x n) = (m x k) \* (k x k) \* (k x n)
{code}


Proposed fix: https://github.com/apache/spark/pull/6953


---

* [SPARK-8560](https://issues.apache.org/jira/browse/SPARK-8560) | *Major* | **Executors page displays negative active tasks**

This is caused by resubmitted tasks. See PR for more detail.


---

* [SPARK-8559](https://issues.apache.org/jira/browse/SPARK-8559) | *Major* | **Support association rule generation in FPGrowth**

It will be more useful and practical to include the association rule generation part for real applications, though it is not hard by user to find association rules from the frequent itemset with frequency which is output by FP growth.
However how to generate association rules in an efficient way is not widely reported.


---

* [SPARK-8558](https://issues.apache.org/jira/browse/SPARK-8558) | *Minor* | **Script /dev/run-tests fails when \_JAVA\_OPTIONS env var set**

Script /dev/run-tests.py fails when \_JAVA\_OPTIONS env. var set.

Steps to reproduce in linux:
1. export \_JAVA\_OPTIONS="-Xmx2048M
2. ./dev/run-tests

[pivot@fe2s spark]$ ./dev/run-tests
Traceback (most recent call last):
  File "./dev/run-tests.py", line 793, in \<module\>
    main()
  File "./dev/run-tests.py", line 722, in main
    java\_version = determine\_java\_version(java\_exe)
  File "./dev/run-tests.py", line 484, in determine\_java\_version
    version, update = version\_str.split('\_')  # eg ['1.8.0', '25']
ValueError: need more than 1 value to unpack

The problem is in 'determine\_java\_version' function in run-tests.py.
It runs 'java' and extracts version from output. However when \_JAVA\_OPTIONS set the output of 'java' command is different and it breaks parser. See the first line

[pivot@fe2s spark]$ java -version
Picked up \_JAVA\_OPTIONS: -Xmx2048M
java version "1.8.0\_31"
Java(TM) SE Runtime Environment (build 1.8.0\_31-b13)
Java HotSpot(TM) 64-Bit Server VM (build 25.31-b07, mixed mode)


---

* [SPARK-8554](https://issues.apache.org/jira/browse/SPARK-8554) | *Major* | **Add the SparkR document files to `.rat-excludes` for `./dev/check-license`**

{noformat}
\> ./dev/check-license \| grep -v boto

Could not find Apache license headers in the following files:
 !????? /Users/01004981/local/src/spark/myspark/R/lib/SparkR/INDEX
 !????? /Users/01004981/local/src/spark/myspark/R/lib/SparkR/help/AnIndex
 !????? /Users/01004981/local/src/spark/myspark/R/lib/SparkR/html/00Index.html
 !????? /Users/01004981/local/src/spark/myspark/R/lib/SparkR/html/R.css
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/DataFrame.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/GroupedData.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/agg.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/arrange.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/cache-methods.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/cacheTable.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/cancelJobGroup.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/clearCache.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/clearJobGroup.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/collect-methods.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/column.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/columns.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/count.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/createDataFrame.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/createExternalTable.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/describe.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/distinct.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/dropTempTable.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/dtypes.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/except.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/explain.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/filter.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/first.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/groupBy.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/hashCode.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/head.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/infer\_type.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/insertInto.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/intersect.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/isLocal.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/join.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/jsonFile.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/limit.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/nafunctions.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/parquetFile.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/persist.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/print.jobj.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/print.structField.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/print.structType.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/printSchema.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/read.df.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/registerTempTable.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/repartition.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/sample.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/saveAsParquetFile.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/saveAsTable.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/schema.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/select.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/selectExpr.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/setJobGroup.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/show.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/showDF.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/sparkR.init.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/sparkR.stop.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/sparkRHive.init.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/sparkRSQL.init.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/sql.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/structField.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/structType.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/table.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/tableNames.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/tables.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/take.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/uncacheTable.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/unionAll.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/unpersist-methods.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/withColumn.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/withColumnRenamed.Rd
 !????? /Users/01004981/local/src/spark/myspark/R/pkg/man/write.df.Rd
{noformat}


---

* [SPARK-8548](https://issues.apache.org/jira/browse/SPARK-8548) | *Major* | **Remove the trailing whitespaces from the SparkR files**

On the {{lint-r}}'s advice, remove the trailing whiltespace from the SparkR files.


---

* [SPARK-8541](https://issues.apache.org/jira/browse/SPARK-8541) | *Minor* | **sumApprox and meanApprox doctests are incorrect**

The doctests for sumApprox and meanApprox methods test against an upper bound but not a lower bound. If there was a regression in the underlying code that caused things to go wrong the doctest may not fail. For example if sumApprox returned 0 the doctest would return -1 which is less than 0.05. Solution is to use the abs() function to test that the approximate answer is within 5% of the exact answer.


---

* [SPARK-8539](https://issues.apache.org/jira/browse/SPARK-8539) | *Major* | **LinearRegressionSummary class for storing LR training stats**

This is a subtask of [SPARK-7674].  Please see the design doc linked from [SPARK-7674] for details.

LinearRegressionSummary will store stats from training, such as objective value on each iteration, avg residual, etc.  It will store a LinearRegressionResults instance for the results on the training dataset.

The assignee should make sure to coordinate with others working on [SPARK-7674].

Note: This can add a limited number of metrics.  We can always add more in the future.


---

* [SPARK-8538](https://issues.apache.org/jira/browse/SPARK-8538) | *Major* | **LinearRegressionResults class for storing LR results on data**

This is an initial task for [SPARK-7674].  Please see the design doc linked from [SPARK-7674] for details.

LinearRegressionResults will store results for a (model, dataset) pair, provided via lazy evaluation.  It can take advantage of existing code such as RegressionMetrics.

Eventually, we may need to introduce an internal abstraction for RegressionResults, but that will not be necessary for this initial tasks.  However, the assignee should make sure to coordinate with others working on [SPARK-7674].

Note: This can add a limited number of metrics.  We can always add more in the future.


---

* [SPARK-8537](https://issues.apache.org/jira/browse/SPARK-8537) | *Major* | **Add a validation rule about the curly braces in SparkR to `.lintr`**

Add a validation rule about the curly braces in SparkR to `.lintr`


---

* [SPARK-8536](https://issues.apache.org/jira/browse/SPARK-8536) | *Major* | **Generalize LDA to asymmetric doc-topic priors**

Several users have requested LDA using asymmetric priors, including learning those hyperparameters.  We should add this functionality.

Note that it could be added to EM, online LDA, or both.  I suggest adding it to online LDA initially, followed by EM in a separate PR.

This JIRA is now limited to the document-topic prior (docConcentration).  See linked JIRA for topicConcentration, which is lower priority.


---

* [SPARK-8535](https://issues.apache.org/jira/browse/SPARK-8535) | *Major* | **PySpark : Can't create DataFrame from Pandas dataframe with no explicit column name**

Trying to create a Spark DataFrame from a pandas dataframe with no explicit column name : 

pandasDF = pd.DataFrame([[1, 2], [5, 6]])
sparkDF = sqlContext.createDataFrame(pandasDF)

\*\*\*\*\*\*\*\*\*\*\*

----\> 1 sparkDF = sqlContext.createDataFrame(pandasDF)

/usr/local/Cellar/apache-spark/1.4.0/libexec/python/pyspark/sql/context.pyc in createDataFrame(self, data, schema, samplingRatio)
    344 
    345         jrdd = self.\_jvm.SerDeUtil.toJavaArray(rdd.\_to\_java\_object\_rdd())
--\> 346         df = self.\_ssql\_ctx.applySchemaToPythonRDD(jrdd.rdd(), schema.json())
    347         return DataFrame(df, self)
    348 

/usr/local/Cellar/apache-spark/1.4.0/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/java\_gateway.py in \_\_call\_\_(self, \*args)
    536         answer = self.gateway\_client.send\_command(command)
    537         return\_value = get\_return\_value(answer, self.gateway\_client,
--\> 538                 self.target\_id, self.name)
    539 
    540         for temp\_arg in temp\_args:

/usr/local/Cellar/apache-spark/1.4.0/libexec/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get\_return\_value(answer, gateway\_client, target\_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--\> 300                     format(target\_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o87.applySchemaToPythonRDD.


---

* [SPARK-8532](https://issues.apache.org/jira/browse/SPARK-8532) | *Blocker* | **In Python's DataFrameWriter, save/saveAsTable/json/parquet/jdbc always override mode**

Although users can use {{df.write.mode("overwrite")}} to specify the mode, when save/saveAsTable/json/parquet/jdbc is called, this mode will be overridden. For example, the implementation of json method is 
{code}
def json(self, path, mode="error"):
  self.\_jwrite.mode(mode).json(path)
{code}
If users only call {{json("path")}}, the mode will be "error" instead of the mode specified in the mode method.


---

* [SPARK-8528](https://issues.apache.org/jira/browse/SPARK-8528) | *Minor* | **Add applicationId to SparkContext object in pyspark**

It is available in Scala API.

Our use case - we want to log applicationId (YARN in hour case) to request help with troubleshooting from the DevOps if our app had failed.


---

* [SPARK-8525](https://issues.apache.org/jira/browse/SPARK-8525) | *Minor* | **Bug in Streaming k-means documentation**

The expected input format is wrong in Streaming K-means documentation.
https://spark.apache.org/docs/latest/mllib-clustering.html#streaming-k-means

It might be a bug in implementation though, not sure.

There shouldn't be any spaces in test data points. I.e. instead of 
(y, [x1, x2, x3]) it should be
(y,[x1,x2,x3])

The exception thrown 
org.apache.spark.SparkException: Cannot parse a double from:  
	at org.apache.spark.mllib.util.NumericParser$.parseDouble(NumericParser.scala:118)
	at org.apache.spark.mllib.util.NumericParser$.parseTuple(NumericParser.scala:103)
	at org.apache.spark.mllib.util.NumericParser$.parse(NumericParser.scala:41)
	at org.apache.spark.mllib.regression.LabeledPoint$.parse(LabeledPoint.scala:49)


Also I would improve documentation saying explicitly that expected data types for both 'x' and 'y' is Double. At the moment it's not obvious especially for 'y'.


---

* [SPARK-8522](https://issues.apache.org/jira/browse/SPARK-8522) | *Major* | **Disable feature scaling in Linear and Logistic Regression**

All compressed sensing applications, and some of the regression use-cases will have better result by turning the feature scaling off. However, if we implement this naively by training the dataset without doing any standardization, the rate of convergency will not be good. This can be implemented by still standardizing the training dataset but we penalize each component differently to get effectively the same objective function but a better numerical problem. As a result, for those columns with high variances, they will be penalized less, and vice versa. Without this, since all the features are standardized, so they will be penalized the same.

In R, there is an option for this.
`standardize`	
Logical flag for x variable standardization, prior to fitting the model sequence. The coefficients are always returned on the original scale. Default is standardize=TRUE. If variables are in the same units already, you might not wish to standardize. See details below for y standardization with family="gaussian".


---

* [SPARK-8521](https://issues.apache.org/jira/browse/SPARK-8521) | *Major* | **Feature Transformers in 1.5**

This is a list of feature transformers we plan to add in Spark 1.5. Feel free to propose useful transformers that are not on the list.


---

* [SPARK-8511](https://issues.apache.org/jira/browse/SPARK-8511) | *Major* | **Modify ML Python tests to remove saved models**

According to the reference of python, {{os.removedirs}} doesn't work if there are any files in the directory we want to remove.
https://docs.python.org/3/library/os.html#os.removedirs

Instead of that, using {{shutil.rmtree()}} would be better to remove a temporary directory to test for saving model.
https://github.com/apache/spark/blob/branch-1.4/python%2Fpyspark%2Fmllib%2Fregression.py#L137


---

* [SPARK-8508](https://issues.apache.org/jira/browse/SPARK-8508) | *Minor* | **Test case "SQLQuerySuite.test script transform for stderr" generates super long output**

This test case writes 100,000 lines of integer triples to stderr, and makes Jenkins build output unnecessarily large and hard to debug.


---

* [SPARK-8506](https://issues.apache.org/jira/browse/SPARK-8506) | *Minor* | **SparkR does not provide an easy way to depend on Spark Packages when performing init from inside of R**

While packages can be specified when using the sparkR or sparkSubmit scripts, the programming guide tells people to create their spark context using the R shell + init. The init does have a parameter for jars but no parameter for packages. Setting the SPARKR\_SUBMIT\_ARGS overwrites some necessary information. I think a good solution would just be adding another field to the init function to allow people to specify packages in the same way as jars.


---

* [SPARK-8498](https://issues.apache.org/jira/browse/SPARK-8498) | *Major* | **Fix NullPointerException in error-handling path in UnsafeShuffleWriter**

This bug was reported by [~prudenko] on the dev list.  When the {{tungsten-sort}} shuffle manager was enabled, an executor died with the following exception:

{code}
15/06/19 17:53:35 WARN TaskSetManager: Lost task 38.0 in stage 41.0 (TID 3176, ip-10-50-225-214.ec2.internal): java.lang.NullPointerException
        at org.apache.spark.shuffle.unsafe.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:151)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}

I think that this is actually due to an error-handling issue.  In the stack trace, the NPE is being thrown from an error-handling branch of a `finally` block:

{code}
public void write(scala.collection.Iterator\<Product2\<K, V\>\> records) throws IOException {
    boolean success = false;
    try {
      while (records.hasNext()) {
        insertRecordIntoSorter(records.next());
      }
      closeAndWriteOutput();
      success = true;
    } finally {
      if (!success) {
        sorter.cleanupAfterError();  // \<---- this is the line throwing the error
      }
    }
  }
{code}

I suspect that what's happening is that an exception is being thrown from user / upstream code in the initial call to records.next(), but the error-handling block is failing because sorter == null since we haven't initialized it yet.

We should fix this bug with a {{sorter != null}} check and should also add a regression test to ShuffleSuite to ensure that exceptions thrown by user code at this step of the shuffle write path don't get masked by error-handling bugs inside of the shuffle code.


---

* [SPARK-8495](https://issues.apache.org/jira/browse/SPARK-8495) | *Major* | **Add a `.lintr` file to validate the SparkR files and the `lint-r` script**

https://issues.apache.org/jira/browse/SPARK-6813

As we discussed, we are planning to go with {{lintr}} to validate the SparkR files. So we should add a rules for it as a {{.lintr}} file.


---

* [SPARK-8489](https://issues.apache.org/jira/browse/SPARK-8489) | *Critical* | **Add regression tests for SPARK-8470**

See SPARK-8470 for more detail. Basically the Spark Hive code silently overwrites the context class loader populated in SparkSubmit, resulting in certain classes missing when we do reflection in `SQLContext#createDataFrame`.

That issue is already resolved in https://github.com/apache/spark/pull/6891, but we should add a regression test for the specific manifestation of the bug in SPARK-8470.


---

* [SPARK-8484](https://issues.apache.org/jira/browse/SPARK-8484) | *Critical* | **Add TrainValidationSplit to ml.tuning**

Add TrainValidationSplit for hyper-parameter tuning. It randomly splits the input dataset into train and validation and use evaluation metric on the validation set to select the best model. It should be similar to CrossValidator, but simpler and less expensive.


---

* [SPARK-8483](https://issues.apache.org/jira/browse/SPARK-8483) | *Major* | **Remove commons-lang3 depedency from flume-sink**

flume-sink module uses only one method from commons-lang3. Since the build would become complex if we create an assembly and would likely make it more difficult for customers, let's just remove the dependency altogether.


---

* [SPARK-8482](https://issues.apache.org/jira/browse/SPARK-8482) | *Trivial* | **Add M4 instances support**

AWS released M4 instances recently (https://aws.amazon.com/blogs/aws/the-new-m4-instance-type-bonus-price-reduction-on-m3-c4/) It will be nice to have support of these instances as well.


---

* [SPARK-8481](https://issues.apache.org/jira/browse/SPARK-8481) | *Minor* | **GaussianMixtureModel predict accepting single vector**

GaussianMixtureModel lacks a method to predict a cluster for a single input vector where no spark context would be involved, i.e.
/\*\* Maps given point to its cluster index. \*/
def predict(point: Vector) : Int


---

* [SPARK-8479](https://issues.apache.org/jira/browse/SPARK-8479) | *Minor* | **Add numNonzeros and numActives to linalg.Matrices**

Add
numNonzeros to scan the number of non zero values and numActives to show the number of values explicitly stored.


---

* [SPARK-8478](https://issues.apache.org/jira/browse/SPARK-8478) | *Minor* | **Harmonize UDF-related code to use uniformly UDF instead of Udf**

Some UDF-related code uses "Udf" naming instead of "UDF".
This JIRA uniformizes the naming in favor of UDF.


---

* [SPARK-8476](https://issues.apache.org/jira/browse/SPARK-8476) | *Minor* | **Setters inc/decDiskBytesSpilled in TaskMetrics should also be private.**

This is a follow-up of SPARK-3288.


---

* [SPARK-8475](https://issues.apache.org/jira/browse/SPARK-8475) | *Minor* | **SparkSubmit with Ivy jars is very slow to load with no internet access**

Spark Submit adds maven central & spark bintray to the ChainResolver before it adds any external resolvers. 

https://github.com/apache/spark/blob/branch-1.4/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L821

When running on a cluster without internet access, this means the spark shell takes forever to launch as it tries these two remote repos before the ones specified in the --repositories list. In our case we have a proxy which the cluster can access it and supply it via --repositories.

This is also a problem for users who maintain a proxy for maven/ivy repos with something like Nexus/Artifactory. Having a repo proxy is popular at many organisations so I'd say this would be a useful change for these users as well. In the current state even if a maven central proxy is supplied, it will still try and hit central. 

I see two options for a fix;

\* Change the order repos are added to the ChainResolver, making the --repositories supplied repos come before anything else. https://github.com/apache/spark/blob/branch-1.4/core/src/main/scala/org/apache/spark/deploy/SparkSubmit.scala#L843 
\* Have a config option (like spark.jars.ivy.useDefaultRemoteRepos, default true) which when false wont add the maven central & bintry to the ChainResolver. 

Happy to do a PR for this fix.


---

* [SPARK-8473](https://issues.apache.org/jira/browse/SPARK-8473) | *Minor* | **Documentation for DCT**

Documentation detailing the DCT feature transformer needs to be provided. In particular, we need to:
 - Describe how the implementation used scales the transform such that the representing matrix for the transform is unitary
 - Document how the frequency bins are arranged (e.g. index 0 corresponds to  the DC offset). In MATLAB terminology, the implementation does not perform a fftshift after transforming.


---

* [SPARK-8471](https://issues.apache.org/jira/browse/SPARK-8471) | *Minor* | **Implement Discrete Cosine Transform feature transformer**

Discrete cosine transform (DCT) is an invertible matrix transformation commonly used to analyze signals (e.g. audio, images, video) in the frequency domain. In contrast to the FFT, the DCT maps real vectors to real vectors. The DCT is oftentimes used to provide an alternative feature representation (e.g. spectrogram representations of audio and video) useful for classification and frequency-domain analysis.

Ideally, an implementation of the DCT should allow both forward and inverse transforms. It should also work for any numeric datatype and both 1D and 2D data.


---

* [SPARK-8468](https://issues.apache.org/jira/browse/SPARK-8468) | *Blocker* | **Cross-validation with RegressionEvaluator prefers higher RMSE**

Please correct me if I'm wrong, but RegressionEvaluator seems to implement the evaluate() method backwards. The interface expects higher return values from evaluate() to indicate better models. RegressionEvaluator returns RMSE by default - a value we should try to minimize.


---

* [SPARK-8463](https://issues.apache.org/jira/browse/SPARK-8463) | *Major* | **No suitable driver found for write.jdbc**

I am getting a java.sql.SQLException: No suitable driver found for jdbc:mysql://dbhost/test when using df.write.jdbc.

I do not get this error when reading from the same database. 

This simple script can repeat the problem.
First one must create a database called test with a table called table1 and insert some rows in it. The user test:secret must have read/write permissions.

\*testJDBC.scala:\*
import java.util.Properties
import org.apache.spark.sql.Row
import java.sql.Struct
import org.apache.spark.sql.types.\{StructField, StructType, IntegerType, StringType}
import org.apache.spark.\{SparkConf, SparkContext}
import org.apache.spark.sql.SQLContext

val properties = new Properties()
properties.setProperty("user", "test")
properties.setProperty("password", "secret")
val readTable = sqlContext.read.jdbc("jdbc:mysql://dbhost/test", "table1", properties)

print(readTable.show())

val rows = sc.parallelize(List(Row(1, "write"), Row(2, "me")))
val writeTable = sqlContext.createDataFrame(rows, StructType(List(StructField("id", IntegerType), StructField("name", StringType))))
writeTable.write.jdbc("jdbc:mysql://dbhost/test", "table2", properties)}}

This is run using:
{{spark-shell --conf spark.executor.extraClassPath=/path/to/mysql-connector-java-5.1.35-bin.jar --driver-class-path /path/to/mysql-connector-java-5.1.35-bin.jar --jars /path/to/mysql-connector-java-5.1.35-bin.jar -i:testJDBC.scala}}

The read works fine and will print the rows in the table. The write fails with {{java.sql.SQLException: No suitable driver found for jdbc:mysql://dbhost/test}}. The new table is successfully created but it is empty.

I have tested this on a Mesos cluster with Spark 1.4.0 and the current master branch as of June 18th.

In the executor logs I do see before the error:
INFO Utils: Fetching http://146.203.54.236:50624/jars/mysql-connector-java-5.1.35-bin.jar
INFO Executor: Adding file:/tmp/mesos/slaves/.../mysql-connector-java-5.1.35-bin.jar to class loader

A workaround is to add the mysql-connector-java-5.1.35-bin.jar to the same location on each executor node as defined in spark.executor.extraClassPath.


---

* [SPARK-8462](https://issues.apache.org/jira/browse/SPARK-8462) | *Minor* | **Documentation fixes for Spark SQL**

This fixes various minor documentation issues on the Spark SQL page


---

* [SPARK-8461](https://issues.apache.org/jira/browse/SPARK-8461) | *Blocker* | **ClassNotFoundException when code generation is enabled**

Build Spark without {{-Phive}} to make sure the isolated classloader for Hive support is irrelevant, then run the following Spark shell snippet:
{code}
sqlContext.range(0, 2).select(lit("a") as 'a).coalesce(1).write.mode("overwrite").json("file:///tmp/foo")
{code}
Exception thrown:
{noformat}
15/06/18 15:36:30 ERROR codegen.GenerateMutableProjection: failed to compile:

      import org.apache.spark.sql.catalyst.InternalRow;

      public SpecificProjection generate(org.apache.spark.sql.catalyst.expressions.Expression[] expr) {
        return new SpecificProjection(expr);
      }

      class SpecificProjection extends org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection {

        private org.apache.spark.sql.catalyst.expressions.Expression[] expressions = null;
        private org.apache.spark.sql.catalyst.expressions.MutableRow mutableRow = null;

        public SpecificProjection(org.apache.spark.sql.catalyst.expressions.Expression[] expr) {
          expressions = expr;
          mutableRow = new org.apache.spark.sql.catalyst.expressions.GenericMutableRow(1);
        }

        public org.apache.spark.sql.catalyst.expressions.codegen.BaseMutableProjection target(org.apache.spark.sql.catalyst.expressions.MutableRow row) {
          mutableRow = row;
          return this;
        }

        /\* Provide immutable access to the last projected row. \*/
        public InternalRow currentValue() {
          return (InternalRow) mutableRow;
        }

        public Object apply(Object \_i) {
          InternalRow i = (InternalRow) \_i;

      /\* expression: a \*/
      Object obj2 = expressions[0].eval(i);
      boolean isNull0 = obj2 == null;
      org.apache.spark.unsafe.types.UTF8String primitive1 = null;
      if (!isNull0) {
        primitive1 = (org.apache.spark.unsafe.types.UTF8String) obj2;
      }

          if(isNull0)
            mutableRow.setNullAt(0);
          else
            mutableRow.update(0, primitive1);


          return mutableRow;
        }
      }

org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6897)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
        at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188)
        at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
        at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880)
        at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
        at org.codehaus.janino.UnitCompiler.access$16700(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$31.getParameterTypes2(UnitCompiler.java:8533)
        at org.codehaus.janino.IClass$IInvocable.getParameterTypes(IClass.java:835)
        at org.codehaus.janino.IClass$IMethod.getDescriptor2(IClass.java:1063)
        at org.codehaus.janino.IClass$IInvocable.getDescriptor(IClass.java:849)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:211)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:199)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:409)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
        at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
        at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
        at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
        at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
        at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
        at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
        at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
        at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
        at org.codehaus.janino.ClassBodyEvaluator.\<init\>(ClassBodyEvaluator.java:72)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:245)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:87)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:29)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:272)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
        at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:285)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:282)
        at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:173)
        at org.apache.spark.sql.execution.Project.buildProjection$lzycompute(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project.buildProjection(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:42)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:41)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:93)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:92)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.hasNext(DataFrame.scala:1471)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1285)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassNotFoundException: Object
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:79)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:344)
        at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
        at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254)
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
        ... 80 more
Caused by: java.lang.ClassNotFoundException: Object
        at java.lang.ClassLoader.findClass(ClassLoader.java:530)
        at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:74)
        ... 87 more
15/06/18 15:36:30 ERROR executor.Executor: Exception in task 0.0 in stage 4.0 (TID 18)
java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
        at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
        at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
        at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
        at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:285)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:282)
        at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:173)
        at org.apache.spark.sql.execution.Project.buildProjection$lzycompute(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project.buildProjection(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:42)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:41)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:93)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:92)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.hasNext(DataFrame.scala:1471)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1285)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6897)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
        at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188)
        at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
        at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880)
        at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
        at org.codehaus.janino.UnitCompiler.access$16700(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$31.getParameterTypes2(UnitCompiler.java:8533)
        at org.codehaus.janino.IClass$IInvocable.getParameterTypes(IClass.java:835)
        at org.codehaus.janino.IClass$IMethod.getDescriptor2(IClass.java:1063)
        at org.codehaus.janino.IClass$IInvocable.getDescriptor(IClass.java:849)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:211)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:199)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:409)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
        at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
        at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
        at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
        at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
        at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
        at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
        at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
        at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
        at org.codehaus.janino.ClassBodyEvaluator.\<init\>(ClassBodyEvaluator.java:72)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:245)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:87)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:29)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:272)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
        ... 38 more
Caused by: java.lang.ClassNotFoundException: Object
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:79)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:344)
        at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
        at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254)
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
        ... 80 more
Caused by: java.lang.ClassNotFoundException: Object
        at java.lang.ClassLoader.findClass(ClassLoader.java:530)
        at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:74)
        ... 87 more
15/06/18 15:36:30 WARN scheduler.TaskSetManager: Lost task 0.0 in stage 4.0 (TID 18, localhost): java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
        at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
        at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
        at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
        at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:285)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:282)
        at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:173)
        at org.apache.spark.sql.execution.Project.buildProjection$lzycompute(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project.buildProjection(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:42)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:41)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:93)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:92)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.hasNext(DataFrame.scala:1471)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1285)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6897)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
        at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188)
        at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
        at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880)
        at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
        at org.codehaus.janino.UnitCompiler.access$16700(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$31.getParameterTypes2(UnitCompiler.java:8533)
        at org.codehaus.janino.IClass$IInvocable.getParameterTypes(IClass.java:835)
        at org.codehaus.janino.IClass$IMethod.getDescriptor2(IClass.java:1063)
        at org.codehaus.janino.IClass$IInvocable.getDescriptor(IClass.java:849)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:211)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:199)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:409)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
        at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
        at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
        at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
        at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
        at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
        at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
        at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
        at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
        at org.codehaus.janino.ClassBodyEvaluator.\<init\>(ClassBodyEvaluator.java:72)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:245)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:87)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:29)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:272)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
        ... 38 more
Caused by: java.lang.ClassNotFoundException: Object
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:79)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:344)
        at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
        at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254)
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
        ... 80 more
Caused by: java.lang.ClassNotFoundException: Object
        at java.lang.ClassLoader.findClass(ClassLoader.java:530)
        at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:74)
        ... 87 more

15/06/18 15:36:30 ERROR scheduler.TaskSetManager: Task 0 in stage 4.0 failed 1 times; aborting job
15/06/18 15:36:30 INFO scheduler.TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
15/06/18 15:36:30 INFO scheduler.TaskSchedulerImpl: Cancelling stage 4
15/06/18 15:36:30 INFO scheduler.DAGScheduler: ResultStage 4 (json at \<console\>:23) failed in 0.054 s
15/06/18 15:36:30 INFO scheduler.DAGScheduler: Job 4 failed: json at \<console\>:23, took 0.059715 s
org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 4.0 failed 1 times, most recent failure: Lost task 0.0 in stage 4.0 (TID 18, localhost): java.util.concurrent.ExecutionException: org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at com.google.common.util.concurrent.AbstractFuture$Sync.getValue(AbstractFuture.java:306)
        at com.google.common.util.concurrent.AbstractFuture$Sync.get(AbstractFuture.java:293)
        at com.google.common.util.concurrent.AbstractFuture.get(AbstractFuture.java:116)
        at com.google.common.util.concurrent.Uninterruptibles.getUninterruptibly(Uninterruptibles.java:135)
        at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2380)
        at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
        at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
        at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
        at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
        at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:285)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:282)
        at org.apache.spark.sql.execution.SparkPlan.newMutableProjection(SparkPlan.scala:173)
        at org.apache.spark.sql.execution.Project.buildProjection$lzycompute(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project.buildProjection(basicOperators.scala:39)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:42)
        at org.apache.spark.sql.execution.Project$$anonfun$1.apply(basicOperators.scala:41)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:93)
        at org.apache.spark.rdd.CoalescedRDD$$anonfun$compute$1.apply(CoalescedRDD.scala:92)
        at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.sql.DataFrame$$anonfun$toJSON$1$$anon$1.hasNext(DataFrame.scala:1471)
        at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply$mcV$sp(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13$$anonfun$apply$6.apply(PairRDDFunctions.scala:1108)
        at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1285)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1116)
        at org.apache.spark.rdd.PairRDDFunctions$$anonfun$saveAsHadoopDataset$1$$anonfun$13.apply(PairRDDFunctions.scala:1095)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
Caused by: org.codehaus.commons.compiler.CompileException: Line 28, Column 35: Object
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6897)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5331)
        at org.codehaus.janino.UnitCompiler.getReferenceType(UnitCompiler.java:5207)
        at org.codehaus.janino.UnitCompiler.getType2(UnitCompiler.java:5188)
        at org.codehaus.janino.UnitCompiler.access$12600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$16.visitReferenceType(UnitCompiler.java:5119)
        at org.codehaus.janino.Java$ReferenceType.accept(Java.java:2880)
        at org.codehaus.janino.UnitCompiler.getType(UnitCompiler.java:5159)
        at org.codehaus.janino.UnitCompiler.access$16700(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$31.getParameterTypes2(UnitCompiler.java:8533)
        at org.codehaus.janino.IClass$IInvocable.getParameterTypes(IClass.java:835)
        at org.codehaus.janino.IClass$IMethod.getDescriptor2(IClass.java:1063)
        at org.codehaus.janino.IClass$IInvocable.getDescriptor(IClass.java:849)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:211)
        at org.codehaus.janino.IClass.getIMethods(IClass.java:199)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:409)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
        at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
        at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
        at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
        at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
        at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
        at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
        at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
        at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
        at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
        at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
        at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
        at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
        at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
        at org.codehaus.janino.ClassBodyEvaluator.\<init\>(ClassBodyEvaluator.java:72)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:245)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:87)
        at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:29)
        at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:272)
        at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
        at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
        ... 38 more
Caused by: java.lang.ClassNotFoundException: Object
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:79)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:344)
        at org.codehaus.janino.ClassLoaderIClassLoader.findIClass(ClassLoaderIClassLoader.java:78)
        at org.codehaus.janino.IClassLoader.loadIClass(IClassLoader.java:254)
        at org.codehaus.janino.UnitCompiler.findTypeByName(UnitCompiler.java:6893)
        ... 80 more
Caused by: java.lang.ClassNotFoundException: Object
        at java.lang.ClassLoader.findClass(ClassLoader.java:530)
        at org.apache.spark.util.ParentClassLoader.findClass(ParentClassLoader.scala:26)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:424)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:34)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:357)
        at org.apache.spark.util.ParentClassLoader.loadClass(ParentClassLoader.scala:30)
        at org.apache.spark.repl.ExecutorClassLoader.findClass(ExecutorClassLoader.scala:74)
        ... 87 more

Driver stacktrace:
        at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1285)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1276)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1275)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
        at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1275)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:749)
        at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:749)
        at scala.Option.foreach(Option.scala:236)
        at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:749)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1484)
        at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1445)
        at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{noformat}


---

* [SPARK-8457](https://issues.apache.org/jira/browse/SPARK-8457) | *Trivial* | **Documentation for N-Gram feature transformer**

Documentation for using n-gram feature transformer needs to be written. In particular, we need to describe the input (ArrayType(StringType)) and output (ArrayType(ArrayType(StringType)) formats, nullability constraints (input is nullable, output is not) and behaviors (null values are ignored in generating n-grams), as well as edge cases (e.g. n-gram length \> input sequence length will  yield a single output n-gram equal to the input sequence).


---

* [SPARK-8456](https://issues.apache.org/jira/browse/SPARK-8456) | *Trivial* | **Python API for N-Gram Feature Transformer**

Write Python API for N-Gram Featurizer


---

* [SPARK-8455](https://issues.apache.org/jira/browse/SPARK-8455) | *Minor* | **Implement N-Gram Feature Transformer**

N-grams are a NLP feature representation which generalize bag of words to include local context (the n-1 preceding words). We can implement N-grams in ML as a feature transformer (likely directly after tokenization).

For example, "this is a test" should tokenize to ["this","is","a","test"], which upon applying a 2-gram feature transform should yield [["this","is"],["is","a"],["a","test"]].


---

* [SPARK-8452](https://issues.apache.org/jira/browse/SPARK-8452) | *Major* | **expose jobGroup API in SparkR**

Following job management calls are missing in SparkR:
{code}
setJobGroup()
cancelJobGroup()
clearJobGroup()
{code}


---

* [SPARK-8451](https://issues.apache.org/jira/browse/SPARK-8451) | *Major* | **SparkSubmitSuite never checks for process exit code**

We just never did. If the subprocess throws an exception we just ignore it.


---

* [SPARK-8450](https://issues.apache.org/jira/browse/SPARK-8450) | *Major* | **PySpark write.parquet raises Unsupported datatype DecimalType()**

I'm getting an Exception when I try to save a DataFrame with a DeciamlType as an parquet file

Minimal Example:
{code}
from decimal import Decimal
from pyspark.sql import SQLContext
from pyspark.sql.types import \*

sqlContext = SQLContext(sc)
schema = StructType([
    StructField('id', LongType()),
    StructField('value', DecimalType())])
rdd = sc.parallelize([[1, Decimal("0.5")],[2, Decimal("2.9")]])
df = sqlContext.createDataFrame(rdd, schema)
df.write.parquet("hdfs://srv:9000/user/ph/decimal.parquet", 'overwrite')

{code}

Stack Trace
{code}
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
\<ipython-input-19-a77dac8de5f3\> in \<module\>()
----\> 1 sr.write.parquet("hdfs://srv:9000/user/ph/decimal.parquet", 'overwrite')

/home/spark/spark-1.4.0-bin-hadoop2.6/python/pyspark/sql/readwriter.pyc in parquet(self, path, mode)
    367         :param mode: one of `append`, `overwrite`, `error`, `ignore` (default: error)
    368         """
--\> 369         return self.\_jwrite.mode(mode).parquet(path)
    370 
    371     @since(1.4)

/home/spark/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java\_gateway.py in \_\_call\_\_(self, \*args)
    536         answer = self.gateway\_client.send\_command(command)
    537         return\_value = get\_return\_value(answer, self.gateway\_client,
--\> 538                 self.target\_id, self.name)
    539 
    540         for temp\_arg in temp\_args:

/home/spark/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get\_return\_value(answer, gateway\_client, target\_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--\> 300                     format(target\_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o361.parquet.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.insert(commands.scala:138)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.run(commands.scala:114)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:68)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:87)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:939)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:939)
	at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:332)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:144)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:135)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:281)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 158 in stage 35.0 failed 4 times, most recent failure: Lost task 158.3 in stage 35.0 (TID 2736, 10.2.160.14): java.lang.RuntimeException: Unsupported datatype DecimalType()
	at scala.sys.package$.error(package.scala:27)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$fromDataType$2.apply(ParquetTypes.scala:374)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$fromDataType$2.apply(ParquetTypes.scala:318)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$.fromDataType(ParquetTypes.scala:317)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$4.apply(ParquetTypes.scala:398)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$4.apply(ParquetTypes.scala:397)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:34)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.parquet.ParquetTypesConverter$.convertFromAttributes(ParquetTypes.scala:396)
	at org.apache.spark.sql.parquet.RowWriteSupport.init(ParquetTableSupport.scala:150)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:278)
	at parquet.hadoop.ParquetOutputFormat.getRecordWriter(ParquetOutputFormat.java:252)
	at org.apache.spark.sql.parquet.ParquetOutputWriter.\<init\>(newParquet.scala:111)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anon$4.newInstance(newParquet.scala:244)
	at org.apache.spark.sql.sources.DefaultWriterContainer.initWriters(commands.scala:386)
	at org.apache.spark.sql.sources.BaseWriterContainer.executorSideSetup(commands.scala:298)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.org$apache$spark$sql$sources$InsertIntoHadoopFsRelation$$writeRows$1(commands.scala:142)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:132)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:132)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}

I also tried to set the precision \< 18
{code}
schema = StructType([
    StructField('id', LongType()),
    StructField('value', DecimalType(16,2))])
{code}

which raises a different exception
{code}
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
\<ipython-input-23-bba70b7c0805\> in \<module\>()
----\> 1 df.write.parquet("hdfs://srv:9000/user/ph/decimal.parquet", 'overwrite')

/home/spark/spark-1.4.0-bin-hadoop2.6/python/pyspark/sql/readwriter.pyc in parquet(self, path, mode)
    367         :param mode: one of `append`, `overwrite`, `error`, `ignore` (default: error)
    368         """
--\> 369         return self.\_jwrite.mode(mode).parquet(path)
    370 
    371     @since(1.4)

/home/spark/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java\_gateway.py in \_\_call\_\_(self, \*args)
    536         answer = self.gateway\_client.send\_command(command)
    537         return\_value = get\_return\_value(answer, self.gateway\_client,
--\> 538                 self.target\_id, self.name)
    539 
    540         for temp\_arg in temp\_args:

/home/spark/spark-1.4.0-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py in get\_return\_value(answer, gateway\_client, target\_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--\> 300                     format(target\_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o417.parquet.
: org.apache.spark.SparkException: Job aborted.
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.insert(commands.scala:138)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.run(commands.scala:114)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:68)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:87)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:939)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:939)
	at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:332)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:144)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:135)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:281)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 159 in stage 41.0 failed 4 times, most recent failure: Lost task 159.3 in stage 41.0 (TID 3211, 10.2.160.14): org.apache.spark.SparkException: Task failed while writing rows.
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.org$apache$spark$sql$sources$InsertIntoHadoopFsRelation$$writeRows$1(commands.scala:161)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:132)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:132)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
Caused by: java.lang.ClassCastException: java.math.BigDecimal cannot be cast to org.apache.spark.sql.types.Decimal
	at org.apache.spark.sql.parquet.MutableRowWriteSupport.consumeType(ParquetTableSupport.scala:365)
	at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:335)
	at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:321)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
	at org.apache.spark.sql.parquet.ParquetOutputWriter.write(newParquet.scala:114)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.org$apache$spark$sql$sources$InsertIntoHadoopFsRelation$$writeRows$1(commands.scala:154)
	... 8 more

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1266)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1257)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1256)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1256)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:730)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:730)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1450)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1411)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

{code}
The corresponding Scala Version works

{code}
import org.apache.spark.SparkContext
import org.apache.spark.sql.{ Row, SQLContext }
import org.apache.spark.sql.types.{ DecimalType, IntegerType, StructType, StructField }
 
object ParquetDecimal {
  def main(args: Array[String]) {
    // Connect to Spark
    val sc = new SparkContext()
    val sqlContext = new SQLContext(sc)
 
    val schema = StructType(Seq(StructField("id", IntegerType), StructField("value", DecimalType(16, 2))))
    val rows = sc.parallelize(Seq(Row(1, BigDecimal("0.9")), Row(2, BigDecimal("2.9"))))
    val df = sqlContext.createDataFrame(rows, schema)
    df.write.parquet("test.parquet")
  }
}
{code}


---

* [SPARK-8446](https://issues.apache.org/jira/browse/SPARK-8446) | *Major* | **Add helper functions for testing physical SparkPlan operators**

SparkSQL has a nice {{QueryTest}} class for writing tests that run queries; I think we should add an analogous test utility for directly unit testing the physical SparkPlan operators.


---

* [SPARK-8445](https://issues.apache.org/jira/browse/SPARK-8445) | *Critical* | **MLlib 1.5 Roadmap**

We expect to see many MLlib contributors for the 1.5 release. To scale out the development, we created this master list for MLlib features we plan to have in Spark 1.5. Please view this list as a wish list rather than a concrete plan, because we don't have an accurate estimate of available resources. Due to limited review bandwidth, features appearing on this list will get higher priority during code review. But feel free to suggest new items to the list in comments. We are experimenting with this process. Your feedback would be greatly appreciated.

h1. Instructions

h2. For contributors:

\* Please read https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark carefully. Code style, documentation, and unit tests are important.
\* If you are a first-time Spark contributor, please always start with a [starter task\|https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20component%20in%20(ML%2C%20MLlib)%20AND%20labels%20%3D%20starter%20AND%20%22Target%20Version%2Fs%22%20%3D%201.5.0] rather than a medium/big feature. Based on our experience, mixing the development process with a big feature usually causes long delay in code review.
\* Never work silently. Let everyone know on the corresponding JIRA page when you start working on some features. This is to avoid duplicate work. For small features, you don't need to wait to get JIRA assigned.
\* For medium/big features or features with dependencies, please get assigned first before coding and keep the ETA updated on the JIRA. If there exist no activity on the JIRA page for a certain amount of time, the JIRA should be released for other contributors.
\* Do not claim multiple (\>3) JIRAs at the same time. Try to finish them one after another.
\* Please review others' PRs (https://spark-prs.appspot.com/#mllib). Code review greatly helps improve others' code as well as yours.

h2. For committers:

\* Try to break down big features into small and specific JIRA tasks and link them properly.
\* Add "starter" label to starter tasks.
\* Put a rough estimate for medium/big features and track the progress.
\* If you start reviewing a PR, please add yourself to the Shepherd field on JIRA.
\* If the code looks good to you, please comment "LGTM". For non-trivial PRs, please ping a maintainer to make a final pass.
\* After merging a PR, create and link JIRAs for Python, example code, and documentation if necessary.

h1. Roadmap (WIP)

This is NOT [a complete list of MLlib JIRAs for 1.5\|https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20component%20in%20(ML%2C%20MLlib)%20AND%20%22Target%20Version%2Fs%22%20%3D%201.5.0%20ORDER%20BY%20priority%20DESC]. We only include umbrella JIRAs and high-level tasks.

h2. Algorithms and performance

\* LDA improvements (SPARK-5572)
\* Log-linear model for survival analysis (SPARK-8518) -\> 1.6
\* Improve GLM's scalability on number of features (SPARK-8520)
\* Tree and ensembles: Move + cleanup code (SPARK-7131), provide class probabilities (SPARK-3727), feature importance (SPARK-5133)
\* Improve GMM scalability and stability (SPARK-5016)
\* Frequent pattern mining improvements (SPARK-6487)
\* R-like stats for ML models (SPARK-7674)
\* Generalize classification threshold to multiclass (SPARK-8069)
\* A/B testing (SPARK-3147)

h2. Pipeline API

\* more feature transformers (SPARK-8521)
\* k-means (SPARK-7879)
\* naive Bayes (SPARK-8600)
\* TrainValidationSplit for tuning (SPARK-8484)
\* Isotonic regression (SPARK-8671)

h2. Model persistence

\* more PMML export (SPARK-8545)
\* model save/load (SPARK-4587)
\* pipeline persistence (SPARK-6725)

h2. Python API for ML

\* List of issues identified during Spark 1.4 QA: (SPARK-7536)
\* Python API for streaming ML algorithms (SPARK-3258)
\* Add missing model methods (SPARK-8633)

h2. SparkR API for ML

\* MLlib + SparkR integration for 1.5 (RFormula + glm) (SPARK-6805)
\* model.matrix for DataFrames (SPARK-6823)

h2. Documentation

\* [Search for documentation improvements \| https://issues.apache.org/jira/issues/?jql=project%20%3D%20SPARK%20AND%20status%20in%20(Open%2C%20%22In%20Progress%22%2C%20Reopened)%20AND%20component%20in%20(Documentation)%20AND%20component%20in%20(ML%2C%20MLlib)]


---

* [SPARK-8444](https://issues.apache.org/jira/browse/SPARK-8444) | *Minor* | **Add Python example in streaming for queueStream usage**

I noticed there was no Python equivalent for Scala queueStream example.  This will have to be slightly different because changes in the Queue after the stream is created are not recognized.


---

* [SPARK-8443](https://issues.apache.org/jira/browse/SPARK-8443) | *Major* | **GenerateMutableProjection Exceeds JVM Code Size Limits**

GenerateMutableProjection put all expressions columns into a single apply function. When there are a lot of columns, the apply function code size exceeds the 64kb limit, which is a hard limit on jvm that cannot change.

This comes up when we were aggregating about 100 columns using codegen and unsafe feature.

I wrote an unit test that reproduces this issue. 
https://github.com/saurfang/spark/blob/codegen\_size\_limit/sql/catalyst/src/test/scala/org/apache/spark/sql/catalyst/expressions/CodeGenerationSuite.scala

This test currently fails at 2048 expressions. It seems the master is more tolerant than branch-1.4 about this because code is more concise.

While the code on master has changed since branch-1.4, I am able to reproduce the problem in master. For now I hacked my way in branch-1.4 to workaround this problem by wrapping each expression with a separate function then call those functions sequentially in apply. The proper way is probably check the length of the projectCode and break it up as necessary. (This seems to be easier in master actually since we are building code by string rather than quasiquote)

Let me know if anyone has additional thoughts on this, I'm happy to contribute a pull request.

Attaching stack trace produced by unit test
{code}
[info] - code size limit \*\*\* FAILED \*\*\* (7 seconds, 103 milliseconds)
[info]   com.google.common.util.concurrent.UncheckedExecutionException: org.codehaus.janino.JaninoRuntimeException: Code of method "(Ljava/lang/Object;)Ljava/lang/Object;" of class "SC$SpecificProjection" grows beyond 64 KB
[info]   at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2263)
[info]   at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
[info]   at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
[info]   at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
[info]   at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:285)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2$$anonfun$apply$mcV$sp$2.apply(CodeGenerationSuite.scala:50)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2$$anonfun$apply$mcV$sp$2.apply(CodeGenerationSuite.scala:48)
[info]   at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:144)
[info]   at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:144)
[info]   at scala.collection.immutable.Range.foreach(Range.scala:141)
[info]   at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:144)
[info]   at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:105)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2.apply$mcV$sp(CodeGenerationSuite.scala:47)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2.apply(CodeGenerationSuite.scala:47)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2.apply(CodeGenerationSuite.scala:47)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuite.run(FunSuite.scala:1555)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[info]   at java.lang.Thread.run(Thread.java:745)
[info]   Cause: org.codehaus.janino.JaninoRuntimeException: Code of method "(Ljava/lang/Object;)Ljava/lang/Object;" of class "SC$SpecificProjection" grows beyond 64 KB
[info]   at org.codehaus.janino.CodeContext.makeSpace(CodeContext.java:941)
[info]   at org.codehaus.janino.CodeContext.write(CodeContext.java:874)
[info]   at org.codehaus.janino.CodeContext.writeBranch(CodeContext.java:965)
[info]   at org.codehaus.janino.UnitCompiler.writeBranch(UnitCompiler.java:10261)
[info]   at org.codehaus.janino.UnitCompiler.compileBoolean2(UnitCompiler.java:2862)
[info]   at org.codehaus.janino.UnitCompiler.access$4800(UnitCompiler.java:185)
[info]   at org.codehaus.janino.UnitCompiler$8.visitAmbiguousName(UnitCompiler.java:2832)
[info]   at org.codehaus.janino.Java$AmbiguousName.accept(Java.java:3138)
[info]   at org.codehaus.janino.UnitCompiler.compileBoolean(UnitCompiler.java:2842)
[info]   at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:1741)
[info]   at org.codehaus.janino.UnitCompiler.access$1200(UnitCompiler.java:185)
[info]   at org.codehaus.janino.UnitCompiler$4.visitIfStatement(UnitCompiler.java:937)
[info]   at org.codehaus.janino.Java$IfStatement.accept(Java.java:2157)
[info]   at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:958)
[info]   at org.codehaus.janino.UnitCompiler.compileStatements(UnitCompiler.java:1007)
[info]   at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:2293)
[info]   at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:822)
[info]   at org.codehaus.janino.UnitCompiler.compileDeclaredMethods(UnitCompiler.java:794)
[info]   at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:507)
[info]   at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:658)
[info]   at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:662)
[info]   at org.codehaus.janino.UnitCompiler.access$600(UnitCompiler.java:185)
[info]   at org.codehaus.janino.UnitCompiler$2.visitMemberClassDeclaration(UnitCompiler.java:350)
[info]   at org.codehaus.janino.Java$MemberClassDeclaration.accept(Java.java:1035)
[info]   at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
[info]   at org.codehaus.janino.UnitCompiler.compileDeclaredMemberTypes(UnitCompiler.java:769)
[info]   at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:532)
[info]   at org.codehaus.janino.UnitCompiler.compile2(UnitCompiler.java:393)
[info]   at org.codehaus.janino.UnitCompiler.access$400(UnitCompiler.java:185)
[info]   at org.codehaus.janino.UnitCompiler$2.visitPackageMemberClassDeclaration(UnitCompiler.java:347)
[info]   at org.codehaus.janino.Java$PackageMemberClassDeclaration.accept(Java.java:1139)
[info]   at org.codehaus.janino.UnitCompiler.compile(UnitCompiler.java:354)
[info]   at org.codehaus.janino.UnitCompiler.compileUnit(UnitCompiler.java:322)
[info]   at org.codehaus.janino.SimpleCompiler.compileToClassLoader(SimpleCompiler.java:383)
[info]   at org.codehaus.janino.ClassBodyEvaluator.compileToClass(ClassBodyEvaluator.java:315)
[info]   at org.codehaus.janino.ClassBodyEvaluator.cook(ClassBodyEvaluator.java:233)
[info]   at org.codehaus.janino.SimpleCompiler.cook(SimpleCompiler.java:192)
[info]   at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:84)
[info]   at org.codehaus.commons.compiler.Cookable.cook(Cookable.java:77)
[info]   at org.codehaus.janino.ClassBodyEvaluator.\<init\>(ClassBodyEvaluator.java:72)
[info]   at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.compile(CodeGenerator.scala:245)
[info]   at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:87)
[info]   at org.apache.spark.sql.catalyst.expressions.codegen.GenerateMutableProjection$.create(GenerateMutableProjection.scala:29)
[info]   at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator$$anon$1.load(CodeGenerator.scala:272)
[info]   at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3599)
[info]   at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2379)
[info]   at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2342)
[info]   at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2257)
[info]   at com.google.common.cache.LocalCache.get(LocalCache.java:4000)
[info]   at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:4004)
[info]   at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4874)
[info]   at org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator.generate(CodeGenerator.scala:285)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2$$anonfun$apply$mcV$sp$2.apply(CodeGenerationSuite.scala:50)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2$$anonfun$apply$mcV$sp$2.apply(CodeGenerationSuite.scala:48)
[info]   at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:144)
[info]   at scala.collection.TraversableOnce$$anonfun$foldLeft$1.apply(TraversableOnce.scala:144)
[info]   at scala.collection.immutable.Range.foreach(Range.scala:141)
[info]   at scala.collection.TraversableOnce$class.foldLeft(TraversableOnce.scala:144)
[info]   at scala.collection.AbstractTraversable.foldLeft(Traversable.scala:105)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2.apply$mcV$sp(CodeGenerationSuite.scala:47)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2.apply(CodeGenerationSuite.scala:47)
[info]   at org.apache.spark.sql.catalyst.expressions.CodeGenerationSuite$$anonfun$2.apply(CodeGenerationSuite.scala:47)
[info]   at org.scalatest.Transformer$$anonfun$apply$1.apply$mcV$sp(Transformer.scala:22)
[info]   at org.scalatest.OutcomeOf$class.outcomeOf(OutcomeOf.scala:85)
[info]   at org.scalatest.OutcomeOf$.outcomeOf(OutcomeOf.scala:104)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:22)
[info]   at org.scalatest.Transformer.apply(Transformer.scala:20)
[info]   at org.scalatest.FunSuiteLike$$anon$1.apply(FunSuiteLike.scala:166)
[info]   at org.apache.spark.SparkFunSuite.withFixture(SparkFunSuite.scala:42)
[info]   at org.scalatest.FunSuiteLike$class.invokeWithFixture$1(FunSuiteLike.scala:163)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTest$1.apply(FunSuiteLike.scala:175)
[info]   at org.scalatest.SuperEngine.runTestImpl(Engine.scala:306)
[info]   at org.scalatest.FunSuiteLike$class.runTest(FunSuiteLike.scala:175)
[info]   at org.scalatest.FunSuite.runTest(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuiteLike$$anonfun$runTests$1.apply(FunSuiteLike.scala:208)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:413)
[info]   at org.scalatest.SuperEngine$$anonfun$traverseSubNodes$1$1.apply(Engine.scala:401)
[info]   at scala.collection.immutable.List.foreach(List.scala:318)
[info]   at org.scalatest.SuperEngine.traverseSubNodes$1(Engine.scala:401)
[info]   at org.scalatest.SuperEngine.org$scalatest$SuperEngine$$runTestsInBranch(Engine.scala:396)
[info]   at org.scalatest.SuperEngine.runTestsImpl(Engine.scala:483)
[info]   at org.scalatest.FunSuiteLike$class.runTests(FunSuiteLike.scala:208)
[info]   at org.scalatest.FunSuite.runTests(FunSuite.scala:1555)
[info]   at org.scalatest.Suite$class.run(Suite.scala:1424)
[info]   at org.scalatest.FunSuite.org$scalatest$FunSuiteLike$$super$run(FunSuite.scala:1555)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuiteLike$$anonfun$run$1.apply(FunSuiteLike.scala:212)
[info]   at org.scalatest.SuperEngine.runImpl(Engine.scala:545)
[info]   at org.scalatest.FunSuiteLike$class.run(FunSuiteLike.scala:212)
[info]   at org.scalatest.FunSuite.run(FunSuite.scala:1555)
[info]   at org.scalatest.tools.Framework.org$scalatest$tools$Framework$$runSuite(Framework.scala:462)
[info]   at org.scalatest.tools.Framework$ScalaTestTask.execute(Framework.scala:671)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:294)
[info]   at sbt.ForkMain$Run$2.call(ForkMain.java:284)
[info]   at java.util.concurrent.FutureTask.run(FutureTask.java:266)
[info]   at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
[info]   at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
[info]   at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-8437](https://issues.apache.org/jira/browse/SPARK-8437) | *Minor* | **Using directory path without wildcard for filename slow for large number of files with wholeTextFiles and binaryFiles**

When calling wholeTextFiles or binaryFiles with a directory path with 10,000s of files in it, Spark hangs for a few minutes before processing the files.

If you add a \* to the end of the path, there is no delay.

This happens for me on Spark 1.3.1 and 1.4 on the local filesystem, HDFS, and on S3.

To reproduce, create a directory with 50,000 files in it, then run:


val a = sc.binaryFiles("file:/path/to/files/")
a.count()

val b = sc.binaryFiles("file:/path/to/files/\*")
b.count()

and monitor the different startup times.

For example, in the spark-shell these commands are pasted in together, so the delay at f.count() is from 10:11:08 t- 10:13:29 to output "Total input paths to process : 49999", then until 10:15:42 to being processing files:

scala\> val f = sc.binaryFiles("file:/home/ewan/large/")
15/06/18 10:11:07 INFO MemoryStore: ensureFreeSpace(160616) called with curMem=0, maxMem=278019440
15/06/18 10:11:07 INFO MemoryStore: Block broadcast\_0 stored as values in memory (estimated size 156.9 KB, free 265.0 MB)
15/06/18 10:11:08 INFO MemoryStore: ensureFreeSpace(17282) called with curMem=160616, maxMem=278019440
15/06/18 10:11:08 INFO MemoryStore: Block broadcast\_0\_piece0 stored as bytes in memory (estimated size 16.9 KB, free 265.0 MB)
15/06/18 10:11:08 INFO BlockManagerInfo: Added broadcast\_0\_piece0 in memory on localhost:40430 (size: 16.9 KB, free: 265.1 MB)
15/06/18 10:11:08 INFO SparkContext: Created broadcast 0 from binaryFiles at \<console\>:21
f: org.apache.spark.rdd.RDD[(String, org.apache.spark.input.PortableDataStream)] = file:/home/ewan/large/ BinaryFileRDD[0] at binaryFiles at \<console\>:21

scala\> f.count()
15/06/18 10:13:29 INFO FileInputFormat: Total input paths to process : 49999
15/06/18 10:15:42 INFO FileInputFormat: Total input paths to process : 49999
15/06/18 10:15:42 INFO CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
15/06/18 10:15:42 INFO SparkContext: Starting job: count at \<console\>:24
15/06/18 10:15:42 INFO DAGScheduler: Got job 0 (count at \<console\>:24) with 49999 output partitions (allowLocal=false)
15/06/18 10:15:42 INFO DAGScheduler: Final stage: ResultStage 0(count at \<console\>:24)
15/06/18 10:15:42 INFO DAGScheduler: Parents of final stage: List()

Adding a \* to the end of the path removes the delay:


scala\> val f = sc.binaryFiles("file:/home/ewan/large/\*")
15/06/18 10:08:29 INFO MemoryStore: ensureFreeSpace(160616) called with curMem=0, maxMem=278019440
15/06/18 10:08:29 INFO MemoryStore: Block broadcast\_0 stored as values in memory (estimated size 156.9 KB, free 265.0 MB)
15/06/18 10:08:29 INFO MemoryStore: ensureFreeSpace(17309) called with curMem=160616, maxMem=278019440
15/06/18 10:08:29 INFO MemoryStore: Block broadcast\_0\_piece0 stored as bytes in memory (estimated size 16.9 KB, free 265.0 MB)
15/06/18 10:08:29 INFO BlockManagerInfo: Added broadcast\_0\_piece0 in memory on localhost:42825 (size: 16.9 KB, free: 265.1 MB)
15/06/18 10:08:29 INFO SparkContext: Created broadcast 0 from binaryFiles at \<console\>:21
f: org.apache.spark.rdd.RDD[(String, org.apache.spark.input.PortableDataStream)] = file:/home/ewan/large/\* BinaryFileRDD[0] at binaryFiles at \<console\>:21

scala\> f.count()
15/06/18 10:08:32 INFO FileInputFormat: Total input paths to process : 49999
15/06/18 10:08:33 INFO FileInputFormat: Total input paths to process : 49999
15/06/18 10:08:35 INFO CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0
15/06/18 10:08:35 INFO SparkContext: Starting job: count at \<console\>:24
15/06/18 10:08:35 INFO DAGScheduler: Got job 0 (count at \<console\>:24) with 49999 output partitions


---

* [SPARK-8435](https://issues.apache.org/jira/browse/SPARK-8435) | *Major* | **Cannot create tables in an specific database using a provider**

Hello,

I've been trying to create tables in different catalogs using a Hive metastore and when I execute the "CREATE" statement, I realized that it is created into the default catalog.

This is what I'm trying. 
{quote}
scala\> sqlContext.sql("CREATE DATABASE IF NOT EXISTS testmetastore COMMENT 'Testing catalogs' ")
scala\> sqlContext.sql("USE testmetastore")
scala\> sqlContext.sql("CREATE TABLE students USING org.apache.spark.sql.parquet OPTIONS (path '/user/hive, highavailability 'true', DefaultLimit '1000')")
{quote}

And this is what I get. I can see that it is kind of working because it seems that when it checks if the table exists, it searchs in the correct catalog (testmetastore). But finally when it tries to create the table, it uses the default catalog.

{quote}
scala\> sqlContext.sql("CREATE TABLE students USING a OPTIONS (highavailability 'true', DefaultLimit '1000')").show
15/06/18 10:28:48 INFO HiveMetaStore: 0: get\_table : db=\*testmetastore\* tbl=students
15/06/18 10:28:48 INFO audit: ugi=ccaballero	ip=unknown-ip-addr	cmd=get\_table : db=testmetastore tbl=students	
15/06/18 10:28:48 INFO Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
15/06/18 10:28:48 INFO Persistence: Request to load fields "comment,name,type" of class org.apache.hadoop.hive.metastore.model.MFieldSchema but object is embedded, so ignored
15/06/18 10:28:48 INFO HiveMetaStore: 0: create\_table: Table(tableName:students, dbName:\*default\*, owner:ccaballero, createTime:1434616128, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col, type:array\<string\>, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe, parameters:{DefaultLimit=1000, serialization.format=1, highavailability=true}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=TRUE, spark.sql.sources.provider=a}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED\_TABLE)
15/06/18 10:28:48 INFO audit: ugi=ccaballero	ip=unknown-ip-addr	cmd=create\_table: Table(tableName:students, dbName:default, owner:ccaballero, createTime:1434616128, lastAccessTime:0, retention:0, sd:StorageDescriptor(cols:[FieldSchema(name:col, type:array\<string\>, comment:from deserializer)], location:null, inputFormat:org.apache.hadoop.mapred.SequenceFileInputFormat, outputFormat:org.apache.hadoop.hive.ql.io.HiveSequenceFileOutputFormat, compressed:false, numBuckets:-1, serdeInfo:SerDeInfo(name:null, serializationLib:org.apache.hadoop.hive.serde2.MetadataTypedColumnsetSerDe, parameters:{DefaultLimit=1000, serialization.format=1, highavailability=true}), bucketCols:[], sortCols:[], parameters:{}, skewedInfo:SkewedInfo(skewedColNames:[], skewedColValues:[], skewedColValueLocationMaps:{})), partitionKeys:[], parameters:{EXTERNAL=TRUE, spark.sql.sources.provider=a}, viewOriginalText:null, viewExpandedText:null, tableType:MANAGED\_TABLE)	
15/06/18 10:28:49 INFO SparkContext: Starting job: show at \<console\>:20
15/06/18 10:28:49 INFO DAGScheduler: Got job 2 (show at \<console\>:20) with 1 output partitions (allowLocal=false)
15/06/18 10:28:49 INFO DAGScheduler: Final stage: ResultStage 2(show at \<console\>:20)
15/06/18 10:28:49 INFO DAGScheduler: Parents of final stage: List()
15/06/18 10:28:49 INFO DAGScheduler: Missing parents: List()
15/06/18 10:28:49 INFO DAGScheduler: Submitting ResultStage 2 (MapPartitionsRDD[6] at show at \<console\>:20), which has no missing parents
15/06/18 10:28:49 INFO MemoryStore: ensureFreeSpace(1792) called with curMem=0, maxMem=278302556
15/06/18 10:28:49 INFO MemoryStore: Block broadcast\_2 stored as values in memory (estimated size 1792.0 B, free 265.4 MB)
15/06/18 10:28:49 INFO MemoryStore: ensureFreeSpace(1139) called with curMem=1792, maxMem=278302556
15/06/18 10:28:49 INFO MemoryStore: Block broadcast\_2\_piece0 stored as bytes in memory (estimated size 1139.0 B, free 265.4 MB)
15/06/18 10:28:49 INFO BlockManagerInfo: Added broadcast\_2\_piece0 in memory on localhost:59110 (size: 1139.0 B, free: 265.4 MB)
15/06/18 10:28:49 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:874
15/06/18 10:28:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (MapPartitionsRDD[6] at show at \<console\>:20)
15/06/18 10:28:49 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
15/06/18 10:28:49 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, PROCESS\_LOCAL, 1379 bytes)
15/06/18 10:28:49 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
15/06/18 10:28:49 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 628 bytes result sent to driver
15/06/18 10:28:49 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 10 ms on localhost (1/1)
15/06/18 10:28:49 INFO DAGScheduler: ResultStage 2 (show at \<console\>:20) finished in 0.010 s
15/06/18 10:28:49 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
15/06/18 10:28:49 INFO DAGScheduler: Job 2 finished: show at \<console\>:20, took 0.016204 s
++
\|\|
++
++

{quote}

Any suggestions would be appreciated.

Thank you.


---

* [SPARK-8434](https://issues.apache.org/jira/browse/SPARK-8434) | *Major* | **Add a "pretty" parameter to show**

Sometimes the user may want to show the complete content of cells, such as "sql("set -v")"


---

* [SPARK-8432](https://issues.apache.org/jira/browse/SPARK-8432) | *Major* | **Fix hashCode and equals() of BinaryType in Row**

The hashCode of BinaryType should be consistent with the bytes in it, and equals() should compare the bytes of BinaryType.


---

* [SPARK-8431](https://issues.apache.org/jira/browse/SPARK-8431) | *Major* | **Add in operator to DataFrame Column in SparkR**

To filter values in a set, we should add {{%in%}} operation into SparkR.

{noformat}
df$a %in% c(1, 2, 3)
{noformat}


---

* [SPARK-8429](https://issues.apache.org/jira/browse/SPARK-8429) | *Minor* | **Add ability to set additional tags**

Currently it is not possible to add custom tags to the cluster instances; tags are quite useful for many things, and it should be pretty straightforward to add an extra parameter to support this.


---

* [SPARK-8422](https://issues.apache.org/jira/browse/SPARK-8422) | *Major* | **Introduce a module abstraction inside of dev/run-tests**

At a high level, we have Spark modules / components which

1. are affected / impacted by file changes (e.g. a module is associated with a set of source files, so changes to those files change the module),
2. contain a set of tests to run, which are triggered via Maven, SBT, or via Python / R scripts.
3. depend on other modules and have dependent modules: if we change core, then every downstream test should be run.  If we change only MLlib, then we can skip the SQL tests but should probably run the Python MLlib tests, etc.

Right now, the per-module logic is spread across a few different places inside of the {{dev/run-tests}} script: we have one function that describes how to detect changes for all modules, another function that (implicitly) deals with module dependencies, etc.

Instead, I propose that we introduce a class for describing a module, use instances of this class to build up a dependency graph, then phrase the "find which tests to run" operations in terms of that graph.  I think that this will be easier to understand / maintain.


---

* [SPARK-8420](https://issues.apache.org/jira/browse/SPARK-8420) | *Blocker* | **Inconsistent behavior with Dataframe Timestamp between 1.3.1 and 1.4.0**

I am trying out 1.4.0 and notice there are some differences in behavior with Timestamp between 1.3.1 and 1.4.0. 

In 1.3.1, I can compare a Timestamp with string.
{code}
scala\> val df = sqlContext.createDataFrame(Seq((1, Timestamp.valueOf("2015-01-01 00:00:00")), (2, Timestamp.valueOf("2014-01-01 00:00:00"))))
...
scala\> df.filter($"\_2" \<= "2014-06-01").show
...
\_1 \_2                  
2  2014-01-01 00:00:...
{code}

However, in 1.4.0, the filter is always false:
{code}
scala\> val df = sqlContext.createDataFrame(Seq((1, Timestamp.valueOf("2015-01-01 00:00:00")), (2, Timestamp.valueOf("2014-01-01 00:00:00"))))
df: org.apache.spark.sql.DataFrame = [\_1: int, \_2: timestamp]

scala\> df.filter($"\_2" \<= "2014-06-01").show
+--+--+
\|\_1\|\_2\|
+--+--+
+--+--+
{code}


Not sure if that is intended, but I cannot find any doc mentioning these inconsistencies.


---

* [SPARK-8416](https://issues.apache.org/jira/browse/SPARK-8416) | *Major* | **Thread dump page should highlight Spark executor threads**

On the Spark thread dump page, it's hard to pick out executor threads from other system threads.  The UI should employ some color coding or highlighting to make this more apparent.


---

* [SPARK-8410](https://issues.apache.org/jira/browse/SPARK-8410) | *Minor* | **Hive VersionsSuite RuntimeException**

While testing Spark Project Hive, there are RuntimeExceptions as follows,

VersionsSuite:
- success sanity check \*\*\* FAILED \*\*\*
  java.lang.RuntimeException: [download failed: org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle), download failed: org.codehaus.groovy#groovy-all;2.1.6!groovy-all.jar, download failed: asm#asm;3.2!asm.jar]
  at org.apache.spark.deploy.SparkSubmitUtils$.resolveMavenCoordinates(SparkSubmit.scala:978)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anonfun$3.apply(IsolatedClientLoader.scala:62)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anonfun$3.apply(IsolatedClientLoader.scala:62)
  at org.apache.spark.sql.catalyst.util.package$.quietly(package.scala:38)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$.org$apache$spark$sql$hive$client$IsolatedClientLoader$$downloadVersion(IsolatedClientLoader.scala:61)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anonfun$1.apply(IsolatedClientLoader.scala:44)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$$anonfun$1.apply(IsolatedClientLoader.scala:44)
  at scala.collection.mutable.MapLike$class.getOrElseUpdate(MapLike.scala:189)
  at scala.collection.mutable.AbstractMap.getOrElseUpdate(Map.scala:91)
  at org.apache.spark.sql.hive.client.IsolatedClientLoader$.forVersion(IsolatedClientLoader.scala:44)
  ...

The tests are executed with the following set of options,

build/mvn --pl sql/hive --fail-never -Pyarn -Phadoop-2.4 -Dhadoop.version=2.6.0 test

Adding the following dependencies in the "spark/sql/hive/pom.xml"  file solves this issue,

\< 	\<dependency\>
\< 		\<groupId\>org.jboss.netty\</groupId\>
\< 		\<artifactId\>netty\</artifactId\>
\< 		\<version\>3.2.2.Final\</version\>
\< 	        \<scope\>test\</scope\>
\< 	\</dependency\>
\< 	\<dependency\>
\< 		\<groupId\>org.codehaus.groovy\</groupId\>
\< 		\<artifactId\>groovy-all\</artifactId\>
\< 		\<version\>2.1.6\</version\>
\< 		\<scope\>test\</scope\>
\< 	\</dependency\>
\< 
\< 	\<dependency\>
\< 		\<groupId\>asm\</groupId\>
\< 		\<artifactId\>asm\</artifactId\>
\< 		\<version\>3.2\</version\>
\< 	        \<scope\>test\</scope\>
\< 	\</dependency\>
\< 

The question is, Is this the correct way to fix this runtimeException ?
If yes, Can a pull request fix this issue permanently ?
If not, suggestions please.

Updates:
The above mentioned quick fix is not working with the latest 1.4 because of
this pull commits :
 [SPARK-8095] Resolve dependencies of --packages in local ivy cache #6788 
https://github.com/apache/spark/pull/6788

Due to this above commit, now the lookup directories during testing phase
has changed as follows,
:: problems summary ::
:::: WARNINGS
		[NOT FOUND  ] org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle) (2ms)

	==== local-m2-cache: tried

	  file:/home/joe/sparkibmsoe/spark/sql/hive/dummy/.m2/repository/org/jboss/netty/netty/3.2.2.Final/netty-3.2.2.Final.jar

		[NOT FOUND  ] org.codehaus.groovy#groovy-all;2.1.6!groovy-all.jar (0ms)

	==== local-m2-cache: tried

	  file:/home/joe/sparkibmsoe/spark/sql/hive/dummy/.m2/repository/org/codehaus/groovy/groovy-all/2.1.6/groovy-all-2.1.6.jar

		[NOT FOUND  ] asm#asm;3.2!asm.jar (0ms)

	==== local-m2-cache: tried

	  file:/home/joe/sparkibmsoe/spark/sql/hive/dummy/.m2/repository/asm/asm/3.2/asm-3.2.jar

		::::::::::::::::::::::::::::::::::::::::::::::

		::              FAILED DOWNLOADS            ::

		:: ^ see resolution messages for details  ^ ::

		::::::::::::::::::::::::::::::::::::::::::::::

		:: org.jboss.netty#netty;3.2.2.Final!netty.jar(bundle)

		:: org.codehaus.groovy#groovy-all;2.1.6!groovy-all.jar

		:: asm#asm;3.2!asm.jar

		::::::::::::::::::::::::::::::::::::::::::::::


---

* [SPARK-8407](https://issues.apache.org/jira/browse/SPARK-8407) | *Major* | **complex type constructors: struct and named\_struct**

struct(val1, val2, val3, ...)
Creates a struct with the given field values. Struct field names will be col1, col2, ....

named\_struct(name1, val1, name2, val2, ...)
Creates a struct with the given field names and values. (As of Hive 0.8.0.)


---

* [SPARK-8406](https://issues.apache.org/jira/browse/SPARK-8406) | *Blocker* | **Race condition when writing Parquet files**

To support appending, the Parquet data source tries to find out the max part number of part-files in the destination directory (the \<id\> in output file name "part-r-\<id\>.gz.parquet") at the beginning of the write job. In 1.3.0, this step happens on driver side before any files are written. However, in 1.4.0, this is moved to task side. Thus, for tasks scheduled later, they may see wrong max part number generated by newly written files by other finished tasks within the same job. This actually causes a race condition. In most cases, this only causes nonconsecutive IDs in output file names. But when the DataFrame contains thousands of RDD partitions, it's likely that two tasks may choose the same part number, thus one of them gets overwritten by the other.

The following Spark shell snippet can reproduce nonconsecutive part numbers:
{code}
sqlContext.range(0, 128).repartition(16).write.mode("overwrite").parquet("foo")
{code}
"16" can be replaced with any integer that is greater than the default parallelism on your machine (usually it means core number, on my machine it's 8).
{noformat}
-rw-r--r--   3 lian supergroup          0 2015-06-17 00:06 /user/lian/foo/\_SUCCESS
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00001.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00002.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00003.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00004.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00005.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00006.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00007.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00008.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00017.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00018.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00019.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00020.gz.parquet
-rw-r--r--   3 lian supergroup        352 2015-06-17 00:06 /user/lian/foo/part-r-00021.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00022.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00023.gz.parquet
-rw-r--r--   3 lian supergroup        353 2015-06-17 00:06 /user/lian/foo/part-r-00024.gz.parquet
{noformat}

And here is another Spark shell snippet for reproducing overwriting:
{code}
sqlContext.range(0, 10000).repartition(500).write.mode("overwrite").parquet("foo")
sqlContext.read.parquet("foo").count()
{code}
Expected answer should be {{10000}}, but you may see a number like {{9960}} due to overwriting. The actual number varies for different runs and different nodes.

Notice that the newly added ORC data source is less likely to hit this issue because it uses task ID and {{System.currentTimeMills()}} to generate the output file name. Thus, the ORC data source may hit this issue only when two tasks with the same task ID (which means they are in two concurrent jobs) are writing to the same location within the same millisecond.


---

* [SPARK-8401](https://issues.apache.org/jira/browse/SPARK-8401) | *Minor* | **Build system scala version selection script fails on Mac OS X**

The {{dev/change-version-to-\*.sh}} selection scripts use syntax for GNU sed which produces incorrect results when run with Mac OS X's built in version of sed. For example:

{noformat}
[msa@Michaels-MacBook-Pro spark-1.4]$ ./dev/change-version-to-2.11.sh 
[msa@Michaels-MacBook-Pro spark-1.4]$ gst
On branch scala-versions
Your branch and 'vamp/scala-versions' have diverged,
and have 7 and 4 different commits each, respectively.
  (use "git pull" to merge the remote branch into yours)
Changes not staged for commit:
  (use "git add \<file\>..." to update what will be committed)
  (use "git checkout -- \<file\>..." to discard changes in working directory)

	modified:   assembly/pom.xml
	modified:   bagel/pom.xml
	modified:   core/pom.xml
	modified:   dev/change-scala-version.sh
	modified:   docs/\_plugins/copy\_api\_dirs.rb
	modified:   examples/pom.xml
	modified:   external/flume-sink/pom.xml
	modified:   external/flume/pom.xml
	modified:   external/kafka-assembly/pom.xml
	modified:   external/kafka/pom.xml
	modified:   external/mqtt/pom.xml
	modified:   external/twitter/pom.xml
	modified:   external/zeromq/pom.xml
	modified:   extras/java8-tests/pom.xml
	modified:   extras/kinesis-asl/pom.xml
	modified:   extras/spark-ganglia-lgpl/pom.xml
	modified:   graphx/pom.xml
	modified:   launcher/pom.xml
	modified:   mllib/pom.xml
	modified:   network/common/pom.xml
	modified:   network/shuffle/pom.xml
	modified:   network/yarn/pom.xml
	modified:   pom.xml
	modified:   repl/pom.xml
	modified:   sql/catalyst/pom.xml
	modified:   sql/core/pom.xml
	modified:   sql/hive-thriftserver/pom.xml
	modified:   sql/hive/pom.xml
	modified:   streaming/pom.xml
	modified:   tools/pom.xml
	modified:   unsafe/pom.xml
	modified:   yarn/pom.xml

Untracked files:
  (use "git add \<file\>..." to include in what will be committed)

	assembly/pom.xml-e
	bagel/pom.xml-e
	core/pom.xml-e
	dev/audit-release/blank\_maven\_build/pom.xml-e
	dev/audit-release/maven\_app\_core/pom.xml-e
	docs/\_plugins/copy\_api\_dirs.rb-e
	examples/pom.xml-e
	external/flume-sink/pom.xml-e
	external/flume/pom.xml-e
	external/kafka-assembly/pom.xml-e
	external/kafka/pom.xml-e
	external/mqtt/pom.xml-e
	external/twitter/pom.xml-e
	external/zeromq/pom.xml-e
        extras/java8-tests/pom.xml-e
	extras/kinesis-asl/pom.xml-e
	extras/spark-ganglia-lgpl/pom.xml-e
	graphx/pom.xml-e
	launcher/pom.xml-e
	mllib/pom.xml-e
	network/common/pom.xml-e
	network/shuffle/pom.xml-e
	network/yarn/pom.xml-e
	pom.xml-e
	repl/pom.xml-e
	sql/catalyst/pom.xml-e
	sql/core/pom.xml-e
	sql/hive-thriftserver/pom.xml-e
	sql/hive/pom.xml-e
	streaming/pom.xml-e
	tools/pom.xml-e
	unsafe/pom.xml-e
	yarn/pom.xml-e

no changes added to commit (use "git add" and/or "git commit -a")
{noformat}

Homebrew and MacPorts provide packages for GNU sed which install it as {{gsed}}. Therefore, I suggest that if the default system {{sed}} command is not GNU sed, we look for {{gsed}} and use it if available.


---

* [SPARK-8399](https://issues.apache.org/jira/browse/SPARK-8399) | *Minor* | **Overlap between histograms and axis' name in Spark Streaming UI**

If you have an histogram skewed towards the maximum of the displayed values as is the case with the number of messages processed per batchInterval with the Kafka direct API (since it's a constant) for example, the histogram will overlap with the name of the X axis (#batches).

Unfortunately, I don't have any screenshots available.


---

* [SPARK-8397](https://issues.apache.org/jira/browse/SPARK-8397) | *Minor* | **Allow custom configuration for TestHive**

We encourage people to use {{TestHive}} in unit tests, because it's impossible to create more than one {{HiveContext}} within one process. The current implementation locks people into using a {{local[2]}} {{SparkContext}} underlying their {{HiveContext}}. We should make it possible to override this using a system property so that people can test against {{local-cluster}} or remote spark clusters to make their tests more realistic.


---

* [SPARK-8395](https://issues.apache.org/jira/browse/SPARK-8395) | *Minor* | **spark-submit documentation is incorrect**

Using a fresh checkout of 1.4.0-bin-hadoop2.6

if you run 
./start-slave.sh  1 spark://localhost:7077

you get
failed to launch org.apache.spark.deploy.worker.Worker:
                             Default is conf/spark-defaults.conf.
  15/06/16 13:11:08 INFO Utils: Shutdown hook called

it seems the worker number is not being accepted  as desccribed here:
https://spark.apache.org/docs/latest/spark-standalone.html

The documentation says:
./sbin/start-slave.sh \<worker#\> \<master-spark-URL\>

but the start.slave-sh script states:
usage="Usage: start-slave.sh \<spark-master-URL\> where \<spark-master-URL\> is like spark://localhost:7077"

I have checked for similar issues using :
https://issues.apache.org/jira/browse/SPARK-6552?jql=text%20~%20%22start-slave%22

and found nothing similar so am raising this as an issue.


---

* [SPARK-8392](https://issues.apache.org/jira/browse/SPARK-8392) | *Minor* | **RDDOperationGraph: getting cached nodes is slow**

def getAllNodes: Seq[RDDOperationNode] = {
    \_childNodes ++ \_childClusters.flatMap(\_.childNodes)
  }
when the \_childClusters has so many nodes, the process will hang on. I think we can improve the efficiency here.


---

* [SPARK-8389](https://issues.apache.org/jira/browse/SPARK-8389) | *Critical* | **Expose KafkaRDDs offsetRange in Python**

Probably requires creating a JavaKafkaPairRDD and also use that in the python APIs


---

* [SPARK-8382](https://issues.apache.org/jira/browse/SPARK-8382) | *Major* | **Improve Analysis Unit test framework**

We have some nice frameworks for doing various unit test {{checkAnswer}}, {{comparePlan}}, {{checkEvaluation}}, etc.  However {{AnalysisSuite}} is kind of sloppy with each test using assertions in different ways.  I'd like a function that looks something like the following:

{code}
def checkAnalysis(
  inputPlan: LogicalPlan,
  expectedPlan: LogicalPlan = null,
  caseInsensitiveOnly: Boolean = false,
  expectedErrors: Seq[String] = Nil)
{code}

This function should construct tests that check the Analyzer works as expected and provides useful error messages when any failures are encountered.  We should then rewrite the existing tests and beef up our coverage here.


---

* [SPARK-8381](https://issues.apache.org/jira/browse/SPARK-8381) | *Major* | **reuse typeConvert when convert Seq[Row] to catalyst type**

This method CatalystTypeConverters.convertToCatalyst is slow, so for batch conversion we should be using converter produced by createToCatalystConverter.


---

* [SPARK-8379](https://issues.apache.org/jira/browse/SPARK-8379) | *Major* | **LeaseExpiredException when using dynamic partition with speculative execution**

when inserting to table using dynamic partitions with \*spark.speculation=true\*  and there is a skew data of some partitions trigger the speculative tasks ,it will throws the exception like
{code}
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException): Lease mismatch on /tmp/hive-jeanlyn/hive\_2015-06-15\_15-20-44\_734\_8801220787219172413-1/-ext-10000/ds=2015-06-15/type=2/part-00301.lzo owned by DFSClient\_attempt\_201506031520\_0011\_m\_000189\_0\_-1513487243\_53 but is accessed by DFSClient\_attempt\_201506031520\_0011\_m\_000042\_0\_-1275047721\_57
{code}


---

* [SPARK-8376](https://issues.apache.org/jira/browse/SPARK-8376) | *Minor* | **Commons Lang 3 is one of the required JAR of Spark Flume Sink but is missing in the docs**

Commons Lang 3 is added as one of the dependencies of Spark Flume Sink since https://github.com/apache/spark/pull/5703. However, the docs has not yet updated.


---

* [SPARK-8373](https://issues.apache.org/jira/browse/SPARK-8373) | *Minor* | **When an RDD has no partition, Python sum will throw "Can not reduce() empty RDD"**

The issue is because "sum" uses "reduce". Replacing it with "fold" will fix it.


---

* [SPARK-8372](https://issues.apache.org/jira/browse/SPARK-8372) | *Minor* | **History server shows incorrect information for application not started**

The history server may show an incorrect App ID for an incomplete application like \<App ID\>.inprogress. This app info will never disappear even after the app is completed.


---

* [SPARK-8368](https://issues.apache.org/jira/browse/SPARK-8368) | *Blocker* | **ClassNotFoundException in closure for map**

After upgraded the cluster from spark 1.3.0 to 1.4.0(rc4), I encountered the following exception:
======begin exception========
{quote}
Exception in thread "main" java.lang.ClassNotFoundException: com.yhd.ycache.magic.Model$$anonfun$9$$anonfun$10
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:278)
	at org.apache.spark.util.InnerClosureFinder$$anon$4.visitMethodInsn(ClosureCleaner.scala:455)
	at com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassReader.accept(Unknown Source)
	at com.esotericsoftware.reflectasm.shaded.org.objectweb.asm.ClassReader.accept(Unknown Source)
	at org.apache.spark.util.ClosureCleaner$.getInnerClosureClasses(ClosureCleaner.scala:101)
	at org.apache.spark.util.ClosureCleaner$.org$apache$spark$util$ClosureCleaner$$clean(ClosureCleaner.scala:197)
	at org.apache.spark.util.ClosureCleaner$.clean(ClosureCleaner.scala:132)
	at org.apache.spark.SparkContext.clean(SparkContext.scala:1891)
	at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:294)
	at org.apache.spark.rdd.RDD$$anonfun$map$1.apply(RDD.scala:293)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)
	at org.apache.spark.rdd.RDD.map(RDD.scala:293)
	at org.apache.spark.sql.DataFrame.map(DataFrame.scala:1210)
	at com.yhd.ycache.magic.Model$.main(SSExample.scala:239)
	at com.yhd.ycache.magic.Model.main(SSExample.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:664)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
{quote}
===============end exception===========

I simplify the code that cause this issue, as following:
==========begin code==================
{noformat}
object Model extends Serializable{
  def main(args: Array[String]) {
    val Array(sql) = args
    val sparkConf = new SparkConf().setAppName("Mode Example")
    val sc = new SparkContext(sparkConf)
    val hive = new HiveContext(sc)
    //get data by hive sql
    val rows = hive.sql(sql)

    val data = rows.map(r =\> { 
      val arr = r.toSeq.toArray
      val label = 1.0
      def fmap = ( input: Any ) =\> 1.0
      val feature = arr.map(\_=\>1.0)
      LabeledPoint(label, Vectors.dense(feature))
    })

    data.count()
  }
}
{noformat}
=====end code===========
This code can run pretty well on spark-shell, but error when submit it to spark cluster (standalone or local mode).  I try the same code on spark 1.3.0(local mode), and no exception is encountered.


---

* [SPARK-8367](https://issues.apache.org/jira/browse/SPARK-8367) | *Major* | **ReliableKafka will loss data when `spark.streaming.blockInterval` was 0**

{code:title=BlockGenerator.scala\|borderStyle=solid}
  /\*\* Change the buffer to which single records are added to. \*/
  private def updateCurrentBuffer(time: Long): Unit = synchronized {
    try {
      val newBlockBuffer = currentBuffer
      currentBuffer = new ArrayBuffer[Any]
      if (newBlockBuffer.size \> 0) {

       val blockId = StreamBlockId(receiverId, time - blockIntervalMs)

        val newBlock = new Block(blockId, newBlockBuffer)
        listener.onGenerateBlock(blockId)
        blocksForPushing.put(newBlock)  // put is blocking when queue is full
        logDebug("Last element in " + blockId + " is " + newBlockBuffer.last)
      }
    } catch {
      case ie: InterruptedException =\>
        logInfo("Block updating timer thread was interrupted")
      case e: Exception =\>
        reportError("Error in block updating thread", e)
    }
  }
{code}

If \*spark.streaming.blockInterval\* was 0, the \*blockId\* in the code will always be the same because of  \*time\* was 0 and \*blockIntervalMs\* was 0 too.

{code:title=ReliableKafkaReceiver.scala\|borderStyle=solid}
   private def rememberBlockOffsets(blockId: StreamBlockId): Unit = {
    // Get a snapshot of current offset map and store with related block id.
    val offsetSnapshot = topicPartitionOffsetMap.toMap
    blockOffsetMap.put(blockId, offsetSnapshot)
    topicPartitionOffsetMap.clear()
  }
{code}
If the \*blockId\* was the same,  Streaming will commit the  \*offset\*  before the really data comsumed(data was waitting to be commit but the offset had updated and commit by previous commit)
So when exception occures, the \*offset\* had commit but the data will loss since the data was in memory and not comsumed yet.


---

* [SPARK-8366](https://issues.apache.org/jira/browse/SPARK-8366) | *Major* | **maxNumExecutorsNeeded should properly handle failed tasks**

I use the \*dynamic executor allocation\* function. 
When an executor is killed, all running tasks on it will be failed. Until reach the maxTaskFailures, this failed task will re-run with a new task id. 
But the \*ExecutorAllocationManager\* won't concern this new tasks to total and pending tasks, because the total stage task number only set when stage submitted.


---

* [SPARK-8364](https://issues.apache.org/jira/browse/SPARK-8364) | *Major* | **Add crosstab to SparkR DataFrames**

Add `crosstab` to SparkR DataFrames, which takes two column names and returns a local R data.frame. This is similar to `table` in R. However, `table` in SparkR is used for loading SQL tables as DataFrames. The return type is data.frame instead table for `crosstab` to be compatible with Scala/Python.


---

* [SPARK-8363](https://issues.apache.org/jira/browse/SPARK-8363) | *Major* | **Move sqrt into math**

It doesn't really belong in Arithmetic. It should also extend UnaryMathExpression.


---

* [SPARK-8359](https://issues.apache.org/jira/browse/SPARK-8359) | *Major* | **Spark SQL Decimal type precision loss on multiplication**

It looks like the precision of decimal can not be raised beyond ~2^112 without causing full value truncation.

The following code computes the power of two up to a specific point
{code}
import org.apache.spark.sql.types.Decimal

val one = Decimal(1)
val two = Decimal(2)

def pow(n : Int) :  Decimal = if (n \<= 0) { one } else { 
  val a = pow(n - 1)
  a.changePrecision(n,0)
  two.changePrecision(n,0)
  a \* two
}

(109 to 120).foreach(n =\> println(pow(n).toJavaBigDecimal.unscaledValue.toString))
649037107316853453566312041152512
1298074214633706907132624082305024
2596148429267413814265248164610048
5192296858534827628530496329220096
1038459371706965525706099265844019
2076918743413931051412198531688038
4153837486827862102824397063376076
8307674973655724205648794126752152
1661534994731144841129758825350430
3323069989462289682259517650700860
6646139978924579364519035301401720
1329227995784915872903807060280344
{code}
Beyond ~2^112 the precision is truncated even if the precision was set to n and should thus handle 10^n without problems..


---

* [SPARK-8358](https://issues.apache.org/jira/browse/SPARK-8358) | *Blocker* | **DataFrame explode with alias and \* fails**

{code}
scala\> Seq((Array("a"), 1)).toDF("a", "b").select(explode($"a").as("a"), $"\*")
org.apache.spark.sql.catalyst.analysis.UnresolvedException: Invalid call to dataType on unresolved object, tree: 'a
        at org.apache.spark.sql.catalyst.analysis.UnresolvedAttribute.dataType(unresolved.scala:60)
        at org.apache.spark.sql.catalyst.expressions.Explode.elementTypes(generators.scala:107)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGenerate$AliasedGenerator$.unapply(Analyzer.scala:577)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGenerate$$anonfun$apply$16$$anonfun$22.apply(Analyzer.scala:535)
        at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveGenerate$$anonfun$apply$16$$anonfun$22.apply(Analyzer.scala:534)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
        at scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:251)
...
{code}


---

* [SPARK-8357](https://issues.apache.org/jira/browse/SPARK-8357) | *Critical* | **Memory leakage on unsafe aggregation path with empty input**

Currently, unsafe-based hash is released on 'next' call but if input is empty, it would not be called ever.


---

* [SPARK-8356](https://issues.apache.org/jira/browse/SPARK-8356) | *Critical* | **Reconcile callUDF and callUdf**

Right now we have two functions {{callUDF}} and {{callUdf}}.  I think the former is used for calling Java functions (and the documentation is wrong) and the latter is for calling functions by name.  Either way this is confusing and we should unify or pick different names.  Also, lets make sure the docs are right.


---

* [SPARK-8355](https://issues.apache.org/jira/browse/SPARK-8355) | *Critical* | **Python DataFrameReader/Writer should mirror scala**

All the functions that I can run in scala should also work in python.  At least {{ctx.read.option}} is missing, but we should also audit to make sure there aren't others.


---

* [SPARK-8354](https://issues.apache.org/jira/browse/SPARK-8354) | *Major* | **Fix off-by-factor-of-8 error when allocating scratch space in UnsafeFixedWidthAggregationMap**

UnsafeFixedWidthAggregationMap contains an off-by-factor-of-8 error when allocating row conversion scratch space: we take a size requirement, measured in bytes, then allocate a long array of that size.  This means that we end up allocating 8x too much conversion space.


---

* [SPARK-8353](https://issues.apache.org/jira/browse/SPARK-8353) | *Major* | **Show anchor links when hovering over documentation headers**

When hovering over documentation headers, we should show clickable links that allow users to deep-link to specific sections of the documentation, similar to GitHub and Bootstrap docs.


---

* [SPARK-8350](https://issues.apache.org/jira/browse/SPARK-8350) | *Minor* | **R unit tests output should be logged to "unit-tests.log"**

Right now it's logged to "R-unit-tests.log". Jenkins currently only archives files named "unit-tests.log", and this is what all other modules (e.g. SQL, network, REPL) use.

1. We should be consistent
2. I don't want to reconfigure Jenkins to accept a different file


---

* [SPARK-8349](https://issues.apache.org/jira/browse/SPARK-8349) | *Major* | **Use expression constructors (rather than apply) in FunctionRegistry**

Right now we use appy methods -- would be better to switch to constructors so expressions with overloaded constructors don't need to define companion objects.


---

* [SPARK-8348](https://issues.apache.org/jira/browse/SPARK-8348) | *Major* | **Add in operator to DataFrame Column**

It is convenient to add "in" operator to column, so we can filter values in a set.

{code}
df.filter(col("brand").in("dell", "sony"))
{code}

In R, the operator should be `%in%`.


---

* [SPARK-8346](https://issues.apache.org/jira/browse/SPARK-8346) | *Major* | **Use InternalRow instread of catalyst.InternalRow**

It's annoying to use catalyst.InternalRow inside catalyst


---

* [SPARK-8344](https://issues.apache.org/jira/browse/SPARK-8344) | *Major* | **Add internal metrics / logging for DAGScheduler to detect long pauses / blocking**

It would be useful to be able to log warnings if the DAGScheduler event processing loop blocks for more than a certain amount of time (or if its message inbox grows too large).  This debugging logging (probably disabled by default) would be very helpful for finding places where the scheduling loop blocks / slows down.

We might be able to infer this information now from the web UI scheduler delays, but that's kind of hard to parse out of logs or use to raise monitoring alerts.


---

* [SPARK-8343](https://issues.apache.org/jira/browse/SPARK-8343) | *Minor* | **Improve the Spark Streaming Guides**

Improve the Spark Streaming Guides by fixing broken links, rewording confusing sections, fixing typos, adding missing words, etc.


---

* [SPARK-8342](https://issues.apache.org/jira/browse/SPARK-8342) | *Major* | **Decimal Math beyond ~2^112 is broken**

Here is a snippet from the spark-shell that should not happen

{code}
scala\> val d = Decimal(Long.MaxValue,100,0) \* Decimal(Long.MaxValue,100,0)
d: org.apache.spark.sql.types.Decimal = 0
scala\> d.toDebugString
res3: String = Decimal(expanded,0,1,0})
{code}

It looks like precision gets reseted on some operations and values are then truncated.


---

* [SPARK-8339](https://issues.apache.org/jira/browse/SPARK-8339) | *Minor* | **Itertools islice requires an integer for the stop argument.**

Itertools islice requires an integer for the stop argument.  The bug is in serializers.py and can prevent and rdd from being written to disk.


---

* [SPARK-8338](https://issues.apache.org/jira/browse/SPARK-8338) | *Minor* | **Ganglia fails to start**

Exception
{code}
Starting httpd: httpd: Syntax error on line 154 of /etc/httpd/conf/httpd.conf: Cannot load /etc/httpd/modules/mod\_authz\_core.so into server: /etc/httpd/modules/mod\_authz\_core.so: cannot open shared object file: No such file or directory
{code}


---

* [SPARK-8336](https://issues.apache.org/jira/browse/SPARK-8336) | *Major* | **Fix NullPointerException with functions.rand()**

The problem was first reported by Justin Yip in the thread 'NullPointerException with functions.rand()'

Here is how to reproduce the problem:
{code}
sqlContext.createDataFrame(Seq((1,2), (3, 100))).withColumn("index", rand(30)).show()
{code}


---

* [SPARK-8330](https://issues.apache.org/jira/browse/SPARK-8330) | *Major* | **DAG visualization: trim whitespace from input**

Just as a safeguard against DOM rewriting.


---

* [SPARK-8329](https://issues.apache.org/jira/browse/SPARK-8329) | *Blocker* | **DataSource options parser no longer accepts '\_'**

This is a regression from 1.3.1


---

* [SPARK-8322](https://issues.apache.org/jira/browse/SPARK-8322) | *Major* | **EC2 script not fully updated for 1.4.0 release**

In the spark\_ec2.py script, the "1.4.0" spark version hasn't been added to the VALID\_SPARK\_VERSIONS map or the SPARK\_TACHYON\_MAP, causing the script to break for the latest release.


---

* [SPARK-8320](https://issues.apache.org/jira/browse/SPARK-8320) | *Minor* | **Add example in streaming programming guide that shows union of multiple input streams**

The section on "Level of Parallelism in Data Receiving" has a Scala and a Java example for union of multiple input streams. A python example should be added.


---

* [SPARK-8319](https://issues.apache.org/jira/browse/SPARK-8319) | *Major* | **Update logic related to key ordering in shuffle dependencies**

The Tungsten ShuffleManager falls back to regular SortShuffleManager whenever the shuffle dependency specifies a key ordering, but technically we only need to fall back when an aggregator is also specified.  We should update the fallback logic to handle this case so that the Tungsten optimizations can apply to more workloads.

I also noticed that the SQL Exchange operator performs defensive copying of shuffle inputs when a key ordering is specified, but this is unnecessary: the only shuffle manager that performs sorting on the map side is SortShuffleManager, and it only performs sorting if an aggregator is specified.  SQL never uses Spark's shuffle for performing aggregation, so this copying is unnecessary.


---

* [SPARK-8317](https://issues.apache.org/jira/browse/SPARK-8317) | *Major* | **Do not push sort into shuffle in Exchange operator**

In some cases, Spark SQL pushes sorting operations into the shuffle layer by specifying a key ordering as part of the shuffle dependency. I think that we should not do this:

- Since we do not delegate aggregation to Spark's shuffle, specifying the keyOrdering as part of the shuffle has no effect on the shuffle map side.
- By performing the shuffle ourselves (by inserting a sort operator after the shuffle instead), we can use the Exchange planner to choose specialized sorting implementations based on the types of rows being sorted.
- We can remove some complexity from SqlSerializer2 by not requiring it to know about sort orderings, since SQL's own sort operators will already perform the necessary defensive copying.


---

* [SPARK-8316](https://issues.apache.org/jira/browse/SPARK-8316) | *Minor* | **Upgrade Maven to 3.3.3**

Maven versions prior to 3.3 apparently have some bugs.

See: https://github.com/apache/spark/pull/6492#issuecomment-111001101


---

* [SPARK-8314](https://issues.apache.org/jira/browse/SPARK-8314) | *Major* | **improvement in performance of MLUtils.appendBias**

MLUtils.appendBias method is heavily used in creating intercepts for linear models. This method uses Breeze's vector concatenation which is very slow compared to the plain System.arrayCopy. This improvement is to change the implementation to use System.arrayCopy. 

We saw the following performance improvements after the change:
Benchmark with mnist dataset for 50 times:
MLUtils.appendBias (SparseVector Before): 47320 ms
MLUtils.appendBias (SparseVector After): 1935 ms

MLUtils.appendBias (DenseVector Before): 5340 ms
MLUtils.appendBias (DenseVector After): 4080 ms

This is almost a 24 times performance boost for SparseVectors.


---

* [SPARK-8310](https://issues.apache.org/jira/browse/SPARK-8310) | *Critical* | **Spark EC2 branch in 1.4 is wrong**

It points to `branch-1.3` of spark-ec2 right now while it should point to `branch-1.4`

cc [~brdwrd] [~pwendell]


---

* [SPARK-8309](https://issues.apache.org/jira/browse/SPARK-8309) | *Critical* | **OpenHashMap doesn't work with more than 12M items**

The problem might be demonstrated with the following testcase:

{code}
  test("support for more than 12M items") {
    val cnt = 12000000 // 12M
    val map = new OpenHashMap[Int, Int](cnt)
    for (i \<- 0 until cnt) {
      map(i) = 1
    }
    val numInvalidValues = map.iterator.count(\_.\_2 == 0)
    assertResult(0)(numInvalidValues)
  }

{code}


---

* [SPARK-8308](https://issues.apache.org/jira/browse/SPARK-8308) | *Minor* | **add missing save load for python doc example and tune down MatrixFactorization iterations**

1. add some missing save/load in python examples, LogisticRegression, LinearRegression, NaiveBayes
2. tune down iterations for MatrixFactorization, since current number will trigger StackOverflow.


---

* [SPARK-8307](https://issues.apache.org/jira/browse/SPARK-8307) | *Major* | **Improve timestamp from parquet**

Currently, it's complicated to convert a timestamp from Parquet or Hive, really slow.


---

* [SPARK-8306](https://issues.apache.org/jira/browse/SPARK-8306) | *Major* | **AddJar command needs to set the new class loader to the HiveConf inside executionHive.state.**

In {{AddJar}} command, we are using {{org.apache.hadoop.hive.ql.metadata.Hive.get().getConf().setClassLoader(newClassLoader)}}. However, the conf returned by {{Hive.get().getConf()}} is not necessary the one set in {{executionHive.state}}. Thus, we may fail to set the correct class loader to {{executionHive}} in some cases.


---

* [SPARK-8305](https://issues.apache.org/jira/browse/SPARK-8305) | *Major* | **Improve codegen**

Fix small issues in codegen:

1. Fix Cast Decimal into Boolean
2. Fix Literal(null)
3. refactor


---

* [SPARK-8302](https://issues.apache.org/jira/browse/SPARK-8302) | *Major* | **Support heterogeneous cluster nodes on YARN**

Some of our customers install Hadoop on different paths across the cluster. When running a Spark app, this leads to a few complications because of how we try to reuse the rest of Hadoop.

Since all configuration for a Spark-on-YARN application is local, the code does not have enough information about how to run things on the rest of the cluster in such cases.

To illustrate: let's say that a node's configuration says that {{SPARK\_DIST\_CLASSPATH=/disk1/hadoop/lib/\*}}. If I launch a Spark app from that machine, but there's a machine on the cluster where Hadoop is actually installed in {{/disk2/hadoop/lib}}, then any container launched on that node will fail.

The problem does not exist (or is much less pronounced) on standalone and mesos since they require a local Spark installation and configuration.

It would be nice if we could easily support this use case on YARN.


---

* [SPARK-8301](https://issues.apache.org/jira/browse/SPARK-8301) | *Critical* | **Improve UTF8String substring/startsWith/endsWith/contains performance**

Many functions in UTF8String are unnecessarily expensive.


---

* [SPARK-8300](https://issues.apache.org/jira/browse/SPARK-8300) | *Major* | **DataFrame hint for broadcast join**

It is not always possible to have perfect cardinality estimation. We should allow users to give hint to the optimizer to do broadcast join.


---

* [SPARK-8297](https://issues.apache.org/jira/browse/SPARK-8297) | *Critical* | **Scheduler backend is not notified in case node fails in YARN**

When a node crashes, yarn detects the failure and notifies spark - but this information is not propagated to scheduler backend (unlike in mesos mode, for example).

It results in repeated re-execution of stages (due to FetchFailedException on shuffle side), resulting finally in application failure.


---

* [SPARK-8290](https://issues.apache.org/jira/browse/SPARK-8290) | *Minor* | **spark class command builder need read SPARK\_JAVA\_OPTS and SPARK\_DRIVER\_MEMORY properly**

SPARK\_JAVA\_OPTS was missed in reconstructing the launcher part, we should add it back so spark-class could read it.

The missing part is here: https://github.com/apache/spark/blob/1c30afdf94b27e1ad65df0735575306e65d148a1/bin/spark-class#L97.


---

* [SPARK-8289](https://issues.apache.org/jira/browse/SPARK-8289) | *Major* | **Provide a specific stack size with all Java implementations to prevent stack overflows with certain tests**

Default stack sizes differ per Java implementation - so tests can pass for those with higher stack sizes (OpenJDK) but will fail with Oracle or IBM Java owing to lower default sizes. In particular we can see this happening with the JavaALSSuite - with 15 iterations, we get stackoverflow errors with Oracle and IBM Java. We don't with OpenJDK. This JIRA aims to address such an issue by providing a default specified stack size to be used for all Java distributions: 4096k specified for both SBT test args and for Maven test args (changing project/ScalaBuild.scala and pom.xml respectively).


---

* [SPARK-8285](https://issues.apache.org/jira/browse/SPARK-8285) | *Trivial* | **CombineSum should be calculated as unlimited decimal first**

{code:title=GeneratedAggregate.scala}
case cs @ CombineSum(expr) =\>
        val calcType = expr.dataType
          expr.dataType match {
            case DecimalType.Fixed(\_, \_) =\>
              DecimalType.Unlimited
            case \_ =\>
              expr.dataType
          }
{code}
calcType is always expr.dataType. credits are all belong to IntelliJ


---

* [SPARK-8283](https://issues.apache.org/jira/browse/SPARK-8283) | *Blocker* | **udf\_struct test failure**

{code}
[info] - udf\_struct \*\*\* FAILED \*\*\* (704 milliseconds)
[info]   Failed to execute query using catalyst:
[info]   Error: org.apache.spark.sql.catalyst.expressions.Literal cannot be cast to org.apache.spark.sql.catalyst.expressions.NamedExpression
[info]   java.lang.ClassCastException: org.apache.spark.sql.catalyst.expressions.Literal cannot be cast to org.apache.spark.sql.catalyst.expressions.NamedExpression
[info]   	at org.apache.spark.sql.catalyst.expressions.CreateStruct$$anonfun$1.apply(complexTypes.scala:64)
[info]   	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]   	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]   	at scala.collection.immutable.List.foreach(List.scala:318)
[info]   	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
[info]   	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
[info]   	at org.apache.spark.sql.catalyst.expressions.CreateStruct.dataType$lzycompute(complexTypes.scala:64)
[info]   	at org.apache.spark.sql.catalyst.expressions.CreateStruct.dataType(complexTypes.scala:61)
[info]   	at org.apache.spark.sql.catalyst.expressions.CreateStruct.dataType(complexTypes.scala:55)
[info]   	at org.apache.spark.sql.catalyst.expressions.ExtractValue$.apply(ExtractValue.scala:43)
[info]   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4.applyOrElse(Analyzer.scala:353)
[info]   	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4.applyOrElse(Analyzer.scala:340)
[info]   	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
[info]   	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
[info]   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
[info]   	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:285)
[info]   	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$5.apply(TreeNode.scala:299)
[info]   	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
[info]   	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
[info]   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
[info]   	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
[info]   	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
[info]   	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
[info]   	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
[info]   	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
[info]   	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
{code}


---

* [SPARK-8282](https://issues.apache.org/jira/browse/SPARK-8282) | *Major* | **Make number of threads used in RBackend configurable**

RBackend starts a netty server which uses two threads. The number of threads is hardcoded. It is useful to have it configurable.


---

* [SPARK-8281](https://issues.apache.org/jira/browse/SPARK-8281) | *Blocker* | **udf\_asin and udf\_acos test failure**

acos/asin in Hive returns NaN for not a number, whereas we always return null.


---

* [SPARK-8280](https://issues.apache.org/jira/browse/SPARK-8280) | *Blocker* | **udf7 failed due to null vs nan semantics**

To execute
{code}
sbt/sbt -Phive -Dspark.hive.whitelist="udf7.\*" "hive/test-only org.apache.spark.sql.hive.execution.HiveCompatibilitySuite"
{code}

If we want to be consistent with Hive, we need to special case our log function.


---

* [SPARK-8279](https://issues.apache.org/jira/browse/SPARK-8279) | *Blocker* | **udf\_round\_3 test fails**

query

{code}
select round(cast(negative(pow(2, 31)) as INT)), round(cast((pow(2, 31) - 1) as INT)), round(-32769), round(32768) from src tablesample (1 rows);
{code}

{code}
[info] - udf\_round\_3 \*\*\* FAILED \*\*\* (4 seconds, 803 milliseconds)
[info]   Failed to execute query using catalyst:
[info]   Error: java.lang.Integer cannot be cast to java.lang.Double
[info]   java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.Double
[info]   	at scala.runtime.BoxesRunTime.unboxToDouble(BoxesRunTime.java:119)
[info]   	at org.apache.spark.sql.catalyst.expressions.BinaryMathExpression.eval(math.scala:86)
[info]   	at org.apache.spark.sql.hive.HiveInspectors$class.toInspector(HiveInspectors.scala:628)
[info]   	at org.apache.spark.sql.hive.HiveGenericUdf.toInspector(hiveUdfs.scala:148)
[info]   	at org.apache.spark.sql.hive.HiveGenericUdf$$anonfun$argumentInspectors$1.apply(hiveUdfs.scala:160)
[info]   	at org.apache.spark.sql.hive.HiveGenericUdf$$anonfun$argumentInspectors$1.apply(hiveUdfs.scala:160)
[info]   	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]   	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]   	at scala.collection.immutable.List.foreach(List.scala:318)
[info]   	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
[info]   	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
[info]   	at org.apache.spark.sql.hive.HiveGenericUdf.argumentInspectors$lzycompute(hiveUdfs.scala:160)
[info]   	at org.apache.spark.sql.hive.HiveGenericUdf.argumentInspectors(hiveUdfs.scala:160)
[info]   	at org.apache.spark.sql.hive.HiveGenericUdf.returnInspector$lzycompute(hiveUdfs.scala:164)
[info]   	at org.apache.spark.sql.hive.HiveGenericUdf.returnInspector(hiveUdfs.scala:163)
[info]   	at org.apache.spark.sql.hive.HiveGenericUdf.dataType$lzycompute(hiveUdfs.scala:180)
[info]   	at org.apache.spark.sql.hive.HiveGenericUdf.dataType(hiveUdfs.scala:180)
[info]   	at org.apache.spark.sql.catalyst.expressions.Cast.resolved$lzycompute(Cast.scala:31)
[info]   	at org.apache.spark.sql.catalyst.expressions.Cast.resolved(Cast.scala:31)
[info]   	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$childrenResolved$1.apply(Expression.scala:121)
[info]   	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$childrenResolved$1.apply(Expression.scala:121)
[info]   	at scala.collection.LinearSeqOptimized$class.forall(LinearSeqOptimized.scala:70)
[info]   	at scala.collection.immutable.List.forall(List.scala:84)
[info]   	at org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:121)
[info]   	at org.apache.spark.sql.catalyst.expressions.Expression.resolved$lzycompute(Expression.scala:109)
[info]   	at org.apache.spark.sql.catalyst.expressions.Expression.resolved(Expression.scala:109)
[info]   	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$childrenResolved$1.apply(Expression.scala:121)
[info]   	at org.apache.spark.sql.catalyst.expressions.Expression$$anonfun$childrenResolved$1.apply(Expression.scala:121)
[info]   	at scala.collection.LinearSeqOptimized$class.forall(LinearSeqOptimized.scala:70)
[info]   	at scala.collection.immutable.List.forall(List.scala:84)
[info]   	at org.apache.spark.sql.catalyst.expressions.Expression.childrenResolved(Expression.scala:121)
[info]   	at org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$ConvertNaNs$$anonfun$apply$2$$anonfun$applyOrElse$2.applyOrElse(HiveTypeCoercion.scala:138)
[info]   	at org.apache.spark.sql.catalyst.analysis.HiveTypeCoercion$ConvertNaNs$$anonfun$apply$2$$anonfun$applyOrElse$2.applyOrElse(HiveTypeCoercion.scala:136)
[info]   	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)
[info]   	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:222)
[info]   	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
[info]   	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:221)
[info]   	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionDown$1(QueryPlan.scala:75)
[info]   	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1$$anonfun$apply$1.apply(QueryPlan.scala:90)
[info]   	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]   	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
[info]   	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
[info]   	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
[info]   	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
[info]   	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
[info]   	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$1.apply(QueryPlan.scala:89)
[info]   	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
[info]   	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
[info]   	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
[info]   	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
[info]   	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
[info]   	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
[info]   	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
[info]   	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
[info]   	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
[info]   	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
[info]   	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
[info]   	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
{code}


---

* [SPARK-8278](https://issues.apache.org/jira/browse/SPARK-8278) | *Critical* | **Remove deprecated JsonRDD functionality in Spark SQL**

The old JSON functionality (deprecated in 1.4) needs to be removed for 1.5.


---

* [SPARK-8274](https://issues.apache.org/jira/browse/SPARK-8274) | *Trivial* | **Fix wrong URLs in MLlib Frequent Pattern Mining Documentation**

There is a mistake in the URLs of the Scala section of FP-Growth in the MLlib Frequent Pattern Mining documentation. The URL points to https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/fpm/FPGrowth.html which is the Java's API, the link should point to the Scala API https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.fpm.FPGrowth

There's another mistake in the FP-GrowthModel in the same section, the link points, again, to the Java's API https://spark.apache.org/docs/latest/api/java/org/apache/spark/mllib/fpm/FPGrowthModel.html, the link should point to the Scala API https://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.mllib.fpm.FPGrowthModel


---

* [SPARK-8272](https://issues.apache.org/jira/browse/SPARK-8272) | *Major* | **BigDecimal in parquet not working**

When trying to save a DDF to parquet file I get the following errror:

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 311, localhost): java.lang.ClassCastException: scala.runtime.BoxedUnit cannot be cast to org.apache.spark.sql.types.Decimal
	at org.apache.spark.sql.parquet.RowWriteSupport.writePrimitive(ParquetTableSupport.scala:220)
	at org.apache.spark.sql.parquet.RowWriteSupport.writeValue(ParquetTableSupport.scala:192)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:171)
	at org.apache.spark.sql.parquet.RowWriteSupport.write(ParquetTableSupport.scala:134)
	at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
	at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$writeShard$1(newParquet.scala:671)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:689)
	at org.apache.spark.sql.parquet.ParquetRelation2$$anonfun$insert$2.apply(newParquet.scala:689)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1204)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1193)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1192)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1192)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1393)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1354)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)

I cannot save the dataframe. Please help.


---

* [SPARK-8271](https://issues.apache.org/jira/browse/SPARK-8271) | *Major* | **string function: soundex**

soundex(string A): string

Returns soundex code of the string. For example, soundex('Miller') results in M460.


---

* [SPARK-8270](https://issues.apache.org/jira/browse/SPARK-8270) | *Major* | **string function: levenshtein**

levenshtein(string A, string B): int

Returns the Levenshtein distance between two strings (as of Hive 1.2.0). For example, levenshtein('kitten', 'sitting') results in 3.


---

* [SPARK-8269](https://issues.apache.org/jira/browse/SPARK-8269) | *Major* | **string function: initcap**

initcap(string A): string

Returns string, with the first letter of each word in uppercase, all other letters in lowercase. Words are delimited by whitespace.


---

* [SPARK-8268](https://issues.apache.org/jira/browse/SPARK-8268) | *Major* | **string function: unbase64**

unbase64(string str): binary

Converts the argument from a base 64 string to BINARY.


---

* [SPARK-8267](https://issues.apache.org/jira/browse/SPARK-8267) | *Major* | **string function: trim**

trim(string A): string

Returns the string resulting from trimming spaces from both ends of A. For example, trim(' foobar ') results in 'foobar'


---

* [SPARK-8266](https://issues.apache.org/jira/browse/SPARK-8266) | *Minor* | **string function: translate**

translate(string\|char\|varchar input, string\|char\|varchar from, string\|char\|varchar to): string

Translates the input string by replacing the characters present in the from string with the corresponding characters in the to string. This is similar to the translate function in PostgreSQL. If any of the parameters to this UDF are NULL, the result is NULL as well.


---

* [SPARK-8265](https://issues.apache.org/jira/browse/SPARK-8265) | *Minor* | **Add LinearDataGenerator to pyspark.mllib.utils**

This is useful in testing various linear models in pyspark


---

* [SPARK-8264](https://issues.apache.org/jira/browse/SPARK-8264) | *Major* | **string function: substring\_index**

substring\_index(string A, string delim, int count): string



Returns the substring from string A before count occurrences of the delimiter delim (as of Hive 1.3.0). If count is positive, everything to the left of the final delimiter (counting from the left) is returned. If count is negative, everything to the right of the final delimiter (counting from the right) is returned. Substring\_index performs a case-sensitive match when searching for delim. Example: substring\_index('www.apache.org', '.', 2) = 'www.apache'.


---

* [SPARK-8263](https://issues.apache.org/jira/browse/SPARK-8263) | *Minor* | **string function: substr/substring should also support binary type**

See Hive's: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF


---

* [SPARK-8262](https://issues.apache.org/jira/browse/SPARK-8262) | *Major* | **string function: split**

split(string str, string pat): array[string]

Splits str around pat (pat is a regular expression).


---

* [SPARK-8261](https://issues.apache.org/jira/browse/SPARK-8261) | *Major* | **string function: space**

space(int n): string

Returns a string of n spaces.


---

* [SPARK-8260](https://issues.apache.org/jira/browse/SPARK-8260) | *Major* | **string function: rtrim**

rtrim(string A): string

Returns the string resulting from trimming spaces from the end(right hand side) of A. For example, rtrim(' foobar ') results in ' foobar'.


---

* [SPARK-8259](https://issues.apache.org/jira/browse/SPARK-8259) | *Major* | **string function: rpad**

rpad(string str, int len, string pad): string

Returns str, right-padded with pad to a length of len.


---

* [SPARK-8258](https://issues.apache.org/jira/browse/SPARK-8258) | *Major* | **string function: reverse**

reverse(string A): string

Returns the reversed string.


---

* [SPARK-8257](https://issues.apache.org/jira/browse/SPARK-8257) | *Major* | **string function: repeat**

repeat(string str, int n): string

Repeats str n times.


---

* [SPARK-8256](https://issues.apache.org/jira/browse/SPARK-8256) | *Major* | **string function: regexp\_replace**

regexp\_replace(string INITIAL\_STRING, string PATTERN, string REPLACEMENT): string

Returns the string resulting from replacing all substrings in INITIAL\_STRING that match the java regular expression syntax defined in PATTERN with instances of REPLACEMENT. For example, regexp\_replace("foobar", "oo\|ar", "") returns 'fb.' Note that some care is necessary in using predefined character classes: using '\s' as the second argument will match the letter s; '\\s' is necessary to match whitespace, etc.


---

* [SPARK-8255](https://issues.apache.org/jira/browse/SPARK-8255) | *Major* | **string function: regexp\_extract**

regexp\_extract(string subject, string pattern, int index): string

Returns the string extracted using the pattern. For example, regexp\_extract('foothebar', 'foo(.\*?)(bar)', 2) returns 'bar.' Note that some care is necessary in using predefined character classes: using '\s' as the second argument will match the letter s; '\\s' is necessary to match whitespace, etc. The 'index' parameter is the Java regex Matcher group() method index. See docs/api/java/util/regex/Matcher.html for more information on the 'index' or Java regex group() method.


---

* [SPARK-8254](https://issues.apache.org/jira/browse/SPARK-8254) | *Major* | **string function: printf**

printf(String format, Obj... args): string

Returns the input formatted according do printf-style format strings.


We need to come up with a name for this in DataFrame -- maybe formatString.


---

* [SPARK-8253](https://issues.apache.org/jira/browse/SPARK-8253) | *Major* | **string function: ltrim**

ltrim(string A): string

Returns the string resulting from trimming spaces from the beginning(left hand side) of A. For example, ltrim(' foobar ') results in 'foobar '.


---

* [SPARK-8252](https://issues.apache.org/jira/browse/SPARK-8252) | *Major* | **string function: lpad**

lpad(string str, int len, string pad): string

Returns str, left-padded with pad to a length of len.


---

* [SPARK-8251](https://issues.apache.org/jira/browse/SPARK-8251) | *Major* | **string function: alias upper / ucase**

Alias upper / ucase in FunctionRegistry.


---

* [SPARK-8250](https://issues.apache.org/jira/browse/SPARK-8250) | *Major* | **string function: alias lower/lcase**

Alias lower/lcase in FunctionRegistry.


---

* [SPARK-8249](https://issues.apache.org/jira/browse/SPARK-8249) | *Major* | **string function: locate**

locate(string substr, string str[, int pos]): int

Returns the position of the first occurrence of substr in str after position pos.


---

* [SPARK-8248](https://issues.apache.org/jira/browse/SPARK-8248) | *Major* | **string function: length**

length(string A): int

Returns the length of the string.


---

* [SPARK-8247](https://issues.apache.org/jira/browse/SPARK-8247) | *Major* | **string function: instr**

instr(string str, string substr): int 

Returns the position of the first occurrence of substr in str. Returns null if either of the arguments are null and returns 0 if substr could not be found in str. Be aware that this is not zero based. The first character in str has index 1.


---

* [SPARK-8246](https://issues.apache.org/jira/browse/SPARK-8246) | *Major* | **string function: get\_json\_object**

get\_json\_object(string json\_string, string path): string

This is actually fairly complicated. Take a look at https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF


Only add this to SQL, not DataFrame.


---

* [SPARK-8245](https://issues.apache.org/jira/browse/SPARK-8245) | *Major* | **string function: format\_number**

format\_number(number x, int d): string

Formats the number X to a format like '#,###,###.##', rounded to D decimal places, and returns the result as a string. If D is 0, the result has no decimal point or fractional part. (As of Hive 0.10.0; bug with float types fixed in Hive 0.14.0, decimal type support added in Hive 0.14.0)


---

* [SPARK-8244](https://issues.apache.org/jira/browse/SPARK-8244) | *Minor* | **string function: find\_in\_set**

find\_in\_set(string str, string strList): int

Returns the first occurance of str in strList where strList is a comma-delimited string. Returns null if either argument is null. Returns 0 if the first argument contains any commas. For example, find\_in\_set('ab', 'abc,b,ab,c,def') returns 3.

Only add this to SQL, not DataFrame.


---

* [SPARK-8243](https://issues.apache.org/jira/browse/SPARK-8243) | *Major* | **string function: encode**

encode(string src, string charset): binary

Encodes the first argument into a BINARY using the provided character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16'). If either argument is null, the result will also be null. (As of Hive 0.12.0.)


---

* [SPARK-8242](https://issues.apache.org/jira/browse/SPARK-8242) | *Major* | **string function: decode**

decode(binary bin, string charset): string

Decodes the first argument into a String using the provided character set (one of 'US-ASCII', 'ISO-8859-1', 'UTF-8', 'UTF-16BE', 'UTF-16LE', 'UTF-16'). If either argument is null, the result will also be null. (As of Hive 0.12.0.)


---

* [SPARK-8241](https://issues.apache.org/jira/browse/SPARK-8241) | *Major* | **string function: concat\_ws**

concat\_ws(string SEP, string A, string B...): string

concat\_ws(string SEP, array\<string\>): string


---

* [SPARK-8240](https://issues.apache.org/jira/browse/SPARK-8240) | *Major* | **string function: concat**

concat(string\|binary A, string\|binary B...): string / binary

Returns the string or bytes resulting from concatenating the strings or bytes passed in as parameters in order. For example, concat('foo', 'bar') results in 'foobar'. Note that this function can take any number of input strings.


---

* [SPARK-8239](https://issues.apache.org/jira/browse/SPARK-8239) | *Major* | **string function: base64**

base64(binary bin): string

Converts the argument from binary to a base 64 string


---

* [SPARK-8238](https://issues.apache.org/jira/browse/SPARK-8238) | *Major* | **string function: ascii**

ascii(string str): int

Returns the numeric value of the first character of str.


---

* [SPARK-8237](https://issues.apache.org/jira/browse/SPARK-8237) | *Major* | **misc function: sha2**

sha2(string/binary, int): string

Calculates the SHA-2 family of hash functions (SHA-224, SHA-256, SHA-384, and SHA-512) (as of Hive 1.3.0). The first argument is the string or binary to be hashed. The second argument indicates the desired bit length of the result, which must have a value of 224, 256, 384, 512, or 0 (which is equivalent to 256). SHA-224 is supported starting from Java 8. If either argument is NULL or the hash length is not one of the permitted values, the return value is NULL. Example: sha2('ABC', 256) = 'b5d4045c3f466fa91fe2cc6abe79232a1a57cdf104f7a26e716e0a1e2789df78'.


---

* [SPARK-8236](https://issues.apache.org/jira/browse/SPARK-8236) | *Major* | **misc function: crc32**

crc32(string/binary): bigint

Computes a cyclic redundancy check value for string or binary argument and returns bigint value (as of Hive 1.3.0). Example: crc32('ABC') = 2743272264.


---

* [SPARK-8235](https://issues.apache.org/jira/browse/SPARK-8235) | *Major* | **misc function: sha1 / sha**

sha1(string/binary): string
sha(string/binary): string


Calculates the SHA-1 digest for string or binary and returns the value as a hex string (as of Hive 1.3.0). Example: sha1('ABC') = '3c01bdbb26f358bab27f267924aa2c9a03fcfdb8'.


---

* [SPARK-8234](https://issues.apache.org/jira/browse/SPARK-8234) | *Major* | **misc function: md5**

md5(string/binary): string

Calculates an MD5 128-bit checksum for the string or binary (as of Hive 1.3.0). The value is returned as a string of 32 hex digits, or NULL if the argument was NULL. Example: md5('ABC') = '902fbdd2b1df0c4f70b4a5d23525e932'.


---

* [SPARK-8232](https://issues.apache.org/jira/browse/SPARK-8232) | *Major* | **complex function: sort\_array**

sort\_array(Array\<T\>)

Sorts the input array in ascending order according to the natural ordering of the array elements and returns it


---

* [SPARK-8231](https://issues.apache.org/jira/browse/SPARK-8231) | *Major* | **complex function: array\_contains**

array\_contains(Array\<T\>, value)

Returns TRUE if the array contains value.


---

* [SPARK-8230](https://issues.apache.org/jira/browse/SPARK-8230) | *Major* | **complex function: size**

size(Map\<K.V\>): int

size(Array\<T\>): int

return the number of elements in the map or array.


---

* [SPARK-8229](https://issues.apache.org/jira/browse/SPARK-8229) | *Major* | **conditional function: isnotnull**

Just need to register it in the FunctionRegistry.


---

* [SPARK-8228](https://issues.apache.org/jira/browse/SPARK-8228) | *Major* | **conditional function: isnull**

Just need to register it in FunctionRegistry.


---

* [SPARK-8227](https://issues.apache.org/jira/browse/SPARK-8227) | *Major* | **math function: unhex**

unhex(STRING a): BINARY

Inverse of hex. Interprets each pair of characters as a hexadecimal number and converts to the byte representation of the number.


---

* [SPARK-8226](https://issues.apache.org/jira/browse/SPARK-8226) | *Major* | **math function: shiftrightunsigned**

shiftrightunsigned(INT a), shiftrightunsigned(BIGINT a)	

Bitwise unsigned right shift (as of Hive 1.2.0). Returns int for tinyint, smallint and int a. Returns bigint for bigint a.


---

* [SPARK-8225](https://issues.apache.org/jira/browse/SPARK-8225) | *Major* | **math function: alias sign / signum**

Alias them in FunctionRegistry.


---

* [SPARK-8224](https://issues.apache.org/jira/browse/SPARK-8224) | *Major* | **math function: shiftright**

shiftrightunsigned(INT a), shiftrightunsigned(BIGINT a)	

Bitwise unsigned right shift (as of Hive 1.2.0). Returns int for tinyint, smallint and int a. Returns bigint for bigint a.


---

* [SPARK-8223](https://issues.apache.org/jira/browse/SPARK-8223) | *Major* | **math function: shiftleft**

shiftleft(INT a)
shiftleft(BIGINT a)

Bitwise left shift (as of Hive 1.2.0). Returns int for tinyint, smallint and int a. Returns bigint for bigint a.


---

* [SPARK-8222](https://issues.apache.org/jira/browse/SPARK-8222) | *Major* | **math function: alias power / pow**

Add to FunctionRegistry power.


---

* [SPARK-8221](https://issues.apache.org/jira/browse/SPARK-8221) | *Major* | **math function: pmod**

pmod(INT a, INT b): INT
pmod(DOUBLE a, DOUBLE b): DOUBLE


Returns the positive value of a mod b.


---

* [SPARK-8220](https://issues.apache.org/jira/browse/SPARK-8220) | *Major* | **math function: positive**

positive(INT a): INT
positive(DOUBLE a): DOUBLE

This is really just an identify function. We should create an Identity expression, and then in the optimizer just removes the Identity functions.


---

* [SPARK-8219](https://issues.apache.org/jira/browse/SPARK-8219) | *Major* | **math function: negative**

This is just an alias for UnaryMinus. Only add it to FunctionRegistry, and not DataFrame.


---

* [SPARK-8218](https://issues.apache.org/jira/browse/SPARK-8218) | *Major* | **math function: log**

log(DOUBLE base, DOUBLE a): DOUBLE

Returns the base-base logarithm of the argument a.


---

* [SPARK-8217](https://issues.apache.org/jira/browse/SPARK-8217) | *Major* | **math function: log2**

log2(double a): double

Returns the base-2 logarithm of the argument a.


---

* [SPARK-8216](https://issues.apache.org/jira/browse/SPARK-8216) | *Major* | **math function: rename log -\> ln**

Rename expression Log -\> Ln.

Also create aliased data frame functions, and update FunctionRegistry.


---

* [SPARK-8215](https://issues.apache.org/jira/browse/SPARK-8215) | *Major* | **math function: pi**

pi(): double

Returns the value of pi. We should make sure foldable = true so it gets folded by the optimizer.


---

* [SPARK-8214](https://issues.apache.org/jira/browse/SPARK-8214) | *Major* | **math function: hex**

hex(BIGINT a): string
hex(STRING a): string
hex(BINARY a): string

If the argument is an INT or binary, hex returns the number as a STRING in hexadecimal format. Otherwise if the number is a STRING, it converts each character into its hexadecimal representation and returns the resulting STRING. (See http://dev.mysql.com/doc/refman/5.0/en/string-functions.html#function\_hex, BINARY version as of Hive 0.12.0.)


---

* [SPARK-8213](https://issues.apache.org/jira/browse/SPARK-8213) | *Major* | **math function: factorial**

factorial(INT a): long

Returns the factorial of a (as of Hive 1.2.0). Valid a is [0..20].


---

* [SPARK-8212](https://issues.apache.org/jira/browse/SPARK-8212) | *Major* | **math function: e**

e(): double

Returns the value of e.


We should make this foldable so it gets folded by the optimizer.


---

* [SPARK-8211](https://issues.apache.org/jira/browse/SPARK-8211) | *Major* | **math function: radians**

Alias toRadians -\> radians in FunctionRegistry.


---

* [SPARK-8210](https://issues.apache.org/jira/browse/SPARK-8210) | *Major* | **math function: degrees**

Alias todegrees -\> degrees.


---

* [SPARK-8209](https://issues.apache.org/jira/browse/SPARK-8209) | *Major* | **math function: conv**

conv(BIGINT num, INT from\_base, INT to\_base): string
conv(STRING num, INT from\_base, INT to\_base): string

Converts a number from a given base to another (see http://dev.mysql.com/doc/refman/5.0/en/mathematical-functions.html#function\_conv).


---

* [SPARK-8208](https://issues.apache.org/jira/browse/SPARK-8208) | *Major* | **math function: ceiling**

We already have ceil -- just need to create an alias for it in FunctionRegistry.


---

* [SPARK-8206](https://issues.apache.org/jira/browse/SPARK-8206) | *Major* | **math function: round**

round(double a): double
Returns the rounded BIGINT value of a.

round(double a, INT d): double
Returns a rounded to d decimal places.


---

* [SPARK-8205](https://issues.apache.org/jira/browse/SPARK-8205) | *Major* | **conditional function: nvl**

nvl(T value, T default\_value): T

Returns default value if value is null else returns value (as of HIve 0.11).

We already have this (called Coalesce). Just need to register an alias for it in FunctionRegistry.


---

* [SPARK-8204](https://issues.apache.org/jira/browse/SPARK-8204) | *Major* | **conditional function: least**

least(T v1, T v2, ...): T

Returns the least value of the list of values (as of Hive 1.1.0).


---

* [SPARK-8203](https://issues.apache.org/jira/browse/SPARK-8203) | *Major* | **conditional functions: greatest**

greatest(T v1, T v2, ...): T

Returns the greatest value of the list of values (as of Hive 1.1.0).


---

* [SPARK-8202](https://issues.apache.org/jira/browse/SPARK-8202) | *Critical* | **PySpark: infinite loop during external sort**

The batch size during external sort will grow up to max 10000, then shrink down to zero, causing infinite loop.

Given the assumption that the items usually have similar size, so we don't need to adjust the batch size after first spill.


---

* [SPARK-8201](https://issues.apache.org/jira/browse/SPARK-8201) | *Major* | **conditional function: if**

We already have an If expression. Just need to register it in FunctionRegistry.


---

* [SPARK-8200](https://issues.apache.org/jira/browse/SPARK-8200) | *Minor* | **Exception in StreamingLinearAlgorithm on Stream with Empty RDD.**

When training a streaming logistic regression model or a streaming linear regression model, any empty RDDs in a stream will cause an exception.

  java.lang.UnsupportedOperationException: empty collection
  at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1288)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
  at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:109)
  at org.apache.spark.rdd.RDD.withScope(RDD.scala:286)
  at org.apache.spark.rdd.RDD.first(RDD.scala:1285)
  at org.apache.spark.mllib.regression.GeneralizedLinearAlgorithm.run(GeneralizedLinearAlgorithm.scala:215)
  at org.apache.spark.mllib.regression.StreamingLinearAlgorithm$$anonfun$trainOn$1.apply(StreamingLinearAlgorithm.scala:91)
  at org.apache.spark.mllib.regression.StreamingLinearAlgorithm$$anonfun$trainOn$1.apply(StreamingLinearAlgorithm.scala:85)
  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ForEachDStream.scala:42)
  at org.apache.spark.streaming.dstream.ForEachDStream$$anonfun$1$$anonfun$apply$mcV$sp$1.apply(ForEachDStream.scala:40)


---

* [SPARK-8199](https://issues.apache.org/jira/browse/SPARK-8199) | *Major* | **date/time function: date\_format**

date\_format(date/timestamp/string ts, string fmt): string

Converts a date/timestamp/string to a value of string in the format specified by the date format fmt (as of Hive 1.2.0). Supported formats are Java SimpleDateFormat formats  https://docs.oracle.com/javase/7/docs/api/java/text/SimpleDateFormat.html. The second argument fmt should be constant. Example: date\_format('2015-04-08', 'y') = '2015'.
date\_format can be used to implement other UDFs, e.g.:
dayname(date) is date\_format(date, 'EEEE')
dayofyear(date) is date\_format(date, 'D')


---

* [SPARK-8198](https://issues.apache.org/jira/browse/SPARK-8198) | *Major* | **date/time function: months\_between**

months\_between(date1, date2): double

Returns number of months between dates date1 and date2 (as of Hive 1.2.0). If date1 is later than date2, then the result is positive. If date1 is earlier than date2, then the result is negative. If date1 and date2 are either the same days of the month or both last days of months, then the result is always an integer. Otherwise the UDF calculates the fractional portion of the result based on a 31-day month and considers the difference in time components date1 and date2. date1 and date2 type can be date, timestamp or string in the format 'yyyy-MM-dd' or 'yyyy-MM-dd HH:mm:ss'. The result is rounded to 8 decimal places. Example: months\_between('1997-02-28 10:30:00', '1996-10-30') = 3.94959677


---

* [SPARK-8197](https://issues.apache.org/jira/browse/SPARK-8197) | *Major* | **date/time function: trunc**

trunc(string date[, string format]): string

trunc(date date[, string format]): date

Returns date truncated to the unit specified by the format (as of Hive 1.2.0). Supported formats: MONTH/MON/MM, YEAR/YYYY/YY. If format is omitted the date will be truncated to the nearest day. Example: trunc('2015-03-17', 'MM') = 2015-03-01.


---

* [SPARK-8196](https://issues.apache.org/jira/browse/SPARK-8196) | *Major* | **date/time function: next\_day**

next\_day(string start\_date, string day\_of\_week): string

next\_day(date start\_date, string day\_of\_week): string

Returns the first date which is later than start\_date and named as day\_of\_week (as of Hive 1.2.0). start\_date is a string/date/timestamp. day\_of\_week is 2 letters, 3 letters or full name of the day of the week (e.g. Mo, tue, FRIDAY). The time part of start\_date is ignored. Example: next\_day('2015-01-14', 'TU') = 2015-01-20.


---

* [SPARK-8195](https://issues.apache.org/jira/browse/SPARK-8195) | *Major* | **date/time function: last\_day**

last\_day(string date): string

last\_day(date date): date

Returns the last day of the month which the date belongs to (as of Hive 1.1.0). date is a string in the format 'yyyy-MM-dd HH:mm:ss' or 'yyyy-MM-dd'. The time part of date is ignored.


---

* [SPARK-8194](https://issues.apache.org/jira/browse/SPARK-8194) | *Major* | **date/time function: add\_months**

add\_months(string start\_date, int num\_months): string

add\_months(date start\_date, int num\_months): date

Returns the date that is num\_months after start\_date. The time part of start\_date is ignored. If start\_date is the last day of the month or if the resulting month has fewer days than the day component of start\_date, then the result is the last day of the resulting month. Otherwise, the result has the same day component as start\_date.


---

* [SPARK-8193](https://issues.apache.org/jira/browse/SPARK-8193) | *Major* | **date/time function: current\_timestamp**

current\_timestamp(): timestamp

Returns the current timestamp at the start of query evaluation (as of Hive 1.2.0). All calls of current\_timestamp within the same query return the same value.


We should just replace this with a timestamp literal in the optimizer.


---

* [SPARK-8192](https://issues.apache.org/jira/browse/SPARK-8192) | *Major* | **date/time function: current\_date**

current\_date(): date

Returns the current date at the start of query evaluation (as of Hive 1.2.0). All calls of current\_date within the same query return the same value.

We should just replace this with a date literal in the optimizer.


---

* [SPARK-8191](https://issues.apache.org/jira/browse/SPARK-8191) | *Major* | **date/time function: to\_utc\_timestamp**

to\_utc\_timestamp(timestamp, string timezone): timestamp

Assumes given timestamp is in given timezone and converts to UTC (as of Hive 0.8.0). For example, to\_utc\_timestamp('1970-01-01 00:00:00','PST') returns 1970-01-01 08:00:00.


---

* [SPARK-8189](https://issues.apache.org/jira/browse/SPARK-8189) | *Major* | **Use 100ns precision for TimestampType**

100ns means we only need 8 bytes to represent a Timestamp.


---

* [SPARK-8188](https://issues.apache.org/jira/browse/SPARK-8188) | *Major* | **date/time function: from\_utc\_timestamp**

from\_utc\_timestamp(timestamp, string timezone): timestamp

Assumes given timestamp is UTC and converts to given timezone (as of Hive 0.8.0). For example, from\_utc\_timestamp('1970-01-01 08:00:00','PST') returns 1970-01-01 00:00:00.


---

* [SPARK-8187](https://issues.apache.org/jira/browse/SPARK-8187) | *Major* | **date/time function: date\_sub**

date\_sub(timestamp startdate, int days): timestamp
date\_sub(timestamp startdate, interval i): timestamp
date\_sub(date date, int days): date
date\_sub(date date, interval i): date


---

* [SPARK-8186](https://issues.apache.org/jira/browse/SPARK-8186) | *Major* | **date/time function: date\_add**

date\_add(timestamp startdate, int days): timestamp
date\_add(timestamp startdate, interval i): timestamp
date\_add(date date, int days): date
date\_add(date date, interval i): date


---

* [SPARK-8185](https://issues.apache.org/jira/browse/SPARK-8185) | *Major* | **date/time function: datediff**

datediff(date enddate, date startdate): int

Returns the number of days from startdate to enddate: datediff('2009-03-01', '2009-02-27') = 2.


See https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF


---

* [SPARK-8184](https://issues.apache.org/jira/browse/SPARK-8184) | *Major* | **date/time function: weekofyear**

weekofyear(string\|date\|timestamp): int

Returns the week number of a timestamp string: weekofyear("1970-11-01 00:00:00") = 44, weekofyear("1970-11-01") = 44.


---

* [SPARK-8183](https://issues.apache.org/jira/browse/SPARK-8183) | *Major* | **date/time function: second**

second(string\|date\|timestamp): int

Returns the second of the timestamp.

See https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF


---

* [SPARK-8182](https://issues.apache.org/jira/browse/SPARK-8182) | *Major* | **date/time function: minute**

minute(string\|date\|timestamp): int

Returns the minute of the timestamp.

See https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF


---

* [SPARK-8181](https://issues.apache.org/jira/browse/SPARK-8181) | *Major* | **date/time function: hour**

hour(string\|date\|timestamp): int

Returns the hour of the timestamp: hour('2009-07-30 12:58:59') = 12, hour('12:58:59') = 12.


---

* [SPARK-8180](https://issues.apache.org/jira/browse/SPARK-8180) | *Major* | **date/time function: day / dayofmonth**

day(string\|date\|timestamp): int
dayofmonth(string\|date\|timestamp): int

Returns the day part of a date or a timestamp string: day("1970-11-01 00:00:00") = 1, day("1970-11-01") = 1.

See https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF


---

* [SPARK-8179](https://issues.apache.org/jira/browse/SPARK-8179) | *Major* | **date/time function: month**

month(string\|date\|timestamp): int

Returns the month part of a date or a timestamp string: month("1970-11-01 00:00:00") = 11, month("1970-11-01") = 11.

See https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF


---

* [SPARK-8178](https://issues.apache.org/jira/browse/SPARK-8178) | *Major* | **date/time function: quarter**

quarter(timestamp): int

Returns the quarter of the year for a date, timestamp, or string in the range 1 to 4. Example: quarter('2015-04-08') = 2.

Note that through implicit type casts, we can support date/string type as well.


---

* [SPARK-8177](https://issues.apache.org/jira/browse/SPARK-8177) | *Major* | **date/time function: year**


{code}
year(timestamp time): int
{code}

Returns the year part of a date or a timestamp string: year("1970-01-01 00:00:00") = 1970, year("1970-01-01") = 1970.


---

* [SPARK-8176](https://issues.apache.org/jira/browse/SPARK-8176) | *Major* | **date/time function: to\_date**

parse a timestamp string and return the date portion
{code}
to\_date(string timestamp): date
{code}

Returns the date part of a timestamp string: to\_date("1970-01-01 00:00:00") = "1970-01-01" (in some date format)


---

* [SPARK-8175](https://issues.apache.org/jira/browse/SPARK-8175) | *Major* | **date/time function: from\_unixtime**

from\_unixtime(bigint unixtime[, string format]): string

Converts the number of seconds from unix epoch (1970-01-01 00:00:00 UTC) to a string representing the timestamp of that moment in the current system time zone in the format of "1970-01-01 00:00:00".

See https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF


---

* [SPARK-8174](https://issues.apache.org/jira/browse/SPARK-8174) | *Blocker* | **date/time function: unix\_timestamp**

3 variants:

{code}
unix\_timestamp(): long
Gets current Unix timestamp in seconds.

unix\_timestamp(string\|date): long
Converts time string in format yyyy-MM-dd HH:mm:ss to Unix timestamp (in seconds), using the default timezone and the default locale, return 0 if fail: unix\_timestamp('2009-03-20 11:30:01') = 1237573801


unix\_timestamp(string date, string pattern): long
Convert time string with given pattern (see [http://docs.oracle.com/javase/tutorial/i18n/format/simpleDateFormat.html]) to Unix time stamp (in seconds), return 0 if fail: unix\_timestamp('2009-03-20', 'yyyy-MM-dd') = 1237532400.
{code}

See: https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF


---

* [SPARK-8169](https://issues.apache.org/jira/browse/SPARK-8169) | *Major* | **Add StopWordsRemover as a transformer**

StopWordsRemover takes a string array column and outputs a string array column with all defined stop words removed. The transformer should also come with a standard set of stop words as default.

{code}
val stopWords = new StopWordsRemover()
  .setInputCol("words")
  .setOutputCol("cleanWords")
  .setStopWords(Array(...)) // optional
val output = stopWords.transform(df)
{code}


---

* [SPARK-8168](https://issues.apache.org/jira/browse/SPARK-8168) | *Major* | **Add Python friendly constructor to PipelineModel**

We are trying to migrate all Python implementations of Pipeline components to Scala. As part of this effort, PipelineModel should have a Python-friendly constructor.


---

* [SPARK-8162](https://issues.apache.org/jira/browse/SPARK-8162) | *Blocker* | **Run spark-shell cause NullPointerException**

run spark-shell on latest master branch, then failed, details are:

{noformat}
Welcome to
      \_\_\_\_              \_\_
     / \_\_/\_\_  \_\_\_ \_\_\_\_\_/ /\_\_
    \_\ \/ \_ \/ \_ `/ \_\_/  '\_/
   /\_\_\_/ .\_\_/\\_,\_/\_/ /\_/\\_\   version 1.5.0-SNAPSHOT
      /\_/

Using Scala version 2.10.4 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0\_40)
Type in expressions to have them evaluated.
Type :help for more information.
error: error while loading JobProgressListener, Missing dependency 'bad symbolic reference. A signature in JobProgressListener.class refers to term annotations
in package com.google.common which is not available.
It may be completely missing from the current classpath, or the version on
the classpath might be incompatible with the version used when compiling JobProgressListener.class.', required by /opt/apache/spark/lib/spark-assembly-1.5.0-SNAPSHOT-hadoop2.7.0.jar(org/apache/spark/ui/jobs/JobProgressListener.class)
java.lang.NullPointerException
	at org.apache.spark.sql.SQLContext.\<init\>(SQLContext.scala:193)
	at org.apache.spark.sql.hive.HiveContext.\<init\>(HiveContext.scala:68)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:422)
	at org.apache.spark.repl.SparkILoop.createSQLContext(SparkILoop.scala:1028)
	at $iwC$$iwC.\<init\>(\<console\>:9)
	at $iwC.\<init\>(\<console\>:18)
	at \<init\>(\<console\>:20)
	at .\<init\>(\<console\>:24)
	at .\<clinit\>(\<console\>)
	at .\<init\>(\<console\>:7)
	at .\<clinit\>(\<console\>)
	at $print(\<console\>)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
	at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)
	at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
	at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
	at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
	at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
	at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:130)
	at org.apache.spark.repl.SparkILoopInit$$anonfun$initializeSpark$1.apply(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:324)
	at org.apache.spark.repl.SparkILoopInit$class.initializeSpark(SparkILoopInit.scala:122)
	at org.apache.spark.repl.SparkILoop.initializeSpark(SparkILoop.scala:64)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1$$anonfun$apply$mcZ$sp$5.apply$mcV$sp(SparkILoop.scala:974)
	at org.apache.spark.repl.SparkILoopInit$class.runThunks(SparkILoopInit.scala:157)
	at org.apache.spark.repl.SparkILoop.runThunks(SparkILoop.scala:64)
	at org.apache.spark.repl.SparkILoopInit$class.postInitialization(SparkILoopInit.scala:106)
	at org.apache.spark.repl.SparkILoop.postInitialization(SparkILoop.scala:64)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:991)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
	at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
	at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
	at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
	at org.apache.spark.repl.Main$.main(Main.scala:31)
	at org.apache.spark.repl.Main.main(Main.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:497)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:663)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:169)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:192)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:111)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

\<console\>:10: error: not found: value sqlContext
       import sqlContext.implicits.\_
              ^
\<console\>:10: error: not found: value sqlContext
       import sqlContext.sql
              ^
{noformat}

JDK: 1.8.0\_40
Hadoop: 2.7.0


---

* [SPARK-8161](https://issues.apache.org/jira/browse/SPARK-8161) | *Major* | **externalBlockStoreInitialized is never set to be true**

externalBlockStoreInitialized is never set to be true, which causes the blocks stored in ExternalBlockStore can not be removed.


---

* [SPARK-8160](https://issues.apache.org/jira/browse/SPARK-8160) | *Major* | **Tungsten style external aggregation**

Support using external sorting to run aggregate so we can easily process aggregates where each partition is much larger than memory size.


---

* [SPARK-8159](https://issues.apache.org/jira/browse/SPARK-8159) | *Major* | **Improve expression function coverage (Spark 1.5)**

This is an umbrella ticket to track new expressions we are adding to SQL/DataFrame.

For each new expression, we should:
1. Add a new Expression implementation in org.apache.spark.sql.catalyst.expressions
2. If applicable, implement the code generated version (by implementing genCode).
3. Add comprehensive unit tests (for all the data types the expressions support).
4. If applicable, add a new function for DataFrame in org.apache.spark.sql.functions, and python/pyspark/sql/functions.py for Python.

For date/time functions, put them in expressions/datetime.scala, and create a DateTimeFunctionSuite.scala for testing.


---

* [SPARK-8158](https://issues.apache.org/jira/browse/SPARK-8158) | *Major* | **HiveShim improvement**

1. explicitly import implicit conversion support.
2. use .nonEmpty instead of .size \> 0
3. use val instead of var
4. comment indention


---

* [SPARK-8154](https://issues.apache.org/jira/browse/SPARK-8154) | *Major* | **Remove Term/Code type aliases in code generation**

From my perspective as a code reviewer, I find them more confusing than using String directly.


---

* [SPARK-8151](https://issues.apache.org/jira/browse/SPARK-8151) | *Blocker* | **Pipeline components should correctly implement copy**

Some pipeline components (models and meta-algorithms) should correctly implement copy in order to work properly in pipeline fitting.


---

* [SPARK-8149](https://issues.apache.org/jira/browse/SPARK-8149) | *Major* | **Break ExpressionEvaluationSuite down to multiple files**

We need to substantially improve unit test coverage for expressions, and as a result it is not possible to have all expression tests in a single file.


---

* [SPARK-8148](https://issues.apache.org/jira/browse/SPARK-8148) | *Major* | **Do not use FloatType in partition column inference**

Always use DoubleType to be more stable and less error prone.


---

* [SPARK-8146](https://issues.apache.org/jira/browse/SPARK-8146) | *Major* | **DataFrame Python API: Alias replace in DataFrameNaFunctions**

We missed aliasing it in na.replace.


---

* [SPARK-8141](https://issues.apache.org/jira/browse/SPARK-8141) | *Major* | **Precompute datatypes for partition columns and reuse it**

spec.partitionColumns.map(\_.dataType) re-runs for each Partition in HadoopFsRelation. When there are many partitions, it costs too much unnecessary time.


---

* [SPARK-8140](https://issues.apache.org/jira/browse/SPARK-8140) | *Trivial* | **Remove empty model check in StreamingLinearAlgorithm**

1. Prevent creating a map of data to find numFeatures
2. If model is empty, then initialize with a zero vector of numFeatures


---

* [SPARK-8139](https://issues.apache.org/jira/browse/SPARK-8139) | *Minor* | **Documents data sources and Parquet output committer related options**

Should document the following two options:

- {{spark.sql.sources.outputCommitterClass}}
- {{spark.sql.parquet.output.committer.class}}


---

* [SPARK-8138](https://issues.apache.org/jira/browse/SPARK-8138) | *Minor* | **Error message for discovered conflicting partition columns is not intuitive**

For data stored as a Hive-style partitioned table, data files should only live in leaf partition directories.

For example, the following directory layout is illegal:
{noformat}
.
 \_SUCCESS
 b=0
  c=0
   part-r-00004.gz.parquet
  part-r-00004.gz.parquet
 b=1
     c=1
      part-r-00008.gz.parquet
     part-r-00008.gz.parquet
{noformat}
For now, we give an unintuitive error message like this:
{noformat}
Conflicting partition column names detected:
 ArrayBuffer(b, c)
ArrayBuffer(b)
{noformat}
This should be improved.


---

* [SPARK-8136](https://issues.apache.org/jira/browse/SPARK-8136) | *Major* | **AM link download test can be flaky**

Sometimes YARN does not replace the link (or replaces it too soon) causing the YarnClusterSuite to fail. On a real cluster, the NM automatically redirects once the app is complete. So we should make the test less strict and have it only check the link's format rather than try to download the logs.


---

* [SPARK-8135](https://issues.apache.org/jira/browse/SPARK-8135) | *Major* | **Don't load defaults when reconstituting Hadoop Configurations**

Calling "new Configuration()" is an expensive operation because it loads any Hadoop configuration XMLs from disk.

In SerializableWritable, we call new Configuration needlessly when instantiating an ObjectWritable.  The ObjectWritable only needs the Configuration for its class cache, not for any Hadoop properties that might be in XML files, so it should be ok to call new Configuration with loadDefaults = false.


---

* [SPARK-8131](https://issues.apache.org/jira/browse/SPARK-8131) | *Critical* | **Improve Database support**

This is the master jira for tracking the improvement on database support.


---

* [SPARK-8129](https://issues.apache.org/jira/browse/SPARK-8129) | *Minor* | **Securely pass auth secrets to executors in standalone cluster mode**

Currently, when authentication is turned on, the standalone cluster manager passes auth secrets to executors (also drivers in cluster mode) as java options on the command line, which isn't secure. The passed secret can be seen by anyone running 'ps' command, e.g.,


bq.  501 94787 94734   0  2:32PM ??         0:00.78 /Library/Java/JavaVirtualMachines/jdk1.7.0\_60.jdk/Contents/Home/jre/bin/java -cp /Users/kan/github/spark/sbin/../conf/:/Users/kan/github/spark/assembly/target/scala-2.10/spark-assembly-1.4.0-SNAPSHOT-hadoop2.3.0.jar:/Users/kan/github/spark/lib\_managed/jars/datanucleus-api-jdo-3.2.6.jar:/Users/kan/github/spark/lib\_managed/jars/datanucleus-core-3.2.10.jar:/Users/kan/github/spark/lib\_managed/jars/datanucleus-rdbms-3.2.9.jar -Xms512M -Xmx512M \*-Dspark.authenticate.secret=090A030E0F0A05010900000A0C0E0C0B03050D05\* -Dspark.driver.port=49625 -Dspark.authenticate=true -XX:MaxPermSize=128m org.apache.spark.executor.CoarseGrainedExecutorBackend --driver-url akka.tcp://sparkDriver@192.168.1.152:49625/user/CoarseGrainedScheduler --executor-id 0 --hostname 192.168.1.152 --cores 8 --app-id app-20150605143259-0000 --worker-url akka.tcp://sparkWorker@192.168.1.152:49623/user/Worker


---

* [SPARK-8127](https://issues.apache.org/jira/browse/SPARK-8127) | *Minor* | **KafkaRDD optimize count() take() isEmpty()**

KafkaRDD can use offset range to avoid doing extra work

Possibly related to SPARK-7122


---

* [SPARK-8126](https://issues.apache.org/jira/browse/SPARK-8126) | *Minor* | **Use temp directory under build dir for unit tests**

Spark's unit tests leave a lot of garbage in /tmp after a run, making it hard to clean things up. Let's place those files under the build dir so that "mvn\|sbt\|git clean" can do their job.


---

* [SPARK-8125](https://issues.apache.org/jira/browse/SPARK-8125) | *Blocker* | **Accelerate ParquetRelation2 metadata discovery**

For large Parquet tables (e.g., with thousands of partitions), it can be very slow to discover Parquet metadata for schema merging and generating splits for Spark jobs. We need to accelerate this processes. One possible solution is to do the discovery via a distributed Spark job.


---

* [SPARK-8124](https://issues.apache.org/jira/browse/SPARK-8124) | *Minor* | **Created more examples on SparkR DataFrames**

(Components please) https://cwiki.apache.org/confluence/display/SPARK/Contributing+to+Spark


---

* [SPARK-8118](https://issues.apache.org/jira/browse/SPARK-8118) | *Minor* | **Turn off noisy log output produced by Parquet 1.7.0**

Parquet 1.7.0 renames package name to "org.apache.parquet", need to adjust {{ParquetRelation.enableLogForwarding}} accordingly to avoid noisy log output.

A better approach than simply muting these log lines is to redirect Parquet logs via SLF4J, so that we can handle them consistently. In general these logs are very useful. Esp. when used to diagnosing Parquet memory issue and filter push-down.


---

* [SPARK-8117](https://issues.apache.org/jira/browse/SPARK-8117) | *Major* | **Push codegen into Expression**

Push the codegen implementation of expression into Expression itself, make it easy to manage and extend.


---

* [SPARK-8116](https://issues.apache.org/jira/browse/SPARK-8116) | *Minor* | **sc.range() doesn't match python range()**

Python's built-in range() and xrange() functions can take 1, 2, or 3 arguments. Ranges with just 1 argument are probably used the most frequently, e.g.:
for i in range(len(myList)): ...

However, in pyspark, the SparkContext range() method throws an error when called with a single argument, due to the way its arguments get passed into python's range function.

There's no good reason that I can think of not to support the same syntax as the built-in function. To fix this, we can set the default of the sc.range() method's `stop` argument to None, and then inside the method, if it is None, replace `stop` with `start` and set `start` to 0, which is what the c implementation of range() does:
https://github.com/python/cpython/blob/master/Objects/rangeobject.c#L87


---

* [SPARK-8114](https://issues.apache.org/jira/browse/SPARK-8114) | *Major* | **Remove wildcard import on TestSQLContext.\_**

We import TestSQLContext.\_ in almost all test suites. This import introduces a lot of methods and should be avoided.


---

* [SPARK-8106](https://issues.apache.org/jira/browse/SPARK-8106) | *Major* | **Set derby.system.durability=test in order to speed up Hive compatibility tests**

Derby has a configuration property named {{derby.system.durability}} that disables I/O synchronization calls for many writes.  This sacrifices durability but can result in large performance gains, which is appropriate for tests.

We should enable this in our test system properties in order to speed up the Hive compatibility tests.  I saw 2-3x speedups locally with this change.


---

* [SPARK-8105](https://issues.apache.org/jira/browse/SPARK-8105) | *Critical* | **sqlContext.table("databaseName.tableName") broke with SPARK-6908**

Since the introduction of Dataframes in Spark 1.3.0 and prior to SPARK-6908 landing into master, a user could get a DataFrame to a Hive table using `sqlContext.table("databaseName.tableName")` 
Since SPARK-6908, the user now receives a NoSuchTableException.

This amounts to a change in  non experimental sqlContext.table() api and will require user code to be modified to work properly with 1.4.0.

The only viable work around I could find is
`sqlContext.sql("select \* from databseName.tableName")`
which seems like a hack.


---

* [SPARK-8104](https://issues.apache.org/jira/browse/SPARK-8104) | *Major* | **move the auto alias logic into Analyzer**

Currently we auto alias expression in parser. However, during parser phase we don't have enough information to do the right alias. For example, Generator that has more than 1 kind of element need MultiAlias, ExtractValue don't need Alias if it's in middle of a ExtractValue chain.


---

* [SPARK-8103](https://issues.apache.org/jira/browse/SPARK-8103) | *Major* | **DAGScheduler should not launch multiple concurrent attempts for one stage on fetch failures**

When there is a fetch failure, {{DAGScheduler}} is supposed to fail the stage, retry the necessary portions of the preceding shuffle stage which generated the shuffle data, and eventually rerun the stage.  

We generally expect to get multiple fetch failures together, but only want to re-start the stage once.  The code already makes an attempt to address this https://github.com/apache/spark/blob/10ba1880878d0babcdc5c9b688df5458ea131531/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1108 .  

{code}
       // It is likely that we receive multiple FetchFailed for a single stage (because we have
        // multiple tasks running concurrently on different executors). In that case, it is possible
        // the fetch failure has already been handled by the scheduler.
        if (runningStages.contains(failedStage)) {
{code}

However, this logic is flawed because the stage may have been \*\*resubmitted\*\* by the time we get these fetch failures.  In that case, {{runningStages.contains(failedStage)}} will be true, but we've already handled these failures.

This results in multiple concurrent non-zombie attempts for one stage.  In addition to being very confusing, and a waste of resources, this also can lead to later stages being submitted before the previous stage has registered its map output.  This happens because

(a) when one attempt finishes all its tasks, it may not register its map output because the stage still has pending tasks, from other attempts https://github.com/apache/spark/blob/10ba1880878d0babcdc5c9b688df5458ea131531/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L1046

{code}
            if (runningStages.contains(shuffleStage) && shuffleStage.pendingTasks.isEmpty) {
{code}

and (b) {{submitStage}} thinks the following stage is ready to go, because {{getMissingParentStages}} thinks the stage is complete as long it has all of its map outputs: https://github.com/apache/spark/blob/10ba1880878d0babcdc5c9b688df5458ea131531/core/src/main/scala/org/apache/spark/scheduler/DAGScheduler.scala#L397

{code}
                if (!mapStage.isAvailable) {
                  missing += mapStage
                }
{code}


So the following stage is submitted repeatedly, but it is doomed to fail because its shuffle output has never been registered with the map output tracker.  Here's an example failure in this case:
{noformat}
WARN TaskSetManager: Lost task 5.0 in stage 3.2 (TID 294, 192.168.1.104): FetchFailed(null, shuffleId=0, mapId=-1, reduceId=5, message=
org.apache.spark.shuffle.MetadataFetchFailedException: Missing output locations for shuffle ...
{noformat}


Note that this is a subset of the problems originally described in SPARK-7308, limited to just the issues effecting the DAGScheduler


---

* [SPARK-8101](https://issues.apache.org/jira/browse/SPARK-8101) | *Minor* | **Upgrade netty to avoid memory leak accord to netty #3837 issues**

There is a direct buffer leak in netty, due to netty 4.0.23-Final not release threadlocal after netty already send message success.

Please Ref: https://github.com/netty/netty/issues/3837


---

* [SPARK-8099](https://issues.apache.org/jira/browse/SPARK-8099) | *Major* | **In yarn-cluster mode, "--executor-cores" can't be setted into SparkConf**

While testing dynamic executor allocation function, I set the executor cores with \*--executor-cores 4\* in spark-submit command. But in \*ExecutorAllocationManager\*, the \*private val tasksPerExecutor =conf.getInt("spark.executor.cores", 1) / conf.getInt("spark.task.cpus", 1)\* is still to be 1.


---

* [SPARK-8098](https://issues.apache.org/jira/browse/SPARK-8098) | *Minor* | **Show correct length of bytes on log page**

The log page should only show desired length of bytes. Currently it shows bytes from the startIndex to the end of the file. The "Next" button on the page is always disabled.


---

* [SPARK-8095](https://issues.apache.org/jira/browse/SPARK-8095) | *Major* | **Spark package dependencies not resolved when package is in local-ivy-cache**

Given a dependency expressed with '--packages', the transitive dependencies are supposed to be automatically included. This is true for most repository types including local-m2-cache, Spark Packages, and central.   For ivy-local-cache, it is not.


---

* [SPARK-8093](https://issues.apache.org/jira/browse/SPARK-8093) | *Critical* | **Spark 1.4 branch's new JSON schema inference has changed the behavior of handling inner empty JSON object.**

This is similar to SPARK-3365. Sample json is attached. Code to reproduce
{code}
var jsonDF = read.json("/tmp/t1.json")
jsonDF.write.parquet("/tmp/t1.parquet")
{code}

The 'integration' object is empty in the json.
StackTrace:
{code}
....
Caused by: java.io.IOException: Could not read footer: java.lang.IllegalStateException: Cannot build an empty group
	at parquet.hadoop.ParquetFileReader.readAllFootersInParallel(ParquetFileReader.java:238)
	at org.apache.spark.sql.parquet.ParquetRelation2$MetadataCache.refresh(newParquet.scala:369)
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$metadataCache$lzycompute(newParquet.scala:154)
	at org.apache.spark.sql.parquet.ParquetRelation2.org$apache$spark$sql$parquet$ParquetRelation2$$metadataCache(newParquet.scala:152)
	at org.apache.spark.sql.parquet.ParquetRelation2.refresh(newParquet.scala:197)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.insert(commands.scala:134)
	... 69 more
Caused by: java.lang.IllegalStateException: Cannot build an empty group
{code}


---

* [SPARK-8091](https://issues.apache.org/jira/browse/SPARK-8091) | *Major* | **SerializationDebugger does not handle classes with writeObject method**

SerializationDebugger skips testing an object whose class has writeObject(), as it was not trivial to test the serializability all the arbitrary stuff that writeObject() could write.


---

* [SPARK-8090](https://issues.apache.org/jira/browse/SPARK-8090) | *Major* | **SerializationDebugger does not handle classes with writeReplace correctly**

The following class with not serializable object used through writeReplace will not be caught correctly by the SerializationDebugger
{code}
class SerializableClassWithWriteReplace()
  extends Serializable {
  private def writeReplace(): Object = {
    new NotSerializableObjectI()
  }
}
{code}

The reason is that SerializationDebugger does not check the type of the replaced object (whether serializable or not).


---

* [SPARK-8088](https://issues.apache.org/jira/browse/SPARK-8088) | *Major* | **ExecutionAllocationManager spamming INFO logs about "Lowering target number of executors"**

I am running a {{spark-shell}} built at 1.4.0-rc4, with:

{code}
  --conf spark.dynamicAllocation.enabled=true \
  --conf spark.dynamicAllocation.minExecutors=5 \
  --conf spark.dynamicAllocation.maxExecutors=300 \
  --conf spark.dynamicAllocation.schedulerBacklogTimeout=3 \
  --conf spark.dynamicAllocation.executorIdleTimeout=600 \
{code}

I can't really type any commands because I am getting 10 of these per second:

{code}
15/06/03 20:49:09 INFO spark.ExecutorAllocationManager: Lowering target number of executors to 5 because not all requests are actually needed (previously 5)
15/06/03 20:49:09 INFO spark.ExecutorAllocationManager: Lowering target number of executors to 5 because not all requests are actually needed (previously 5)
15/06/03 20:49:09 INFO spark.ExecutorAllocationManager: Lowering target number of executors to 5 because not all requests are actually needed (previously 5)
15/06/03 20:49:09 INFO spark.ExecutorAllocationManager: Lowering target number of executors to 5 because not all requests are actually needed (previously 5)
15/06/03 20:49:09 INFO spark.ExecutorAllocationManager: Lowering target number of executors to 5 because not all requests are actually needed (previously 5)
15/06/03 20:49:09 INFO spark.ExecutorAllocationManager: Lowering target number of executors to 5 because not all requests are actually needed (previously 5)
{code}

It should not print anything if it is not in fact lowering the number of executors / is already at the minimum, right?


---

* [SPARK-8087](https://issues.apache.org/jira/browse/SPARK-8087) | *Blocker* | **PipelineModel.copy didn't copy the stages**

So extra params in transform do not work.


---

* [SPARK-8085](https://issues.apache.org/jira/browse/SPARK-8085) | *Major* | **Pass in user-specified schema in read.df**

This will help cases where we use the CSV reader and want each column to be of a specific type


---

* [SPARK-8084](https://issues.apache.org/jira/browse/SPARK-8084) | *Major* | **SparkR install script should fail with error if any packages required are not found**

This is to avoid cases where the script fails but the build is green

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Package/73/console


---

* [SPARK-8083](https://issues.apache.org/jira/browse/SPARK-8083) | *Major* | **Fix return to drivers link in Mesos driver page**

The current path is set to "/" but this doesn't work with a proxy. We need to prepend the proxy base uri if it's set.


---

* [SPARK-8080](https://issues.apache.org/jira/browse/SPARK-8080) | *Minor* | **Custom Receiver.store with Iterator type do not give correct count at Spark UI**

In Custom receiver if I call store with Iterator type (store(dataIterator: Iterator[T]): Unit ) , Spark UI does not show the correct count of records in block which leads to wrong value for Input Rate, Scheduling Delay and Input SIze.


---

* [SPARK-8079](https://issues.apache.org/jira/browse/SPARK-8079) | *Major* | **NPE when HadoopFsRelation.prepareForWriteJob throws exception**

Take {{ParquetRelation2}} as an example, the following Spark shell code may cause an unexpected NPE:
{code}
import sqlContext.\_
import sqlContext.implicits.\_

range(1, 3).select($"id" as "a b").write.format("parquet").save("file:///tmp/foo")
{code}
Exceptions thrown:
{noformat}
import sqlContext.\_
import sqlContext.implicits.\_

range(1, 3).select($"id" as "a b").write.format("parquet").save("file:///tmp/foo")

java.lang.RuntimeException: Attribute name "a b" contains invalid character(s) among " ,;{}()   =". Please use alias to rename it.
        at scala.sys.package$.error(package.scala:27)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$checkSpecialCharacters$2.apply(ParquetTypes.scala:414)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$$anonfun$checkSpecialCharacters$2.apply(ParquetTypes.scala:412)
        at scala.collection.immutable.List.foreach(List.scala:318)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$.checkSpecialCharacters(ParquetTypes.scala:412)
        at org.apache.spark.sql.parquet.ParquetTypesConverter$.convertToString(ParquetTypes.scala:423)
        at org.apache.spark.sql.parquet.RowWriteSupport$.setSchema(ParquetTableSupport.scala:383)
        at org.apache.spark.sql.parquet.ParquetRelation2.prepareJobForWrite(newParquet.scala:230)
        ...
java.lang.NullPointerException
        at org.apache.spark.sql.sources.BaseWriterContainer.abortJob(commands.scala:372)
        at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.insert(commands.scala:137)
        at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.run(commands.scala:114)
        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
        at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
        ...
{noformat}
Note that the first {{RuntimeException}} is expected, while the following NPE is not.

The reason of the NPE is that, {{BaseWriterContainer.driverSideSetup()}} calls {{relation.prepareForWriteJob()}} AND initializes the {{OutputCommitter}} used for the subsequent write job. However, if the former throws an exception, the latter is not properly initialized, thus an NPE is thrown when aborting the job because the {{OutputCommitter}} is still null.


---

* [SPARK-8077](https://issues.apache.org/jira/browse/SPARK-8077) | *Minor* | **Optimisation of TreeNode for large number of children**

Large IN clauses are parsed very slowly. For example SQL below (10K items in IN) takes 45-50s. 

{code}
s"""SELECT \* FROM Person WHERE ForeName IN ('${(1 to 10000).map("n" + \_).mkString("','")}')"""
{code}

This is principally due to TreeNode which repeatedly call contains on children, where children in this case is a List that is 10K long. In effect parsing for large IN clauses is O(N squared).

A small change that uses a lazily initialised Set based on children for contains reduces parse time to around 2.5s

I'd like to create PR for change, as we often use IN clauses with a few thousand items.


---

* [SPARK-8075](https://issues.apache.org/jira/browse/SPARK-8075) | *Major* | **apply type checking interface to more expressions**

As https://github.com/apache/spark/pull/6405 has been merged, we need to apply the type checking interface to more expressions, and finally remove the default implementation of it in Expression.


---

* [SPARK-8074](https://issues.apache.org/jira/browse/SPARK-8074) | *Major* | **Parquet should throw AnalysisException during setup for data type/name related failures**

Change sys.error/RuntimeException to AnalysisException.


---

* [SPARK-8072](https://issues.apache.org/jira/browse/SPARK-8072) | *Blocker* | **Better AnalysisException for writing DataFrame with identically named columns**

We should check if there are duplicate columns, and if yes, throw an explicit error message saying there are duplicate columns. See current error message below. 

{code}
In [3]: df.withColumn('age', df.age)
Out[3]: DataFrame[age: bigint, name: string, age: bigint]

In [4]: df.withColumn('age', df.age).write.parquet('test-parquet.out')
---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
\<ipython-input-4-eecb85256898\> in \<module\>()
----\> 1 df.withColumn('age', df.age).write.parquet('test-parquet.out')

/scratch/rxin/spark/python/pyspark/sql/readwriter.py in parquet(self, path, mode)
    350         \>\>\> df.write.parquet(os.path.join(tempfile.mkdtemp(), 'data'))
    351         """
--\> 352         self.\_jwrite.mode(mode).parquet(path)
    353 
    354     @since(1.4)

/Users/rxin/anaconda/lib/python2.7/site-packages/py4j-0.8.1-py2.7.egg/py4j/java\_gateway.pyc in \_\_call\_\_(self, \*args)
    535         answer = self.gateway\_client.send\_command(command)
    536         return\_value = get\_return\_value(answer, self.gateway\_client,
--\> 537                 self.target\_id, self.name)
    538 
    539         for temp\_arg in temp\_args:

/Users/rxin/anaconda/lib/python2.7/site-packages/py4j-0.8.1-py2.7.egg/py4j/protocol.pyc in get\_return\_value(answer, gateway\_client, target\_id, name)
    298                 raise Py4JJavaError(
    299                     'An error occurred while calling {0}{1}{2}.\n'.
--\> 300                     format(target\_id, '.', name), value)
    301             else:
    302                 raise Py4JError(

Py4JJavaError: An error occurred while calling o35.parquet.
: org.apache.spark.sql.AnalysisException: Reference 'age' is ambiguous, could be: age#0L, age#3L.;
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolve(LogicalPlan.scala:279)
	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveChildren(LogicalPlan.scala:116)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4$$anonfun$16.apply(Analyzer.scala:350)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4$$anonfun$16.apply(Analyzer.scala:350)
	at org.apache.spark.sql.catalyst.analysis.package$.withPosition(package.scala:48)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4.applyOrElse(Analyzer.scala:350)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8$$anonfun$applyOrElse$4.applyOrElse(Analyzer.scala:341)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:285)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.org$apache$spark$sql$catalyst$plans$QueryPlan$$transformExpressionUp$1(QueryPlan.scala:108)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2$$anonfun$apply$2.apply(QueryPlan.scala:123)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:244)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at scala.collection.TraversableLike$class.map(TraversableLike.scala:244)
	at scala.collection.AbstractTraversable.map(Traversable.scala:105)
	at org.apache.spark.sql.catalyst.plans.QueryPlan$$anonfun$2.apply(QueryPlan.scala:122)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.sql.catalyst.plans.QueryPlan.transformExpressionsUp(QueryPlan.scala:127)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8.applyOrElse(Analyzer.scala:341)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$$anonfun$apply$8.applyOrElse(Analyzer.scala:243)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$transformUp$1.apply(TreeNode.scala:286)
	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
	at org.apache.spark.sql.catalyst.trees.TreeNode.transformUp(TreeNode.scala:285)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:243)
	at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveReferences$.apply(Analyzer.scala:242)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:61)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:59)
	at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
	at scala.collection.immutable.List.foldLeft(List.scala:84)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:59)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:51)
	at scala.collection.immutable.List.foreach(List.scala:318)
	at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:51)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:903)
	at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:903)
	at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:901)
	at org.apache.spark.sql.DataFrame.\<init\>(DataFrame.scala:131)
	at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.run(commands.scala:98)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult$lzycompute(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.sideEffectResult(commands.scala:57)
	at org.apache.spark.sql.execution.ExecutedCommand.doExecute(commands.scala:68)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:88)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:148)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:87)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd$lzycompute(SQLContext.scala:920)
	at org.apache.spark.sql.SQLContext$QueryExecution.toRdd(SQLContext.scala:920)
	at org.apache.spark.sql.sources.ResolvedDataSource$.apply(ddl.scala:338)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:144)
	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:135)
	at org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:281)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:231)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379)
	at py4j.Gateway.invoke(Gateway.java:259)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:133)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:207)
	at java.lang.Thread.run(Thread.java:744)
{code}


---

* [SPARK-8070](https://issues.apache.org/jira/browse/SPARK-8070) | *Major* | **Improve createDataFrame in Python**

Currently, createDataFrame will issue multiple jobs in Spark (mostly are small jobs of take()), but it's still better to avoid them if possible.


---

* [SPARK-8069](https://issues.apache.org/jira/browse/SPARK-8069) | *Minor* | **Add support for cutoff to RandomForestClassifier**

Consider adding support for cutoffs similar to http://cran.r-project.org/web/packages/randomForest/randomForest.pdf 

(Joseph) I just wrote a [little design doc \| https://docs.google.com/document/d/1nV6m7sqViHkEpawelq1S5\_QLWWAouSlv81eiEEjKuJY/edit?usp=sharing] for this.


---

* [SPARK-8068](https://issues.apache.org/jira/browse/SPARK-8068) | *Minor* | **Add confusionMatrix method at class MulticlassMetrics in pyspark/mllib**

There is no confusionMatrix method at class MulticlassMetrics in pyspark/mllib. This method is actually implemented in scala mllib. To achieve this, we just need add a function call to the corresponding one in scala mllib.


---

* [SPARK-8063](https://issues.apache.org/jira/browse/SPARK-8063) | *Major* | **Spark master URL conflict between MASTER env variable and --master command line option**

Currently, Spark supports several ways to specify the Spark master URL, like --master option for spark-submit, spark.master configuration option, MASTER env variable. They have different precedences, for example, --master overrides MASTER if both are specified.

However, for SparkR shell, it always use the master URL specified by MASTER, not honoring --master.


---

* [SPARK-8059](https://issues.apache.org/jira/browse/SPARK-8059) | *Minor* | **Reduce latency between executor requests and RM heartbeat**

This is a follow up to SPARK-7533. On top of the changes done as part of that issue, we could reduce allocation latency by waking up the allocation thread when the driver send new requests.


---

* [SPARK-8058](https://issues.apache.org/jira/browse/SPARK-8058) | *Major* | **Add tests for SPARK-7853 and SPARK-8020**

This jira is used to track the work of adding tests for SPARK-7853 (make sure {{spark-shell}} with and without {{--jars}} works with the isolated class loader) and SPARK-8020 (we are using correct metastore versions and jars setting to initialize {{metadataHive}}).


---

* [SPARK-8057](https://issues.apache.org/jira/browse/SPARK-8057) | *Major* | **Call TaskAttemptContext.getTaskAttemptID using Reflection**

Someone may use the Spark core jar in the maven repo with hadoop 1. SPARK-2075 has already resolved the compatibility issue to support it. But "SparkHadoopMapRedUtil.commitTask" broke it recently.


---

* [SPARK-8056](https://issues.apache.org/jira/browse/SPARK-8056) | *Major* | **Design an easier way to construct schema for both Scala and Python**

StructType is fairly hard to construct, especially in Python.


---

* [SPARK-8054](https://issues.apache.org/jira/browse/SPARK-8054) | *Major* | **Java compatibility fixes for MLlib 1.4**

See [SPARK-7529]


---

* [SPARK-8052](https://issues.apache.org/jira/browse/SPARK-8052) | *Major* | **Hive on Spark: CAST string AS BIGINT produces wrong value**

Example hive query:
SELECT CAST("775983671874188101" as BIGINT)
produces:           775983671874188160L
Look at: last 2 digits.


---

* [SPARK-8051](https://issues.apache.org/jira/browse/SPARK-8051) | *Major* | **StringIndexerModel (and other models) shouldn't complain if the input column is missing.**

If a transformer is not used during transformation, it should keep silent if the input column is missing.


---

* [SPARK-8049](https://issues.apache.org/jira/browse/SPARK-8049) | *Major* | **OneVsRest's output includes a temp column**

The temp accumulator column "mbc$acc" is included in the output which should be removed with withoutColumn.


---

* [SPARK-8043](https://issues.apache.org/jira/browse/SPARK-8043) | *Minor* | **update NaiveBayes and SVM examples in doc**

I found some issues during testing the save/load examples in markdown Documents, as a part of 1.4 QA plan


---

* [SPARK-8032](https://issues.apache.org/jira/browse/SPARK-8032) | *Major* | **Make NumPy version checking in mllib/\_\_init\_\_.py**

The current checking does version `1.x' is less than `1.4' this will fail if x has greater than 1 digit, since x \> 4, however `1.x` \< `1.4`


---

* [SPARK-8031](https://issues.apache.org/jira/browse/SPARK-8031) | *Trivial* | **Version number written to Hive metastore is "0.13.1aa" instead of "0.13.1a"**

While debugging {{CliSuite}} for 1.4.0-SNAPSHOT, noticed the following WARN log line:
{noformat}
15/06/02 13:40:29 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 0.13.1aa
{noformat}
The problem is that, the version of Hive dependencies 1.4.0-SNAPSHOT uses is {{0.13.1a}} (the one shaded by [~pwendell]), but the version showed in this line is {{0.13.1aa}} (one more {{a}}). The WARN log itself is OK since {{CliSuite}} initializes a brand new temporary Derby metastore.

While initializing Hive metastore, Hive calls {{ObjectStore.checkSchema()}} and may write the "short" version string to metastore. This short version string is defined by {{hive.version.shortname}} in the POM. However, [it was defined as {{0.13.1aa}}\|https://github.com/pwendell/hive/commit/32e515907f0005c7a28ee388eadd1c94cf99b2d4#diff-600376dffeb79835ede4a0b285078036R62]. Confirmed with [~pwendell] that it should be a typo.

This doesn't cause any trouble for now, but we probably want to fix this in the future if we ever need to release another shaded version of Hive 0.13.1.


---

* [SPARK-8019](https://issues.apache.org/jira/browse/SPARK-8019) | *Major* | **[SparkR] Create worker R processes with a command other then Rscript**

Currently, SparkR creates worker R processes by calling the command
"Rscript", so it depends on R being installed with that command
globally visible.

This could be a problem if one wants to use an R engine that is not
installed in this way.  For example, suppose that one has multiple
versions of R on the worker machines, and wants to try a new version
of R under SparkR before it has been formally installed.  Ideally, one
could do this by running SparkR and specifying the full path name to
the Rscript command (such as "/usr/local/R-alt/bin/Rscript").

I faced this problem in a different situation: I am working on an
alternate R engine (TERR), which has an alternate version of the
Rscript command (TERRScript).  I could make TERR work with SparkR by
setting up appropriate links from the file Rscript to my TERRscript,
but I'd rather not disable normal access to R.

I finally dealt with this by making a one-line change to
core/src/main/scala/org/apache/spark/api/r/RRDD.scala (which I will
shortly submit as a pull request for this bug) that uses the new
environment variable "spark.sparkr.r.command" to get the path for
spawning R engines.  If this variable is not defined, it defaults to
"Rscript", so we get the old behavior.  With this change, I can start
SparkR to use TERR with a command such as:

{noformat}
sc \<- sparkR.init(
        sparkEnvir=list(spark.sparkr.use.daemon="false",
                        spark.sparkr.r.command="/usr/local/TERR/bin/TERRscript"))
{noformat}

This is a very low-risk change that could be generally useful to other
people.


---

* [SPARK-8018](https://issues.apache.org/jira/browse/SPARK-8018) | *Major* | **KMeans should accept initial cluster centers as param**

KMeans should allow model initialization using an existing set of cluster centers.


---

* [SPARK-8013](https://issues.apache.org/jira/browse/SPARK-8013) | *Critical* | **Get JDBC server working with Scala 2.11**

It's worth some investigation here, but I believe the simplest solution is to see if we can get Scala to shade it's use of JLine to avoid JLine conflicts between Hive and the Spark repl.

It's also possible that there is a simpler internal solution to the conflict (I haven't looked at it in a long time). So doing some investigation of that would be good. IIRC, there is use of Jline in our own repl code, in addition to in Hive and also in the Scala 2.11 repl. Back when we created the 2.11 build I couldn't harmonize all the versions in a nice way.


---

* [SPARK-8010](https://issues.apache.org/jira/browse/SPARK-8010) | *Major* | **Implict promote Numeric type to String type in HiveTypeCoercion**

1. Given a query
`select coalesce(null, 1, '1') from dual` will cause exception:
  
  java.lang.RuntimeException: Could not determine return type of Coalesce for IntegerType,StringType

2. Given a query:
`select case when true then 1 else '1' end from dual` will cause exception:

  java.lang.RuntimeException: Types in CASE WHEN must be the same or coercible to a common type: StringType != IntegerType

I checked the code, the main cause is the HiveTypeCoercion doesn't do implicit convert when there is a IntegerType and StringType.

Numeric types can be promoted to string type in case throw exceptions.

Since Hive will always do this. It need to be fixed.


---

* [SPARK-8005](https://issues.apache.org/jira/browse/SPARK-8005) | *Major* | **Support INPUT\_\_FILE\_\_NAME virtual column**

INPUT\_\_FILE\_\_NAME: input file name.

One way to do this is to do it through a thread local variable in the SqlNewHadoopRDD.scala, and read that thread local variable in an expression. (similar to SparkPartitionID expression)


---

* [SPARK-8004](https://issues.apache.org/jira/browse/SPARK-8004) | *Major* | **Spark does not enclose column names when fetchting from jdbc sources**

Spark failes to load tables that have a keyword as column names

Sample error:
{code}

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 157.0 failed 1 times, most recent failure: Lost task 0.0 in stage 157.0 (TID 4322, localhost): com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near 'key,value FROM [XXXXXX]'
{code}

A correct query would have been
{code}
SELECT `key`.`value` FROM ....
{code}


---

* [SPARK-8003](https://issues.apache.org/jira/browse/SPARK-8003) | *Major* | **Support SPARK\_\_PARTITION\_\_ID in SQL**

SPARK\_\_PARTITION\_\_ID column should return the partition index of the Spark partition. Note that we already have a DataFrame function for it: https://github.com/apache/spark/blob/78a6723e8758b429f877166973cc4f1bbfce73c4/sql/core/src/main/scala/org/apache/spark/sql/functions.scala#L705


---

* [SPARK-8001](https://issues.apache.org/jira/browse/SPARK-8001) | *Minor* | **Make AsynchronousListenerBus.waitUntilEmpty throw TimeoutException if timeout**

TimeoutException is a more explicit failure. In addition, the caller may forget to call {{assert}} to check the return value of {{AsynchronousListenerBus.waitUntilEmpty}}.


---

* [SPARK-7998](https://issues.apache.org/jira/browse/SPARK-7998) | *Major* | **Improve frequent items documentation**

The current freqItems API is really awkward to use. It returns a DataFrame with a single row, in which each value is an array of frequent items. 

This design doesn't work well for exploratory data analysis (running show -- when there are more than 2 or 3 frequent values, the values get cut off):
{code}
In [74]: df.stat.freqItems(["a", "b", "c"], 0.4).show()
+------------------+------------------+-----------------+
\|       a\_freqItems\|       b\_freqItems\|      c\_freqItems\|
+------------------+------------------+-----------------+
\|ArrayBuffer(11, 1)\|ArrayBuffer(2, 22)\|ArrayBuffer(1, 3)\|
+------------------+------------------+-----------------+
{code}

It also doesn't work well for serious engineering, since it is hard to get the value out.

We should create a new function (so we maintain source/binary compatibility) that returns a list of list of values.


---

* [SPARK-7993](https://issues.apache.org/jira/browse/SPARK-7993) | *Blocker* | **Improve DataFrame.show() output**

1. Each column should be at the minimum 3 characters wide. Right now if the widest value is 1, it is just 1 char wide, which looks ugly. Example below:

2. If a DataFrame have more than N number of rows (N = 20 by default for show), at the end we should display a message like "only showing the top 20 rows".

{code}
+--+--+-+
\| a\| b\|c\|
+--+--+-+
\| 1\| 2\|3\|
\| 1\| 2\|1\|
\| 1\| 2\|3\|
\| 3\| 6\|3\|
\| 1\| 2\|3\|
\| 5\|10\|1\|
\| 1\| 2\|3\|
\| 7\|14\|3\|
\| 1\| 2\|3\|
\| 9\|18\|1\|
\| 1\| 2\|3\|
\|11\|22\|3\|
\| 1\| 2\|3\|
\|13\|26\|1\|
\| 1\| 2\|3\|
\|15\|30\|3\|
\| 1\| 2\|3\|
\|17\|34\|1\|
\| 1\| 2\|3\|
\|19\|38\|3\|
+--+--+-+
only showing top 20 rows   \<---- add this at the end
{code}

3. For array values, instead of printing "ArrayBuffer", we should just print square brackets:

{code}
+------------------+------------------+-----------------+
\|       a\_freqItems\|       b\_freqItems\|      c\_freqItems\|
+------------------+------------------+-----------------+
\|ArrayBuffer(11, 1)\|ArrayBuffer(2, 22)\|ArrayBuffer(1, 3)\|
+------------------+------------------+-----------------+
{code}

should be

{code}
+-----------+-----------+-----------+
\|a\_freqItems\|b\_freqItems\|c\_freqItems\|
+-----------+-----------+-----------+
\|    [11, 1]\|    [2, 22]\|     [1, 3]\|
+-----------+-----------+-----------+
{code}


---

* [SPARK-7991](https://issues.apache.org/jira/browse/SPARK-7991) | *Major* | **Python DataFrame: support passing a list into describe**

DataFrame.describe in Python takes a vararg, i.e. it can be invoked this way:
{code}
df.describe('col1', 'col2', 'col3')
{code}

Most of our DataFrame functions accept a list in addition to varargs. describe should do the same, i.e. it should also accept a Python list:
{code}
df.describe(['col1', 'col2', 'col3'])
{code}


---

* [SPARK-7990](https://issues.apache.org/jira/browse/SPARK-7990) | *Major* | **Add methods to facilitate equi-join on multiple join keys**

We have a variant of the join function that facilitates equi-join on a single join key, but we don't have one to do it for multiple join keys.

This is the existing Python API:
{code}
def join(self, other, joinExprs=None, joinType=None):
{code}

I think we should rename joinExprs to "on", and joinType to "how" to match Pandas. And then the "on" column should support either a string, a join condition, a list of string, or a list of join condition ("and" together).

In order to support the Python API, we'd need to add a variant for Scala as well. I think we can add another join method that looks like
{code}
def join(other: DataFrame, on: Seq[String], joinType: String): DataFrame
{code}
and update the existing Scala one to call this one.


---

* [SPARK-7989](https://issues.apache.org/jira/browse/SPARK-7989) | *Major* | **Fix flaky tests in ExternalShuffleServiceSuite and SparkListenerWithClusterSuite**

The flaky tests in ExternalShuffleServiceSuite and SparkListenerWithClusterSuite will fail if there are not enough executors up before running the jobs.


---

* [SPARK-7988](https://issues.apache.org/jira/browse/SPARK-7988) | *Critical* | **Mechanism to control receiver scheduling**

Streaming receivers are currently scheduled by the underlying Spark scheduler in the same way as other tasks. Will help performance if we added a knob to control receiver scheduling. In most cases, even distribution of receivers among workers should yield best results. In some cases, dense scheduling (e.g, all receivers on one node) may perform better. Maybe support for a new configuration parameter spark.receiver.scheduling.mode (or some such) would help. With 'default' as the current scheduling scheme and 'round-robin' for even distribution among workers (and potentially a third mode for 'dense' scheduling).


---

* [SPARK-7986](https://issues.apache.org/jira/browse/SPARK-7986) | *Major* | **Refactor scalastyle-config.xml to divide it into 3 sections**

This file should be divided into 3 sections:
 (1) rules that we enforce.
 (2) rules that we would like to enforce, but haven't cleaned up the codebase to turn on yet  (or we need to make the scalastyle rule more configurable).
 (3) rules that we don't want to enforce.


---

* [SPARK-7983](https://issues.apache.org/jira/browse/SPARK-7983) | *Minor* | **Add require for one-based indices in loadLibSVMFile**

Add require for one-based indices in loadLibSVMFile

Customers frequently use zero-based indices in their LIBSVM files. No warnings or errors from Spark will be reported during their computation afterwards, and usually it will lead to wired result for many algorithms (like GBDT).

add a quick check.


---

* [SPARK-7980](https://issues.apache.org/jira/browse/SPARK-7980) | *Major* | **Support SQLContext.range(end)**

SQLContext.range should also allow only specifying the end position, similar to Python's own range.


---

* [SPARK-7977](https://issues.apache.org/jira/browse/SPARK-7977) | *Major* | **Disallow println**

Very often we see pull requests that added println from debugging, but the author forgot to remove it before code review.

We can use the regex checker to disallow println. For legitimate use of println, we can then disable the rule where they are used.

Add to scalastyle-config.xml file:
{code}
  \<check customId="println" level="error" class="org.scalastyle.scalariform.TokenChecker" enabled="true"\>
    \<parameters\>\<parameter name="regex"\>^println$\</parameter\>\</parameters\>
    \<customMessage\>\<![CDATA[Are you sure you want to println? If yes, wrap the code block with 
      // scalastyle:off println
      println(...)
      // scalastyle:on println]]\>\</customMessage\>
  \</check\>
{code}


---

* [SPARK-7969](https://issues.apache.org/jira/browse/SPARK-7969) | *Minor* | **Drop method on Dataframes should handle Column**

For now the drop method available on Dataframe since Spark 1.4.0 only accepts a column name (as a string), it should also accept a Column as input.


---

* [SPARK-7961](https://issues.apache.org/jira/browse/SPARK-7961) | *Critical* | **Redesign SQLConf for better error message reporting**

Right now, we don't validate config values and as a result will throw exceptions when queries or DataFrame operations are run.

Imagine if one user sets config variable "spark.sql.retainGroupColumns" (requires "true", "false") to "hello". The set action itself will complete fine. When another user runs a query, it will throw the following exception:
{code}
java.lang.IllegalArgumentException: For input string: "hello"
    at scala.collection.immutable.StringLike$class.parseBoolean(StringLike.scala:238)
    at scala.collection.immutable.StringLike$class.toBoolean(StringLike.scala:226)
    at scala.collection.immutable.StringOps.toBoolean(StringOps.scala:31)
    at org.apache.spark.sql.SQLConf.dataFrameRetainGroupColumns(SQLConf.scala:265)
    at org.apache.spark.sql.GroupedData.toDF(GroupedData.scala:74)
    at org.apache.spark.sql.GroupedData.agg(GroupedData.scala:227)
{code}

This is highly confusing. We should redesign SQLConf to validate data input at set time (during setConf call).


---

* [SPARK-7956](https://issues.apache.org/jira/browse/SPARK-7956) | *Major* | **Use Janino to compile SQL expression**

The overhead of current implementation of codegen is to high (50ms - 500ms), which blocks us from turning it on by default.

We should try to investigate using Janino to compile the SQL expressions into JVM bytecode, which should be much faster to compile (about 10ms).


---

* [SPARK-7955](https://issues.apache.org/jira/browse/SPARK-7955) | *Major* | **Dynamic allocation: longer timeout for executors with cached blocks**

When dynamic allocation is enabled, executor idle time is currently the only parameter considered. This can be annoying if executors get removed but have cached blocks. This can cause sever performance degradation.


---

* [SPARK-7952](https://issues.apache.org/jira/browse/SPARK-7952) | *Major* | **equality check between boolean type and numeric type is broken.**

Currently we only support literal numeric values.


---

* [SPARK-7945](https://issues.apache.org/jira/browse/SPARK-7945) | *Minor* | **Do trim to values of properties**

Now applications submited by org.apache.spark.launcher.Main read properties file without doing trim to values in it. 

If user left a space after a value(say spark.driver.extraClassPath) then it probably affect global functions(like some jar could not be included in the classpath), so we should do it like Utils.getPropertiesFromFile.


---

* [SPARK-7944](https://issues.apache.org/jira/browse/SPARK-7944) | *Critical* | **Spark-Shell 2.11 1.4.0-RC-03 does not add jars to class path**

When I run the spark-shell with the --jars argument and supply a path to a single jar file, none of the classes in the jar are available in the REPL.

I have encountered this same behaviour in both 1.3.1 and 1.4.0\_RC-03 builds for scala 2.11. I have yet to do a 1.4.0 RC-03 build for scala 2.10, but the contents of the jar are available in the 1.3.1\_2.10 REPL.


---

* [SPARK-7939](https://issues.apache.org/jira/browse/SPARK-7939) | *Major* | **Make URL partition recognition return String by default for all partition column types and values**

Imagine the following HDFS paths:

/data/split=00
/data/split=01
...
/data/split=FF

If I have less than or equal to 10 partitions (00, 01, ... 09), currently partition recognition will treat column 'split' as integer column. 

If I have more than 10 partitions, column 'split' will be recognized as String...

This is very confusing. \*So I'm suggesting to treat partition columns as String by default\*, and allow user to specify types if needed.

Another example is date:
/data/date=2015-04-01 =\> 'date' is String
/data/date=20150401 =\> 'date' is Int

Jianshi


---

* [SPARK-7937](https://issues.apache.org/jira/browse/SPARK-7937) | *Major* | **Cannot compare Hive named\_struct. (when using argmax, argmin)**

Imagine the following SQL:

Intention: get last used bank account country.
 
{code:sql}
select bank\_account\_id, 
  max(named\_struct(
    'src\_row\_update\_ts', unix\_timestamp(src\_row\_update\_ts,'yyyy/M/D HH:mm:ss'), 
    'bank\_country', bank\_country)).bank\_country 
from bank\_account\_monthly
where year\_month='201502' 
group by bank\_account\_id
{code}

=\> 
{noformat}
Error: org.apache.spark.SparkException: Job aborted due to stage failure: Task 94 in stage 96.0 failed 4 times, most recent failure: Lost task 94.3 in stage 96.0 (TID 22281, xxxx): java.lang.RuntimeException: Type StructType(StructField(src\_row\_update\_ts,LongType,true), StructField(bank\_country,StringType,true)) does not support ordered operations
        at scala.sys.package$.error(package.scala:27)
        at org.apache.spark.sql.catalyst.expressions.LessThan.ordering$lzycompute(predicates.scala:222)
        at org.apache.spark.sql.catalyst.expressions.LessThan.ordering(predicates.scala:215)
        at org.apache.spark.sql.catalyst.expressions.LessThan.eval(predicates.scala:235)
        at org.apache.spark.sql.catalyst.expressions.MaxFunction.update(aggregates.scala:147)
        at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$7.apply(Aggregate.scala:165)
        at org.apache.spark.sql.execution.Aggregate$$anonfun$doExecute$1$$anonfun$7.apply(Aggregate.scala:149)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.RDD$$anonfun$mapPartitions$1$$anonfun$apply$17.apply(RDD.scala:686)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:277)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:244)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:70)
        at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:41)
        at org.apache.spark.scheduler.Task.run(Task.scala:70)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{noformat}


---

* [SPARK-7916](https://issues.apache.org/jira/browse/SPARK-7916) | *Major* | **MLlib Python doc parity check for classification and regression.**

Check then make the MLlib Python classification and regression doc to be as complete as the Scala doc.


---

* [SPARK-7915](https://issues.apache.org/jira/browse/SPARK-7915) | *Major* | **Support specifying the column list for target table in CTAS**

{code}
create table t1 (a int, b string) as select key, value from src;

desc t1;
key	int	NULL
value	string	NULL
{code}

Thus Hive doesn't support specifying the column list for target table in CTAS, however, we should either throwing exception explicitly, or supporting the this feature, we just pick up the later, which seems useful and straightforward.


---

* [SPARK-7913](https://issues.apache.org/jira/browse/SPARK-7913) | *Minor* | **Increase the maximum capacity of PartitionedPairBuffer, PartitionedSerializedPairBuffer and AppendOnlyMap**

We can change the growing strategy to increase the maximum capacity of PartitionedPairBuffer, PartitionedSerializedPairBuffer and AppendOnlyMap.


---

* [SPARK-7910](https://issues.apache.org/jira/browse/SPARK-7910) | *Minor* | **Expose partitioner information in JavaRDD**

It would be useful to expose the partitioner info in the Java & Python APIs for RDDs.


---

* [SPARK-7902](https://issues.apache.org/jira/browse/SPARK-7902) | *Critical* | **SQL UDF doesn't support UDT in PySpark**

We don't convert Python SQL internal types to Python types in SQL UDF execution. This causes problems if the input arguments contain UDTs or the return type is a UDT. Right now, the raw SQL types are passed into the Python UDF and the return value is not converted to Python SQL types.

This is the code (from [~rams]) to produce this bug. (Actually, it triggers another bug first right now.)
{code}
from pyspark.mllib.linalg import SparseVector
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

df = sqlContext.createDataFrame([(SparseVector(2, {0: 0.0}),)], ["features"])
sz = udf(lambda s: s.size, IntegerType())
df.select(sz(df.features).alias("sz")).collect()
{code}


---

* [SPARK-7887](https://issues.apache.org/jira/browse/SPARK-7887) | *Major* | **Remove EvaluatedType from SQL Expression**

It's not a very useful type to use. We can just remove it to simplify expressions slightly.


---

* [SPARK-7886](https://issues.apache.org/jira/browse/SPARK-7886) | *Blocker* | **Add built-in expressions to FunctionRegistry**

Once we do this, we no longer needs to hardcode expressions into the parser (both for internal SQL and Hive QL).


---

* [SPARK-7884](https://issues.apache.org/jira/browse/SPARK-7884) | *Major* | **Move block deserialization from BlockStoreShuffleFetcher to ShuffleReader**

The current Spark shuffle has some hard-coded assumptions about how shuffle managers will read and write data.

The BlockStoreShuffleFetcher.fetch method relies on the ShuffleBlockFetcherIterator that assumes shuffle data is written using the BlockManager.getDiskWriter method and doesn't allow for customization.


---

* [SPARK-7879](https://issues.apache.org/jira/browse/SPARK-7879) | *Critical* | **KMeans API for spark.ml Pipelines**

Create a K-Means API for the spark.ml Pipelines API.  This should wrap the existing KMeans implementation in spark.mllib.

This should be the first clustering method added to Pipelines, and it will be important to consider [SPARK-7610] and think about designing the clustering API.  We do not have to have abstractions from the beginning (and probably should not) but should think far enough ahead so we can add abstractions later on.


---

* [SPARK-7878](https://issues.apache.org/jira/browse/SPARK-7878) | *Minor* | **Rename Stage.jobId to Stage.earliestJobId**

The jobId field in stage refers to the earliest job that uses that job; there is another field, jobIds, that lists all jobs for the stage. We should rename this field to avoid future bugs where people think jobId refers to the one and only job for the stage (e.g., SPARK-6880).


---

* [SPARK-7862](https://issues.apache.org/jira/browse/SPARK-7862) | *Major* | **Query would hang when the using script has error output in SparkSQL**

Steps to reproduce:

val data = (1 to 100000).map { i =\> (i, i, i) }
data.toDF("d1", "d2", "d3").registerTempTable("script\_trans")
 sql("SELECT TRANSFORM (d1, d2, d3) USING 'cat 1\>&2' AS (a,b,c) FROM script\_trans")


---

* [SPARK-7859](https://issues.apache.org/jira/browse/SPARK-7859) | *Major* | **Collect\_SET behaves different under different version of JDK**

To reproduce 
{code}
JAVA\_HOME=/home/hcheng/Java/jdk1.8.0\_45 \| build/sbt -Phadoop-2.3 -Phive  'test-only org.apache.spark.sql.hive.execution.HiveWindowFunctionQueryWithoutCodeGenSuite'
{code}

{panel}
- windowing.q -- 20. testSTATs \*\*\* FAILED \*\*\*
  Results do not match for windowing.q -- 20. testSTATs:
...

Manufacturer#1	almond antique burnished rose metallic	2	258.10677784349235	258.10677784349235	[34,2,6]	66619.10876874991	0.811328754177887	2801.7074999999995               
Manufacturer#1	almond antique burnished rose metallic	2	258.10677784349235	258.10677784349235	[2,34,6]	66619.10876874991	0.811328754177887	2801.7074999999995
{panel}


---

* [SPARK-7855](https://issues.apache.org/jira/browse/SPARK-7855) | *Major* | **Move hash-style shuffle code out of ExternalSorter and into own file**

ExternalSorter contains a bunch of code for handling the bypassMergeThreshold / hash-style shuffle path.  I think that it would significantly simplify the code to move this functionality out of ExternalSorter and into a separate class which shares a common interface (insertAll / writePartitionedFile()).  This is a stepping-stone towards eventually removing this bypass path (see SPARK-6026)


---

* [SPARK-7854](https://issues.apache.org/jira/browse/SPARK-7854) | *Minor* | **refine Kryo configuration limits test**

refine the code style, make it more abstraction.


---

* [SPARK-7846](https://issues.apache.org/jira/browse/SPARK-7846) | *Major* | **Use different way to pass spark.yarn.keytab and spark.yarn.principal in different modes**

--principal and --keytabl options are passed to client but when we started thrift server or spark-shell these two are also passed into the Main class (org.apache.spark.sql.hive.thriftserver.HiveThriftServer2 and org.apache.spark.repl.Main).

In these two main class, arguments passed in will be processed with some 3rd libraries, which will lead to some error: "Invalid option: --principal" or "Unrecgnised option: --principal".

We should pass these command args in different forms, say system properties.


---

* [SPARK-7845](https://issues.apache.org/jira/browse/SPARK-7845) | *Critical* | **Bump "Hadoop 1" tests to version 1.2.1**

A small number of API's in Hadoop were added between 1.0.4 and 1.2.1. It appears this is one cause of SPARK-7843 since some Hive code relies on newer Hadoop API's. My feeling is we should just bump our tested version up to 1.2.1 (both versions are extremely old). If users are still on \< 1.2.1 and run into some of these corner cases, we can consider doing some engineering and supporting the older versions. I'd like to bump our test version though and let this be driven by users, if they exist.

https://github.com/apache/spark/blob/master/dev/run-tests#L43


---

* [SPARK-7837](https://issues.apache.org/jira/browse/SPARK-7837) | *Critical* | **NPE when save as parquet in speculative tasks**

The query is like {{df.orderBy(...).saveAsTable(...)}}.

When there is no partitioning columns and there is a skewed key, I found the following exception in speculative tasks. After these failures, seems we could not call {{SparkHadoopMapRedUtil.commitTask}} correctly.

{code}
java.lang.NullPointerException
	at parquet.hadoop.InternalParquetRecordWriter.flushRowGroupToStore(InternalParquetRecordWriter.java:146)
	at parquet.hadoop.InternalParquetRecordWriter.close(InternalParquetRecordWriter.java:112)
	at parquet.hadoop.ParquetRecordWriter.close(ParquetRecordWriter.java:73)
	at org.apache.spark.sql.parquet.ParquetOutputWriter.close(newParquet.scala:115)
	at org.apache.spark.sql.sources.DefaultWriterContainer.abortTask(commands.scala:385)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation.org$apache$spark$sql$sources$InsertIntoHadoopFsRelation$$writeRows$1(commands.scala:150)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:122)
	at org.apache.spark.sql.sources.InsertIntoHadoopFsRelation$$anonfun$insert$1.apply(commands.scala:122)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:70)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:213)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:745)
{code}


---

* [SPARK-7835](https://issues.apache.org/jira/browse/SPARK-7835) | *Major* | **Refactor HeartbeatReceiverSuite for coverage and clean up**

As of the writing of this description, the existing test suite has a lot of duplicate code and doesn't even cover the most fundamental feature of the HeartbeatReceiver, which is expiring hosts that have not responded in a while.

https://github.com/apache/spark/blob/31d5d463e76b6611c854c6cf27059fec8198adc9/core/src/test/scala/org/apache/spark/HeartbeatReceiverSuite.scala

We should rewrite this test suite to increase coverage and decrease duplicate code.


---

* [SPARK-7826](https://issues.apache.org/jira/browse/SPARK-7826) | *Major* | **Suppress extra calling getCacheLocs.**

There are too many extra call method {{getCacheLocs}} for {{DAGScheduler}}, which includes Akka communication.
To improve {{DAGScheduler}} performance, suppress extra calling the method.

In my application with over 1200 stages, the execution time became 3.8 min from 8.5 min with my patch.


---

* [SPARK-7824](https://issues.apache.org/jira/browse/SPARK-7824) | *Major* | **Collapsing operator reordering and constant folding into a single batch to push down the single side.**

SQL:
{noformat}
select \* from tableA join tableB on (a \> 3 and b = d) or (a \> 3 and b = e)
{noformat}

Plan before modify
{noformat}
== Optimized Logical Plan ==
Project [a#293,b#294,c#295,d#296,e#297]
 Join Inner, Some(((a#293 \> 3) && ((b#294 = d#296) \|\| (b#294 = e#297))))
  MetastoreRelation default, tablea, None
  MetastoreRelation default, tableb, None
{noformat}

Plan after modify
{noformat}
== Optimized Logical Plan ==
Project [a#293,b#294,c#295,d#296,e#297]
 Join Inner, Some(((b#294 = d#296) \|\| (b#294 = e#297)))
  Filter (a#293 \> 3)
   MetastoreRelation default, tablea, None
  MetastoreRelation default, tableb, None
{noformat}


---

* [SPARK-7820](https://issues.apache.org/jira/browse/SPARK-7820) | *Critical* | **Java8-tests suite compile error under SBT**

Lots of compilation error is shown when java 8 test suite is enabled in SBT:

{{JAVA\_HOME=/usr/java/jdk1.8.0\_45 ./sbt/sbt -Pyarn -Phadoop-2.4 -Dhadoop.version=2.6.0 -Pjava8-tests}}

{code}
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:43: error: cannot find symbol
[error] public class Java8APISuite extends LocalJavaStreamingContext implements Serializable {
[error]                                    ^
[error]   symbol: class LocalJavaStreamingContext
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:55: error: cannot find symbol
[error]     JavaDStream\<String\> stream = JavaTestUtils.attachTestInputStream(ssc, inputData, 1);
[error]                                                                      ^
[error]   symbol:   variable ssc
[error]   location: class Java8APISuite
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:55: error: cannot find symbol
[error]     JavaDStream\<String\> stream = JavaTestUtils.attachTestInputStream(ssc, inputData, 1);
[error]                                  ^
[error]   symbol:   variable JavaTestUtils
[error]   location: class Java8APISuite
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:57: error: cannot find symbol
[error]     JavaTestUtils.attachTestOutputStream(letterCount);
[error]     ^
[error]   symbol:   variable JavaTestUtils
[error]   location: class Java8APISuite
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:58: error: cannot find symbol
[error]     List\<List\<Integer\>\> result = JavaTestUtils.runStreams(ssc, 2, 2);
[error]                                                           ^
[error]   symbol:   variable ssc
[error]   location: class Java8APISuite
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:58: error: cannot find symbol
[error]     List\<List\<Integer\>\> result = JavaTestUtils.runStreams(ssc, 2, 2);
[error]                                  ^
[error]   symbol:   variable JavaTestUtils
[error]   location: class Java8APISuite
[error] /mnt/data/project/apache-spark/extras/java8-tests/src/test/java/org/apache/spark/streaming/Java8APISuite.java:73: error: cannot find symbol
[error]     JavaDStream\<String\> stream = JavaTestUtils.attachTestInputStream(ssc, inputData, 1);
[error]                                                                      ^
[error]   symbol:   variable ssc
[error]   location: class Java8APISuite
{code}

The class {{JavaAPISuite}} relies on {{LocalJavaStreamingContext}} which exists in streaming test jar. It is OK for maven compile, since it will generate test jar, but will be failed in sbt test compile, sbt do not generate test jar by default.


---

* [SPARK-7811](https://issues.apache.org/jira/browse/SPARK-7811) | *Trivial* | **Fix typo on slf4j configuration on metrics.properties.template**

There are a minor typo on slf4jsink configuration at metrics.properties.template. 

slf4j is mispelled as sl4j on 2 of the configuration. 

Correcting the typo so users' custom settings will be loaded correctly.


---

* [SPARK-7810](https://issues.apache.org/jira/browse/SPARK-7810) | *Major* | **rdd.py "\_load\_from\_socket" cannot load data from jvm socket if ipv6 is used**

Method "\_load\_from\_socket" in rdd.py cannot load data from jvm socket if ipv6 is used. The current method only works well with ipv4. New modification should work around both two protocols.


---

* [SPARK-7808](https://issues.apache.org/jira/browse/SPARK-7808) | *Major* | **Scala package doc for spark.ml.feature**

We added several feature transformers in Spark 1.4. It would be great to add package doc for `spark.ml.feature`.


---

* [SPARK-7795](https://issues.apache.org/jira/browse/SPARK-7795) | *Major* | **Speed up task serialization in standalone mode**

My experiments with scheduling very short tasks in standalone cluster mode indicated that a significant amount of time was being spent in scheduling the tasks (\>500ms for 256 tasks). I found that most of the time was being spent in creating a new instance of serializer for each task. Changing this to just one serializer brought down the scheduling time to 8ms.


---

* [SPARK-7792](https://issues.apache.org/jira/browse/SPARK-7792) | *Major* | **HiveContext registerTempTable not thread safe**

{code:java}
public class ThreadRepro {
    public static void main(String[] args) throws Exception{
       new ThreadRepro().sparkPerfTest();
    }

    public void sparkPerfTest(){

        final AtomicLong counter = new AtomicLong();
        SparkConf conf = new SparkConf();
        conf.setAppName("My Application");
        conf.setMaster("local[7]");
        SparkContext sc = new SparkContext(conf);

        org.apache.spark.sql.hive.HiveContext hc = new org.apache.spark.sql.hive.HiveContext(sc);
        int poolSize = 10;
        ExecutorService pool = Executors.newFixedThreadPool(poolSize);
        for (int i=0; i\<poolSize;i++ )
            pool.execute(new QueryJob(hc, i, counter));

        pool.shutdown();
        try {
            pool.awaitTermination(60, TimeUnit.MINUTES);
        }catch(Exception e){
            System.out.println("Thread interrupted");
        }
        System.out.println("All jobs complete");
        System.out.println(" Counter is "+counter.get());

    }
}

class QueryJob implements Runnable{
    String threadId;
    org.apache.spark.sql.hive.HiveContext sqlContext;
    String key;
    AtomicLong counter;
    final AtomicLong local\_counter = new AtomicLong();

    public QueryJob(org.apache.spark.sql.hive.HiveContext \_sqlContext,int id,AtomicLong ctr){

        threadId = "thread\_"+id;
        this.sqlContext= \_sqlContext;
        this.counter = ctr;
    }
    public void run() {
        for (int i = 0; i \< 100; i++) {
            String tblName = threadId +"\_"+i;
            DataFrame df = sqlContext.emptyDataFrame();
            df.registerTempTable(tblName);
            String \_query = String.format("select count(\*) from %s",tblName);
            System.out.println(String.format(" registered table %s; catalog (%s) ",tblName,debugTables()));
            List\<Row\> res;
            try {
                res = sqlContext.sql(\_query).collectAsList();
            }catch (Exception e){
                System.out.println("\*Exception "+ debugTables() +"\*\*");
                throw e;
            }
            sqlContext.dropTempTable(tblName);
            System.out.println(" dropped table "+tblName);
            try {
                Thread.sleep(3000);//lets make this a not-so-tight loop
            }catch(Exception e){
                System.out.println("Thread interrupted");
            }
        }
    }

    private String debugTables(){
        String v = Joiner.on(',').join(sqlContext.tableNames());
        if (v==null)return ""; else return v;
    }
}
{code}

this will periodically produce the following:

{quote}
 registered table thread\_0\_50; catalog (thread\_1\_50)
 registered table thread\_4\_50; catalog (thread\_4\_50,thread\_1\_50)
 registered table thread\_1\_50; catalog (thread\_1\_50)
 dropped table thread\_1\_50
 dropped table thread\_4\_50
\*Exception \*\*
Exception in thread "pool-6-thread-1" java.lang.Error: org.apache.spark.sql.AnalysisException: no such table thread\_0\_50; line 1 pos 21
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1151)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
  at java.lang.Thread.run(Thread.java:745)
Caused by: org.apache.spark.sql.AnalysisException: no such table thread\_0\_50; line 1 pos 21
  at org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.failAnalysis(package.scala:42)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.getTable(Analyzer.scala:177)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$6.applyOrElse(Analyzer.scala:186)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$6.applyOrElse(Analyzer.scala:181)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:188)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:188)
  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:51)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:187)
  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$4.apply(TreeNode.scala:208)
  at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
  at scala.collection.Iterator$class.foreach(Iterator.scala:727)
  at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
  at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
  at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
  at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
  at scala.collection.AbstractIterator.to(Iterator.scala:1157)
  at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
  at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
  at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
  at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformChildrenDown(TreeNode.scala:238)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:193)
  at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:178)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:181)
  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:171)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:61)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1$$anonfun$apply$2.apply(RuleExecutor.scala:59)
  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:111)
  at scala.collection.immutable.List.foldLeft(List.scala:84)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:59)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$apply$1.apply(RuleExecutor.scala:51)
  at scala.collection.immutable.List.foreach(List.scala:318)
  at org.apache.spark.sql.catalyst.rules.RuleExecutor.apply(RuleExecutor.scala:51)
  at org.apache.spark.sql.SQLContext$QueryExecution.analyzed$lzycompute(SQLContext.scala:1082)
  at org.apache.spark.sql.SQLContext$QueryExecution.analyzed(SQLContext.scala:1082)
  at org.apache.spark.sql.SQLContext$QueryExecution.assertAnalyzed(SQLContext.scala:1080)
  at org.apache.spark.sql.DataFrame.\<init\>(DataFrame.scala:133)
  at org.apache.spark.sql.DataFrame$.apply(DataFrame.scala:51)
  at org.apache.spark.sql.hive.HiveContext.sql(HiveContext.scala:101)
  at test.unit.QueryJob.run(ThreadRepro.java:93)
  at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
{quote}

Line 93 is the .sql call...


---

* [SPARK-7785](https://issues.apache.org/jira/browse/SPARK-7785) | *Minor* | **Add pretty printing to pyspark.mllib.linalg.Matrices**

Add \_\_str\_\_ and  \_\_repr\_\_ to matrices.


---

* [SPARK-7781](https://issues.apache.org/jira/browse/SPARK-7781) | *Major* | **GradientBoostedTrees is missing maxBins parameter in pyspark**

I'm running Spark v1.3.1 and when I run the following against my dataset:

{code}
model = GradientBoostedTrees.trainRegressor(trainingData, categoricalFeaturesInfo=catFeatures, maxDepth=6, numIterations=3)

The job will fail with the following message:
Traceback (most recent call last):
  File "/Users/drake/fd/spark/mltest.py", line 73, in \<module\>
    model = GradientBoostedTrees.trainRegressor(trainingData, categoricalFeaturesInfo=catFeatures, maxDepth=6, numIterations=3)
  File "/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/tree.py", line 553, in trainRegressor
    loss, numIterations, learningRate, maxDepth)
  File "/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/tree.py", line 438, in \_train
    loss, numIterations, learningRate, maxDepth)
  File "/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/common.py", line 120, in callMLlibFunc
    return callJavaFunc(sc, api, \*args)
  File "/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/pyspark/mllib/common.py", line 113, in callJavaFunc
    return \_java2py(sc, func(\*args))
  File "/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/java\_gateway.py", line 538, in \_\_call\_\_
  File "/Users/drake/spark/spark-1.3.1-bin-hadoop2.6/python/lib/py4j-0.8.2.1-src.zip/py4j/protocol.py", line 300, in get\_return\_value
15/05/20 16:40:12 INFO BlockManager: Removing block rdd\_32\_95
py4j.protocol.Py4JJavaError: An error occurred while calling o69.trainGradientBoostedTreesModel.
: java.lang.IllegalArgumentException: requirement failed: DecisionTree requires maxBins (= 32) \>= max categories in categorical features (= 1895)
	at scala.Predef$.require(Predef.scala:233)
	at org.apache.spark.mllib.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:128)
	at org.apache.spark.mllib.tree.RandomForest.run(RandomForest.scala:138)
	at org.apache.spark.mllib.tree.DecisionTree.run(DecisionTree.scala:60)
	at org.apache.spark.mllib.tree.GradientBoostedTrees$.org$apache$spark$mllib$tree$GradientBoostedTrees$$boost(GradientBoostedTrees.scala:150)
	at org.apache.spark.mllib.tree.GradientBoostedTrees.run(GradientBoostedTrees.scala:63)
	at org.apache.spark.mllib.tree.GradientBoostedTrees$.train(GradientBoostedTrees.scala:96)
	at org.apache.spark.mllib.api.python.PythonMLLibAPI.trainGradientBoostedTreesModel(PythonMLLibAPI.scala:595)
{code}

So, it's complaining about the maxBins, if I provide maxBins=1900 and re-run it:

{code}
model = GradientBoostedTrees.trainRegressor(trainingData, categoricalFeaturesInfo=catFeatures, maxDepth=6, numIterations=3, maxBins=1900)

Traceback (most recent call last):
  File "/Users/drake/fd/spark/mltest.py", line 73, in \<module\>
    model = GradientBoostedTrees.trainRegressor(trainingData, categoricalFeaturesInfo=catF
eatures, maxDepth=6, numIterations=3, maxBins=1900)
TypeError: trainRegressor() got an unexpected keyword argument 'maxBins'
{code}

It now says it knows nothing of maxBins.

If I run the same command against DecisionTree or RandomForest (with maxBins=1900) it works just fine.

Seems like a bug in GradientBoostedTrees.


---

* [SPARK-7775](https://issues.apache.org/jira/browse/SPARK-7775) | *Critical* | **YARN AM tried to sleep negative milliseconds**

{code}
SLF4J: See http://www.slf4j.org/codes.html#multiple\_bindings for an explanation.
SLF4J: Actual binding is of type [org.slf4j.impl.Log4jLoggerFactory]
Exception in thread "Reporter" java.lang.IllegalArgumentException: timeout value is negative
  at java.lang.Thread.sleep(Native Method)
  at org.apache.spark.deploy.yarn.ApplicationMaster$$anon$1.run(ApplicationMaster.scala:356)
{code}

This kills the "reporter thread", which does some allocating too.


---

* [SPARK-7743](https://issues.apache.org/jira/browse/SPARK-7743) | *Major* | **Upgrade Parquet to 1.7**

There are many outstanding issues with the parquet format that have been resolved between the version depended on by spark (1.6.0rc3 as of spark 1.3.1) and the most recent parquet release (1.6.0).

Some of these are things include not supporting schema migration when using parquet with avro, not supporting summary metadata in the parquet footers causing null pointer exceptions reading, and many others.

See https://github.com/apache/parquet-mr/blob/master/CHANGES.md#version-160 for the full list of fixes.


---

* [SPARK-7739](https://issues.apache.org/jira/browse/SPARK-7739) | *Minor* | **Improve ChiSqSelector example code in the user guide**

As discussed in http://apache-spark-user-list.1001560.n3.nabble.com/Discretization-td22811.html

We should mention the values are gray levels (0-255) and change the division to integer division.


---

* [SPARK-7735](https://issues.apache.org/jira/browse/SPARK-7735) | *Minor* | **Raise Exception on non-zero exit from pyspark pipe commands**

In pyspark errors are ignored when using the rdd.pipe function. This is different to the scala behaviour where abnormal exit of the piped command is raised. I have submitted a pull request on github which I believe will bring the pyspark behaviour closer to the scala behaviour.

A simple case of where this bug may be problematic is using a network bash utility to perform computations on an rdd. Currently, network errors will be ignored and blank results returned when it would be more desirable to raise an exception so that spark can retry the failed task.


---

* [SPARK-7733](https://issues.apache.org/jira/browse/SPARK-7733) | *Minor* | **Update build, code to use Java 7 for 1.5.0+**

Following the announced intention to drop Java 6 support in Spark 1.5, we should go ahead and switch master to Java 7 for Spark 1.5. We can also then remove a few checks and comments that were specific to supporting Java 6.

I think we might also usefully use the try-with-resources feature to improve / tighten up resource management in Java code, and look at replacing some uses of Guava functions with new JDK equivalents.

It's probably not quite worth replacing {{List\<String\> foo = new ArrayList\<String\>()}} with {{List\<String\> foo = new ArrayList\<\>()}} everywhere but that simplification might be useful to apply over time as the Java code is otherwise changed.


---

* [SPARK-7717](https://issues.apache.org/jira/browse/SPARK-7717) | *Minor* | **Spark Standalone Web UI showing incorrect total memory, workers and cores**

I launched a Spark master in standalone mode in one of my host and then launched 3 workers on three different hosts. The workers successfully connected to my master and the Web UI showed the correct details. Specifically, the Web UI correctly shows that the total memory and the total cores available for the cluster.

However on one of the worker, I did a "kill -9 \<worker process id\>" and restarted the worker again. This time though, the master's Web UI shows incorrect total memory and number of cores. The total memory is shown to be 4\*n, where "n" is the memory in each worker. Also the total workers is shown as 4 and the total number of cores shown is incorrect, it shows 4\*c, where "c" is the number of cores on each worker.


---

* [SPARK-7715](https://issues.apache.org/jira/browse/SPARK-7715) | *Major* | **Update MLlib Programming Guide for 1.4**

Before the release, we need to update the MLlib Programming Guide.  Updates will include:
\* Add migration guide subsection.
\*\* Use the results of the QA audit JIRAs.
\* Check phrasing, especially in main sections (for outdated items such as "In this release, ..."


---

* [SPARK-7714](https://issues.apache.org/jira/browse/SPARK-7714) | *Major* | **SparkR tests should use more specific expectations than expect\_true**

SparkR's test use testthat's {{expect\_true(foo == bar)}}, but using expectations like {{expect\_equal(foo, bar)}} will give informative error messages if the assertion fails.  We should update the existing tests to use the more specific matchers, such as expect\_equal, expect\_is, expect\_identical, expect\_error, etc.

See http://r-pkgs.had.co.nz/tests.html for more documentation on testtthat expectation functions.


---

* [SPARK-7705](https://issues.apache.org/jira/browse/SPARK-7705) | *Minor* | **Cleanup of .sparkStaging directory fails if application is killed**

When a streaming application is killed while running on YARN the .sparkStaging directory is not cleaned up. Setting spark.yarn.preserve.staging.files=false does not help and still leaves the files around.

The changes in SPARK-7503 do not catch this case since there is no exception in the shutdown. When the application gets killed the AM gets told to shutdown and the shutdown hook is run but the clean up is not triggered.


---

* [SPARK-7699](https://issues.apache.org/jira/browse/SPARK-7699) | *Major* | **Dynamic allocation: initial executors may be canceled before first job**

spark.dynamicAllocation.minExecutors 2
spark.dynamicAllocation.initialExecutors  3
spark.dynamicAllocation.maxExecutors 4

Just run the spark-shell with above configurations, the initial executor number is 2.


---

* [SPARK-7691](https://issues.apache.org/jira/browse/SPARK-7691) | *Major* | **Use type-specific row accessor functions in CatalystTypeConverters' toScala functions**

CatalystTypeConverters's Catalyst row to Scala row converters access columns' values via the generic {{Row.get()}} call rather than using type-specific accessor methods.  If we refactor the internal converter interfaces slightly, we can pass the row and column number into the converter function and allow it to do its own type-specific field extraction, similar to what we do in UnsafeRowConverter.  This is a blocker for being able to unit test new operators that I'm developing as part of Project Tungsten, since those operators may output {{UnsafeRow}} instances which don't support the generic {{get()}}.


---

* [SPARK-7690](https://issues.apache.org/jira/browse/SPARK-7690) | *Major* | **MulticlassClassificationEvaluator for tuning Multiclass Classifiers**

Provide a MulticlassClassificationEvaluator with weighted F1-score to tune multiclass classifiers using Pipeline API.
MLLib already provides a MulticlassMetrics functionality which can be wrapped around a MulticlassClassificationEvaluator to expose weighted F1-score as metric.
The functionality could be similar to scikit(http://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1\_score.html)  in that we can support micro, macro and weighted versions of the F1-score (with weighted being default)


---

* [SPARK-7667](https://issues.apache.org/jira/browse/SPARK-7667) | *Major* | **MLlib Python API consistency check**

Check and ensure the MLlib Python API(class/method/parameter) consistent with Scala.

The following APIs are not consistent:
\* class
\* method
\*\* recommendation.MatrixFactorizationModel.predictAll() (Because it's a public API, so not change it)
\* parameter
\*\* feature.StandardScaler.fit()
\*\* many transform() function of feature module


---

* [SPARK-7666](https://issues.apache.org/jira/browse/SPARK-7666) | *Major* | **MLlib Python doc parity check**

Check then make the MLlib Python doc to be as complete as the Scala doc.


---

* [SPARK-7663](https://issues.apache.org/jira/browse/SPARK-7663) | *Minor* | **[MLLIB] feature.Word2Vec throws empty iterator error when the vocabulary size is zero**

mllib.feature.Word2Vec at line 442: https://github.com/apache/spark/blob/master/mllib/src/main/scala/org/apache/spark/mllib/feature/Word2Vec.scala#L442 uses `.head` to get the vector size. But it would throw an empty iterator error if the `minCount` is large enough to remove all words in the dataset.

But due to this is not a common scenario, so maybe we can ignore it. If so, we can close the issue directly. If not, I can add some code to print more elegant error hits.


---

* [SPARK-7657](https://issues.apache.org/jira/browse/SPARK-7657) | *Minor* | **[YARN] Show driver link in Spark UI**

Currently, the driver link does not show up in the application UI. It is painful to debug apps running in cluster mode if the link does not show up. Client mode is fine since the links are local to the client machine.

In YARN mode, it is possible to just get this from the YARN container report.


---

* [SPARK-7639](https://issues.apache.org/jira/browse/SPARK-7639) | *Major* | **Add Python API for Statistics.kernelDensity**

Add Python API for org.apache.spark.mllib.stat.Statistics.kernelDensity


---

* [SPARK-7637](https://issues.apache.org/jira/browse/SPARK-7637) | *Minor* | **StructType.merge slow with large nenormalised tables O(N2)**

StructType.merge does a linear scan through the left schema and for each element scans the right schema. This results in a O(N2) algorithm. 
I have found this to be very slow when dealing with large denormalised parquet files.
I would like to make a small change to this function to map the fields of both the left and right schemas resulting in O(N).
This has a sizable increase in performance for large denormalised schemas.

10000x10000 column merge 
2891ms Original  
32ms with mapped field approach.

This merge can be called many times depending upon the number of files that you need to merge the schemas for, compounding the performance.


---

* [SPARK-7635](https://issues.apache.org/jira/browse/SPARK-7635) | *Minor* | **SparkContextSchedulerCreationSuite tests may fail due to unrecognized UnsatisfiedLinkError message.**

When mesos is not available, these tests fail due to the difference in the UnsatisfiedLinkError message with IBM Java vs OpenJDK:

- mesos fine-grained \*\*\* FAILED \*\*\*
  "mesos (Not found in java.library.path)" did not contain "no mesos in" (SparkContextSchedulerCreationSuite.scala:162)
- mesos coarse-grained \*\*\* FAILED \*\*\*
  "mesos (Not found in java.library.path)" did not contain "no mesos in" (SparkContextSchedulerCreationSuite.scala:162)
- mesos with zookeeper \*\*\* FAILED \*\*\*
  "mesos (Not found in java.library.path)" did not contain "no mesos in" (SparkContextSchedulerCreationSuite.scala:162)

PR to be submitted shortly.


---

* [SPARK-7633](https://issues.apache.org/jira/browse/SPARK-7633) | *Major* | **Streaming Logistic Regression- Python bindings**

Add Python API for StreamingLogisticRegressionWithSGD


---

* [SPARK-7605](https://issues.apache.org/jira/browse/SPARK-7605) | *Major* | **Python API for ElementwiseProduct**

Python API for org.apache.spark.mllib.feature.ElementwiseProduct


---

* [SPARK-7604](https://issues.apache.org/jira/browse/SPARK-7604) | *Major* | **Python API for PCA and PCAModel**

Python API for org.apache.spark.mllib.feature.PCA and org.apache.spark.mllib.feature.PCAModel


---

* [SPARK-7583](https://issues.apache.org/jira/browse/SPARK-7583) | *Major* | **User guide update for RegexTokenizer**

Copied from [SPARK-7443]:
{quote}
Now that we have algorithms in spark.ml which are not in spark.mllib, we should start making subsections for the spark.ml API as needed. We can follow the structure of the spark.mllib user guide.
\* The spark.ml user guide can provide: (a) code examples and (b) info on algorithms which do not exist in spark.mllib.
\* We should not duplicate info in the spark.ml guides. Since spark.mllib is still the primary API, we should provide links to the corresponding algorithms in the spark.mllib user guide for more info.
{quote}

Note: I created a new subsection for links to spark.ml-specific guides in this JIRA's PR: [SPARK-7557]. This transformer can go within the new subsection. I'll try to get that PR merged ASAP.


---

* [SPARK-7562](https://issues.apache.org/jira/browse/SPARK-7562) | *Major* | **Improve error reporting for expression data type mismatch**

There is currently no error reporting for expression data types in analysis (we rely on "resolved" for that, which doesn't provide great error messages for types). It would be great to have that in checkAnalysis.

Ideally, it should be the responsibility of each Expression itself to specify the types it requires, and report errors that way. We would need to define a simple interface for that so each Expression can implement. The default implementation can just use the information provided by ExpectsInputTypes.expectedChildTypes. 

cc [~marmbrus] what we discussed offline today.


---

* [SPARK-7558](https://issues.apache.org/jira/browse/SPARK-7558) | *Major* | **Log test name when starting and finishing each test**

Right now it's really tough to interpret testing output because logs for different tests are interspersed in the same unit-tests.log file. This makes it particularly hard to diagnose flaky tests. This would be much easier if we logged the test name before and after every test (e.g. "Starting test XX", "Finished test XX"). Then you could get right to the logs.

I think one way to do this might be to create a custom test fixture that logs the test class name and then mix that into every test suite /cc [~joshrosen] for his superb knowledge of Scalatest.


---

* [SPARK-7555](https://issues.apache.org/jira/browse/SPARK-7555) | *Major* | **User guide update for ElasticNet**

Copied from [SPARK-7443]:
{quote}
Now that we have algorithms in spark.ml which are not in spark.mllib, we should start making subsections for the spark.ml API as needed. We can follow the structure of the spark.mllib user guide.
\* The spark.ml user guide can provide: (a) code examples and (b) info on algorithms which do not exist in spark.mllib.
\* We should not duplicate info in the spark.ml guides. Since spark.mllib is still the primary API, we should provide links to the corresponding algorithms in the spark.mllib user guide for more info.
{quote}


---

* [SPARK-7550](https://issues.apache.org/jira/browse/SPARK-7550) | *Blocker* | **Support setting the right schema & serde when writing to Hive metastore**

As of 1.4, Spark SQL does not properly set the table schema and serde when writing a table to Hive's metastore. Would be great to do that properly so users can use non-Spark SQL systems to read those tables.


---

* [SPARK-7533](https://issues.apache.org/jira/browse/SPARK-7533) | *Major* | **Decrease spacing between AM-RM heartbeats.**

The current default of spark.yarn.scheduler.heartbeat.interval-ms is 5 seconds.  This is really long.  For reference, the MR equivalent is 1 second.

To avoid noise and unnecessary communication, we could have a fast rate for when we're waiting for executors and a slow rate for when we're just heartbeating.


---

* [SPARK-7527](https://issues.apache.org/jira/browse/SPARK-7527) | *Minor* | **Wrong detection of REPL mode in ClosureCleaner**

If REPL class is not present on the classpath, the {{inIntetpreter}} boolean switch shall be {{false}}, not {{true}} at: https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/util/ClosureCleaner.scala#L247


---

* [SPARK-7524](https://issues.apache.org/jira/browse/SPARK-7524) | *Major* | **add configs for keytab and principal, move originals to internal**

As spark now supports long running service by updating tokens for namenode, but only accept parameters passed with "--k=v" format which is not very convinient.

I wanna add spark.\* configs in properties file and system property


---

* [SPARK-7515](https://issues.apache.org/jira/browse/SPARK-7515) | *Minor* | **Update documentation for PySpark on YARN with cluster mode**

Now PySpark on YARN with cluster mode is supported so let's update doc.


---

* [SPARK-7514](https://issues.apache.org/jira/browse/SPARK-7514) | *Major* | **Add MinMaxScaler to feature transformation**

Add a popular scaling method to feature component, which is commonly known as min-max normalization or Rescaling.

Core function is,
Normalized( x ) = (x - min) / (max - min) \* scale + newBase

where newBase and scale are parameters of the VectorTransformer. newBase is the new minimum number for the feature, and scale controls the range after transformation. This is a little complicated than the basic MinMax normalization, yet it provides flexibility so that users can control the range more specifically. like [0.1, 0.9] in some NN application.

for case that max == min, 0.5 is used as the raw value.

reference:
 http://en.wikipedia.org/wiki/Feature\_scaling
http://stn.spotfire.com/spotfire\_client\_help/index.htm#norm/norm\_scale\_between\_0\_and\_1.htm


---

* [SPARK-7497](https://issues.apache.org/jira/browse/SPARK-7497) | *Critical* | **test\_count\_by\_value\_and\_window is flaky**

Saw this test failure in https://amplab.cs.berkeley.edu/jenkins/job/SparkPullRequestBuilder/32268/console

{code}
======================================================================
FAIL: test\_count\_by\_value\_and\_window (\_\_main\_\_.WindowFunctionTests)
----------------------------------------------------------------------
Traceback (most recent call last):
  File "pyspark/streaming/tests.py", line 418, in test\_count\_by\_value\_and\_window
    self.\_test\_func(input, func, expected)
  File "pyspark/streaming/tests.py", line 133, in \_test\_func
    self.assertEqual(expected, result)
AssertionError: Lists differ: [[1], [2], [3], [4], [5], [6], [6], [6], [6], [6]] != [[1], [2], [3], [4], [5], [6], [6], [6]]

First list contains 2 additional elements.
First extra element 8:
[6]

- [[1], [2], [3], [4], [5], [6], [6], [6], [6], [6]]
?                                     ----------

+ [[1], [2], [3], [4], [5], [6], [6], [6]]

----------------------------------------------------------------------
{code}


---

* [SPARK-7446](https://issues.apache.org/jira/browse/SPARK-7446) | *Minor* | **Inverse transform for StringIndexer**

It is useful to convert the encoded indices back to their string representation for result inspection. We can add a parameter to StringIndexer/StringIndexModel for this.


---

* [SPARK-7444](https://issues.apache.org/jira/browse/SPARK-7444) | *Minor* | **Eliminate noisy css warn/error logs for UISeleniumSuite**

Eliminate the following noisy logs for {{UISeleniumSuite}}:

{code}
15/05/07 10:09:50.196 pool-1-thread-1-ScalaTest-running-UISeleniumSuite WARN DefaultCssErrorHandler: CSS error: 'http://192.168.0.170:4040/static/bootstrap.min.css' [793:167] Error in style rule. (Invalid token "\*". Was expecting one of: \<EOF\>, \<S\>, \<IDENT\>, "}", ";".)
15/05/07 10:09:50.196 pool-1-thread-1-ScalaTest-running-UISeleniumSuite WARN DefaultCssErrorHandler: CSS warning: 'http://192.168.0.170:4040/static/bootstrap.min.css' [793:167] Ignoring the following declarations in this rule.
15/05/07 10:09:50.197 pool-1-thread-1-ScalaTest-running-UISeleniumSuite WARN DefaultCssErrorHandler: CSS error: 'http://192.168.0.170:4040/static/bootstrap.min.css' [799:325] Error in style rule. (Invalid token "\*". Was expecting one of: \<EOF\>, \<S\>, \<IDENT\>, "}", ";".)
15/05/07 10:09:50.197 pool-1-thread-1-ScalaTest-running-UISeleniumSuite WARN DefaultCssErrorHandler: CSS warning: 'http://192.168.0.170:4040/static/bootstrap.min.css' [799:325] Ignoring the following declarations in this rule.
15/05/07 10:09:50.198 pool-1-thread-1-ScalaTest-running-UISeleniumSuite WARN DefaultCssErrorHandler: CSS error: 'http://192.168.0.170:4040/static/bootstrap.min.css' [805:18] Error in style rule. (Invalid token "\*". Was expecting one of: \<EOF\>, \<S\>, \<IDENT\>, "}", ";".)
15/05/07 10:09:50.198 pool-1-thread-1-ScalaTest-running-UISeleniumSuite WARN DefaultCssErrorHandler: CSS warning: 'http://192.168.0.170:4040/static/bootstrap.min.css' [805:18] Ignoring the following declarations in this rule.
{code}


---

* [SPARK-7440](https://issues.apache.org/jira/browse/SPARK-7440) | *Major* | **Remove physical Distinct operator in favor of Aggregate**

We can just rewrite distinct using groupby (i.e. aggregate operator).


---

* [SPARK-7426](https://issues.apache.org/jira/browse/SPARK-7426) | *Minor* | **spark.ml AttributeFactory.fromStructField should allow other NumericTypes**

It currently only supports DoubleType, but it should support others, at least for fromStructField (importing into ML attribute format, rather than exporting).


---

* [SPARK-7423](https://issues.apache.org/jira/browse/SPARK-7423) | *Minor* | **spark.ml Classifier predict should not convert vectors to dense format**

spark.ml.classification.ClassificationModel and ProbabilisticClassificationModel both use DenseVector.argmax to implement prediction (computing the prediction from the rawPrediction or probability Vectors).  It would be best to implement argmax for Vector and SparseVector and use Vector.argmax, rather than converting Vectors to dense format.


---

* [SPARK-7422](https://issues.apache.org/jira/browse/SPARK-7422) | *Minor* | **Add argmax to Vector, SparseVector**

DenseVector has an argmax method which is currently private to Spark.  It would be nice to add that method to Vector and SparseVector.  Adding it to SparseVector would require being careful about handling the inactive elements correctly and efficiently.

We should make argmax public and add unit tests.


---

* [SPARK-7419](https://issues.apache.org/jira/browse/SPARK-7419) | *Critical* | **Flaky test: o.a.s.streaming.CheckpointSuite**

Failing with error messages like
{code}
5 did not equal 7 Number of outputs do not match
{code}

Various tests in the suite seem to be failing with similar error messages:
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB\_JENKINS\_BUILD\_PROFILE=hadoop2.3,label=centos/2228/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB\_JENKINS\_BUILD\_PROFILE=hadoop2.0,label=centos/2230/


---

* [SPARK-7389](https://issues.apache.org/jira/browse/SPARK-7389) | *Major* | **Tachyon integration improvement**

Two main changes:

1. Add two functions in ExternalBlockManager, which are putValues and getValues, because the implementation may not rely on the putBytes and getBytes

2. improve Tachyon integration.
Currently, when putting data into Tachyon, Spark first serialize all data in one partition into a ByteBuffer, and then write into Tachyon, this will use much memory and increase GC overhead

when getting data from Tachyon, getValues depends on getBytes, which also read all data into On heap byte arry, and result in much memory usage.
This PR changes the approach of the two functions, make them read / write data by stream to reduce memory usage.

In our testing, when data size is huge, this patch reduces about 30% GC time and 70% full GC time, and total execution time reduces about 10%


---

* [SPARK-7387](https://issues.apache.org/jira/browse/SPARK-7387) | *Minor* | **CrossValidator example code in Python**

We should add example code for CrossValidator after SPARK-6940 is merged. This should be similar to the CrossValidator example in Scala/Java.


---

* [SPARK-7368](https://issues.apache.org/jira/browse/SPARK-7368) | *Major* | **add QR decomposition for RowMatrix**

Add QR decomposition for RowMatrix.

There's a great distributed algorithm for QR decomposition, which I'm currently referring to.

Austin R. Benson, David F. Gleich, James Demmel. "Direct QR factorizations for tall-and-skinny matrices in MapReduce architectures", 2013 IEEE International Conference on Big Data


---

* [SPARK-7357](https://issues.apache.org/jira/browse/SPARK-7357) | *Minor* | **Improving HBaseTest example**

Minor improvement to HBaseTest example, when Hbase related configurations e.g: zookeeper quorum, zookeeper client port or zookeeper.znode.parent are not set to default (localhost:2181), connection to zookeeper might hang as shown in following stack

15/03/26 18:31:20 INFO zookeeper.ZooKeeper: Initiating client connection, connectString=xxx.xxx.xxx:2181 sessionTimeout=90000 watcher=hconnection-0x322a4437, quorum=xxx.xxx.xxx:2181, baseZNode=/hbase
15/03/26 18:31:21 INFO zookeeper.ClientCnxn: Opening socket connection to server 9.30.94.121:2181. Will not attempt to authenticate using SASL (unknown error)
15/03/26 18:31:21 INFO zookeeper.ClientCnxn: Socket connection established to xxx.xxx.xxx/9.30.94.121:2181, initiating session
15/03/26 18:31:21 INFO zookeeper.ClientCnxn: Session establishment complete on server xxx.xxx.xxx/9.30.94.121:2181, sessionid = 0x14c53cd311e004b, negotiated timeout = 40000
15/03/26 18:31:21 INFO client.ZooKeeperRegistry: ClusterId read in ZooKeeper is null

this is due to hbase-site.xml is not placed on spark class path.


---

* [SPARK-7293](https://issues.apache.org/jira/browse/SPARK-7293) | *Critical* | **Report memory used in aggregations and joins**

It would be good to service to users in the Spark UI how much memory we allocate doing aggregations for a particular query.


---

* [SPARK-7289](https://issues.apache.org/jira/browse/SPARK-7289) | *Major* | **Combine Limit and Sort to avoid total ordering**

Optimize following sql

select key from (select \* from testData order by key) t limit 5

from 

== Parsed Logical Plan ==
'Limit 5
 'Project ['key]
  'Subquery t
   'Sort ['key ASC], true
    'Project [\*]
     'UnresolvedRelation [testData], None

== Analyzed Logical Plan ==
Limit 5
 Project [key#0]
  Subquery t
   Sort [key#0 ASC], true
    Project [key#0,value#1]
     Subquery testData
      LogicalRDD [key#0,value#1], MapPartitionsRDD[1] 

== Optimized Logical Plan ==
Limit 5
 Project [key#0]
  Sort [key#0 ASC], true
   LogicalRDD [key#0,value#1], MapPartitionsRDD[1] 
== Physical Plan ==
Limit 5
 Project [key#0]
  Sort [key#0 ASC], true
   Exchange (RangePartitioning [key#0 ASC], 5), []
    PhysicalRDD [key#0,value#1], MapPartitionsRDD[1] 

to

== Parsed Logical Plan ==
'Limit 5
 'Project ['key]
  'Subquery t
   'Sort ['key ASC], true
    'Project [\*]
     'UnresolvedRelation [testData], None

== Analyzed Logical Plan ==
Limit 5
 Project [key#0]
  Subquery t
   Sort [key#0 ASC], true
    Project [key#0,value#1]
     Subquery testData
      LogicalRDD [key#0,value#1], MapPartitionsRDD[1]

== Optimized Logical Plan ==
Project [key#0]
 Limit 5
  Sort [key#0 ASC], true
   LogicalRDD [key#0,value#1], MapPartitionsRDD[1] 

== Physical Plan ==
Project [key#0]
 TakeOrdered 5, [key#0 ASC]
  PhysicalRDD [key#0,value#1], MapPartitionsRDD[1]


---

* [SPARK-7287](https://issues.apache.org/jira/browse/SPARK-7287) | *Critical* | **Flaky test: o.a.s.deploy.SparkSubmitSuite --packages**

Error message was not helpful (did not complete within 60 seconds or something).

Observed only in master:

https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB\_JENKINS\_BUILD\_PROFILE=hadoop1.0,label=centos/2239/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-SBT/AMPLAB\_JENKINS\_BUILD\_PROFILE=hadoop2.0,label=centos/2238/
https://amplab.cs.berkeley.edu/jenkins/job/Spark-Master-Maven-pre-YARN/hadoop.version=1.0.4,label=centos/2163/
...


---

* [SPARK-7265](https://issues.apache.org/jira/browse/SPARK-7265) | *Trivial* | **Improving documentation for Spark SQL Hive support**

miscellaneous documentation improvement for Spark SQL Hive support, Yarn cluster deployment.


---

* [SPARK-7261](https://issues.apache.org/jira/browse/SPARK-7261) | *Blocker* | **Change default log level to WARN in the REPL**

We should add a log4j properties file for the repl (log4j-defaults-repl.properties) that has the level of warning. The main reason for doing this is that we now display nice progress bars in the REPL so the need for task level INFO messages is much less.

The best way to accomplish this is the following:
1. Add a second logging defaults file called log4j-defaults-repl.properties that has log level WARN. https://github.com/apache/spark/blob/branch-1.4/core/src/main/resources/org/apache/spark/log4j-defaults.properties
2. When logging is initialized, check whether you are inside the REPL. If so, then use that one:
https://github.com/apache/spark/blob/branch-1.4/core/src/main/scala/org/apache/spark/Logging.scala#L124
3. The printed message should say something like:
Using Spark's repl log4j profile: org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel("INFO")


---

* [SPARK-7254](https://issues.apache.org/jira/browse/SPARK-7254) | *Major* | **Extend PIC to handle Graphs directly**

We should extend the PowerIterationClustering API to handle Graphs.  Users can do spectral clustering on graphs using PIC currently, but they must handle the boilerplate of converting the Graph to an RDD for PIC, running PIC, and then matching the results back with their Graph.


---

* [SPARK-7235](https://issues.apache.org/jira/browse/SPARK-7235) | *Major* | **Refactor the GroupingSet implementation**

The logical plan `Expand` takes the `output` as constructor argument, which break the references chain for logical plan optimization. We need to refactor the code.


---

* [SPARK-7199](https://issues.apache.org/jira/browse/SPARK-7199) | *Major* | **Add date and timestamp support to UnsafeRow**

We should add date and timestamp support to UnsafeRow.  This should be fairly easy, as both data types are fixed-length.


---

* [SPARK-7186](https://issues.apache.org/jira/browse/SPARK-7186) | *Blocker* | **Decouple internal Row from external Row**

Currently, we use o.a.s.sql.Row both internally and externally. The external interface is wider than what the internal needs because it is designed to facilitate end-user programming. This design has proven to be very error prone and cumbersome for internal Row implementations.

As a first step, we should just create an InternalRow interface in the catalyst module, which is identical to the current Row interface. And we should switch all internal operators/expressions to use this InternalRow instead. When we need to expose Row, we convert the InternalRow implementation into Row for users.

After this, we can start removing methods that don't make sense for InternalRow (in a separate ticket). This is probably one of the most important refactoring in Spark 1.5.


---

* [SPARK-7184](https://issues.apache.org/jira/browse/SPARK-7184) | *Major* | **Investigate turning codegen on by default**

If it is not the default, users get suboptimal performance out of the box, and the codegen path falls behind the interpreted path over time.

The best option might be to have only the codegen path.


---

* [SPARK-7180](https://issues.apache.org/jira/browse/SPARK-7180) | *Major* | **SerializationDebugger fails with ArrayOutOfBoundsException**

Simple reproduction:
{code}
class Parent extends Serializable {
  val a = "a"
  val b = "b"
}

class Child extends Parent with Serializable {
  val c = Array(1)
  val d = Array(2)
  val e = Array(3)
  val f = Array(4)
  val g = Array(5)
  val o = new Object
}

// ArrayOutOfBoundsException
SparkEnv.get.closureSerializer.newInstance().serialize(new Child)
{code}

I dug into this a little and found that we are trying to fill the fields of `Parent` with the values of `Child`. See the following output I generated by adding println's everywhere:
{code}
\* Visiting object org.apache.spark.serializer.Child@2c3299f6 of type org.apache.spark.serializer.Child
  - Found 2 class data slot descriptions
  - Looking at desc #1: org.apache.spark.serializer.Parent: static final long serialVersionUID = 3254964199136071914L;
    - Found 2 fields
      - Ljava/lang/String; a
      - Ljava/lang/String; b
    - getObjFieldValues: 
      - [I@23faa614
      - [I@1cad7d80
      - [I@420a6d35
      - [I@3a87d472
      - [I@2b8ca663
      - java.lang.Object@1effc3eb
{code}
SerializationDebugger#visitSerializable found two fields that belong to the parents, but it tried to cram the child's values into these two fields. The mismatch of number of fields here throws the ArrayOutOfBoundExceptions as a result. The culprit is this line: https://github.com/apache/spark/blob/4d9e560b5470029143926827b1cb9d72a0bfbeff/core/src/main/scala/org/apache/spark/serializer/SerializationDebugger.scala#L150, which runs reflection on the object `Child` even when it's considering the description for `Parent`.

I ran into this when trying to serialize a test suite that extends `FunSuite` (don't ask why).


---

* [SPARK-7171](https://issues.apache.org/jira/browse/SPARK-7171) | *Minor* | **Allow for more flexible use of metric sources**

With the current API, the user is allowed to add a custom metric source by providing its class in metrics configuration. Metrics themselves are provided by Codahale and therefore they allow to register multiple metrics in a single source. Basically we can break the available types of metrics into two types: "push" and "pull" - by push metrics I mean that some execution code updates the metric by itself either periodically or every n events. On the other hand, the pull metrics include some function which pulls the data from the execution environment, when triggered. 

h5.Problem
The metric source is instantiated and registered during initialisation. Then, the user has no way to access the instantiated object. It is also almost impossible to access the execution environment of the current task. Therefore, the user who wanted to provide his own {{RDD}} implementation along with a dedicated metrics source, would find it very difficult to do this in a safe, concise and elegant way.

h5.Proposed solution
At least, for the "push" metrics, it would be nice to be able to retrieve the metrics source of particular type or with particular id from {{TaskContext}}. It would allow custom tasks to update various metrics and would greatly improve the usability of metrics.
This could be achieved quite easily since {{TaskContext}} is created by {{Executor}}, which has access to the metrics system, it would inject some method to retrieve the particular metrics source. 
This solution wouldn't change the current API, but just introduce one more method in {{TaskContext}}.


---

* [SPARK-7169](https://issues.apache.org/jira/browse/SPARK-7169) | *Minor* | **Allow to specify metrics configuration more flexibly**

Metrics are configured in {{metrics.properties}} file. Path to this file is specified in {{SparkConf}} at a key {{spark.metrics.conf}}. The property is read when {{MetricsSystem}} is created which means, during {{SparkEnv}} initialisation. 

h5.Problem
When the user runs his application he has no way to provide the metrics configuration for executors. Although one can specify the path to metrics configuration file (1) the path is common for all the nodes and the client machine so there is implicit assumption that all the machines has same file in the same location, and (2) actually the user needs to copy the file manually to the worker nodes because the file is read before the user files are populated to the executor local directories. All of this makes it very difficult to play with the metrics configuration.

h5. Proposed solution
I think that the easiest and the most consistent solution would be to move the configuration from a separate file directly to {{SparkConf}}. We may prefix all the configuration settings from the metrics configuration by, say {{spark.metrics.props}}. For the backward compatibility, these properties would be loaded from the specified as it works now. Such a solution doesn't change the API so maybe it could be even included in patch release of Spark 1.2 and Spark 1.3.

Appreciate any feedback.


---

* [SPARK-7161](https://issues.apache.org/jira/browse/SPARK-7161) | *Minor* | **Provide REST api to download event logs from History Server**

The idea is to tar up the logs and return the tar.gz file using a REST api. This can be used for debugging even after the app is done.

I am planning to take a look at this.


---

* [SPARK-7158](https://issues.apache.org/jira/browse/SPARK-7158) | *Blocker* | **collect and take return different results**

Reported by [~rams]

{code}
import java.util.UUID
import org.apache.spark.sql.\_
import org.apache.spark.sql.types.\_
val rdd = sc.parallelize(List(1,2,3), 2)
val schema = StructType(List(StructField("index",IntegerType,true)))
val df = sqlContext.createDataFrame(rdd.map(p =\> Row(p)), schema)
def id:() =\> String = () =\> {UUID.randomUUID().toString()}
def square:Int =\> Int = (x: Int) =\> {x \* x}
val dfWithId = df.withColumn("id",callUDF(id, StringType)).cache() //expect the ID to have materialized at this point
dfWithId.collect()
//res0: Array[org.apache.spark.sql.Row] = Array([1,43c7b8e2-b4a3-43ee-beff-0bb4b7d6c1b1], [2,efd061be-e8cc-43fa-956e-cfd6e7355982], [3,79b0baab-627c-4761-af0d-8995b8c5a125])

val dfWithIdAndSquare = dfWithId.withColumn("square",callUDF(square, IntegerType, col("index")))
dfWithIdAndSquare.collect()
//res1: Array[org.apache.spark.sql.Row] = Array([1,a3b2e744-a0a1-40fe-8133-87a67660b4ab,1], [2,0a7052a0-6071-4ef5-a25a-2670248ea5cd,4], [3,209f269e-207a-4dfd-a186-738be5db2eff,9])
//why are the IDs in lines 11 and 15 different?
{code}

The randomly generated IDs are the same if show (which uses take under the hood) is used instead of collect.


---

* [SPARK-7153](https://issues.apache.org/jira/browse/SPARK-7153) | *Major* | **support Long type ordinal in GetItem**

In GetItem, we will cast the ordinal into Int first. However, if the ordinal is Long type, execution will fail even the value of ordinal meets the requirement. The reason is boxing. In java, we can convert long to int, but can't convert Long to Integer.
{code}
test("get item") {
  jsonRDD(sparkContext.makeRDD(
    """{"a": [1,2,3], "b": 2}""" :: Nil)).registerTempTable("t")
  checkAnswer(sql("SELECT a[b] FROM t"), Row(3))
}
{code}
This test will fail as "b" is inferred as Long type.


---

* [SPARK-7137](https://issues.apache.org/jira/browse/SPARK-7137) | *Trivial* | **Add checkInputColumn back to Params and print more info**

In the PR for [https://issues.apache.org/jira/browse/SPARK-5957], Params.checkInputColumn was moved to SchemaUtils and renamed to checkColumnType.  The downside is that it no longer has access to the parameter info, so it cannot state which input column parameter was incorrect.

We should keep checkColumnType but also add checkInputColumn back to Params.  It should print out the parameter name and description.  Internally, it may call checkColumnType.


---

* [SPARK-7131](https://issues.apache.org/jira/browse/SPARK-7131) | *Major* | **Move tree,forest implementation from spark.mllib to spark.ml**

We want to change and improve the spark.ml API for trees and ensembles, but we cannot change the old API in spark.mllib.  To support the changes we want to make, we should move the implementation from spark.mllib to spark.ml.  We will generalize and modify it, but will also ensure that we do not change the behavior of the old API.

This JIRA should be done in several PRs, in this order:
1. Copy the implementation over to spark.ml and change the spark.ml classes to use that implementation, rather than calling the spark.mllib implementation.  The current spark.ml tests will ensure that the 2 implementations learn exactly the same models.  Note: This should include performance testing to make sure the updated code does not have any regressions.
2. Remove the spark.mllib implementation, and make the spark.mllib APIs wrappers around the spark.ml implementation.  The spark.ml tests will again ensure that we do not change any behavior.
3. Move the unit tests to spark.ml, and change the spark.mllib unit tests to verify model equivalence.

After these updates, we can more safely generalize and improve the spark.ml implementation.


---

* [SPARK-7127](https://issues.apache.org/jira/browse/SPARK-7127) | *Minor* | **Broadcast spark.ml tree ensemble models for predict**

GBTRegressor/Classifier and RandomForestRegressor/Classifier should broadcast models and then predict.  This will mean overriding transform().

Note: Try to reduce duplicated code via the TreeEnsembleModel abstraction.


---

* [SPARK-7119](https://issues.apache.org/jira/browse/SPARK-7119) | *Critical* | **ScriptTransform doesn't consider the output data type**

{code:sql}
from (from src select transform(key, value) using 'cat' as (thing1 int, thing2 string)) t select thing1 + 2;
{code}

{noformat}
15/04/24 00:58:55 ERROR CliDriver: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 0.0 failed 1 times, most recent failure: Lost task 0.0 in stage 0.0 (TID 0, localhost): java.lang.ClassCastException: org.apache.spark.sql.types.UTF8String cannot be cast to java.lang.Integer
	at scala.runtime.BoxesRunTime.unboxToInt(BoxesRunTime.java:106)
	at scala.math.Numeric$IntIsIntegral$.plus(Numeric.scala:57)
	at org.apache.spark.sql.catalyst.expressions.Add.eval(arithmetic.scala:127)
	at org.apache.spark.sql.catalyst.expressions.Alias.eval(namedExpressions.scala:118)
	at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:68)
	at org.apache.spark.sql.catalyst.expressions.InterpretedMutableProjection.apply(Projection.scala:52)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$$anon$11.next(Iterator.scala:328)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at scala.collection.AbstractIterator.foreach(Iterator.scala:1157)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at scala.collection.AbstractIterator.to(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at scala.collection.AbstractIterator.toBuffer(Iterator.scala:1157)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at scala.collection.AbstractIterator.toArray(Iterator.scala:1157)
	at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:819)
	at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:819)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1618)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1618)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:63)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:209)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
{noformat}


---

* [SPARK-7114](https://issues.apache.org/jira/browse/SPARK-7114) | *Major* | **parse error for DataFrame.filter after aggregate**

DataFrame.filter has 2 overloaded versions. One of it accept String parameter to represent condition expression.
{code}
val df = ... // df has 2 columns: key, value
val agg = df.groupBy("key").count()
agg.filter(df("count") \> 1) // this success
agg.filter("count \> 1") // this failed
{code}
the error message is:
{code}
[1.7] failure: ``('' expected but `\>' found

count \> 1
      ^
java.lang.RuntimeException: [1.7] failure: ``('' expected but `\>' found

count \> 1
      ^
{code}


---

* [SPARK-7088](https://issues.apache.org/jira/browse/SPARK-7088) | *Critical* | **[REGRESSION] Spark 1.3.1 breaks analysis of third-party logical plans**

We're using some custom logical plans. We are now migrating from Spark 1.3.0 to 1.3.1 and found a few incompatible API changes. All of them seem to be in internal code, so we understand that. But now the ResolveReferences rule, that used to work with third-party logical plans just does not work, without any possible workaround that I'm aware other than just copying ResolveReferences rule and using it with our own fix.

The change in question is this section of code:
{code}
        }.headOption.getOrElse { // Only handle first case, others will be fixed on the next pass.
          sys.error(
            s"""
              \|Failure when resolving conflicting references in Join:
              \|$plan
              \|
              \|Conflicting attributes: ${conflictingAttributes.mkString(",")}
              """.stripMargin)
        }
{code}

Which causes the following error on analysis:

{code}
Failure when resolving conflicting references in Join:
'Project ['l.name,'r.name,'FUNC1('l.node,'r.node) AS c2#37,'FUNC2('l.node,'r.node) AS c3#38,'FUNC3('r.node,'l.node) AS c4#39]
 'Join Inner, None
  Subquery l
   Subquery h
    Project [name#12,node#36]
     CustomPlan H, u, (p#13L = s#14L), [ord#15 ASC], IS NULL p#13L, node#36
      Subquery v
       Subquery h\_src
        LogicalRDD [name#12,p#13L,s#14L,ord#15], MapPartitionsRDD[1] at mapPartitions at ExistingRDD.scala:37
  Subquery r
   Subquery h
    Project [name#40,node#36]
     CustomPlan H, u, (p#41L = s#42L), [ord#43 ASC], IS NULL pred#41L, node#36
      Subquery v
       Subquery h\_src
        LogicalRDD [name#40,p#41L,s#42L,ord#43], MapPartitionsRDD[1] at mapPartitions at ExistingRDD.scala:37
{code}


---

* [SPARK-7078](https://issues.apache.org/jira/browse/SPARK-7078) | *Major* | **Cache-aware binary processing in-memory sort**

A cache-friendly sort algorithm that can be used eventually for:
\* sort-merge join
\* shuffle

See the old alpha sort paper: http://research.microsoft.com/pubs/68249/alphasort.doc

Note that state-of-the-art for sorting has improved quite a bit, but we can easily optimize the sorting algorithm itself later.


---

* [SPARK-7075](https://issues.apache.org/jira/browse/SPARK-7075) | *Major* | **Project Tungsten (Spark 1.5 Phase 1)**

Based on our observation, majority of Spark workloads are not bottlenecked by I/O or network, but rather CPU and memory. This project focuses on 3 areas to improve the efficiency of memory and CPU for Spark applications, to push performance closer to the limits of the underlying hardware.

\*Memory Management and Binary Processing\*
- Avoiding non-transient Java objects (store them in binary format), which reduces GC overhead.
- Minimizing memory usage through denser in-memory data format, which means we spill less.
- Better memory accounting (size of bytes) rather than relying on heuristics
- For operators that understand data types (in the case of DataFrames and SQL), work directly against binary format in memory, i.e. have no serialization/deserialization

\*Cache-aware Computation\*
- Faster sorting and hashing for aggregations, joins, and shuffle

\*Code Generation\*
- Faster expression evaluation and DataFrame/SQL operators
- Faster serializer


Several parts of project Tungsten leverage the DataFrame model, which gives us more semantics about the application. We will also retrofit the improvements onto Sparks RDD API whenever possible.

This epic tracks work items for Spark 1.5. More tickets can be found in:

SPARK-7075: Tungsten-related work in Spark 1.5
SPARK-9697: Tungsten-related work in Spark 1.6


---

* [SPARK-7067](https://issues.apache.org/jira/browse/SPARK-7067) | *Major* | **Can't resolve nested column in ORDER BY**

In order to avoid breaking existing HiveQL queries, the current way we resolve column in ORDER BY is: first resolve based on what comes from the select clause and then fall back on its child only when this fails.

However, this case will fail:
{code}
test("orderby queries") {
  jsonRDD(sparkContext.makeRDD(
    """{"a": {"b": [{"c": 1}]}, "b": [{"d": 1}]}""" :: Nil)).registerTempTable("t")
  sql("SELECT a.b FROM t ORDER BY b[0].d").queryExecution.analyzed
}
{code}

As hive doesn't support resolve ORDER BY attribute not exist in select clause, so this problem is spark sql only.


---

* [SPARK-7063](https://issues.apache.org/jira/browse/SPARK-7063) | *Minor* | **Update lz4 for Java 7 to avoid: when lz4 compression is used, it causes core dump**

this issue is initially noticed by using IBM JDK, below please find the stack track of this issue, caused by violating the rule in critical section. 

#0 0x000000314340f3cb in raise () from /service/pmrs/45638/20/lib64/libpthread.so.0
#1 0x00007f795b0323be in j9dump\_create () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9prt27.so
#2 0x00007f795a88ba2a in doSystemDump () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9dmp27.so
#3 0x00007f795b0405d5 in j9sig\_protect () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9prt27.so
#4 0x00007f795a88a1fd in runDumpFunction () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9dmp27.so
#5 0x00007f795a88dbab in runDumpAgent () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9dmp27.so
#6 0x00007f795a8a1c49 in triggerDumpAgents () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9dmp27.so
#7 0x00007f795a4518fe in doTracePoint () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9trc27.so
#8 0x00007f795a45210e in j9Trace () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9trc27.so
#9 0x00007f79590e46e1 in MM\_StandardAccessBarrier::jniReleasePrimitiveArrayCritical(J9VMThread\*, \_jarray\*, void\*, int) ()
from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9gc27.so
#10 0x00007f7938bc397c in Java\_net\_jpountz\_lz4\_LZ4JNI\_LZ4\_1compress\_1limitedOutput () from /service/pmrs/45638/20/tmp/liblz4-java7155003924599399415.so
#11 0x00007f795b707149 in VMprJavaSendNative () from /service/pmrs/45638/20/opt/ibm/biginsights/jdk/jre/lib/amd64/compressedrefs/libj9vm27.so
#12 0x0000000000000000 in ?? ()

this is an issue introduced by a bug in net.jpountz.lz4.lz4-1.2.0.jar, and fixed in 1.3.0 version.  Sun JDK /Open JDK doesn't complain this issue, but this issue will trigger assertion failure when IBM JDK is used. here is the link to the fix 
https://github.com/jpountz/lz4-java/commit/07229aa2f788229ab4f50379308297f428e3d2d2


---

* [SPARK-7050](https://issues.apache.org/jira/browse/SPARK-7050) | *Minor* | **Fix Python Kafka test assembly jar not found issue under Maven build**

The behavior of {{mvn package}} and {{sbt kafka-assembly/assembly}} under kafka-assembly module is different, sbt will generate an assembly jar under target/scala-version/, while mvn generates this jar under target/, which will make python Kafka streaming unit test fail to find the related jar.


---

* [SPARK-7045](https://issues.apache.org/jira/browse/SPARK-7045) | *Minor* | **Word2Vec: avoid intermediate representation when creating model**

Word2VecModel now stores the word vectors as a single, flat array; Word2Vec does as well.  However, when Word2Vec creates the model, it builds an intermediate representation.  We should skip that intermediate representation.

However, it will be nice to create a public constructor for Word2VecModel which takes that intermediate representation (a Map from String words to their Vectors), since it's a user-friendly representation.


---

* [SPARK-7042](https://issues.apache.org/jira/browse/SPARK-7042) | *Minor* | **Spark version of akka-actor\_2.11 is not compatible with the official akka-actor\_2.11 2.3.x**

When connecting to a remote Spark cluster (that runs Spark branch-1.3 built with Scala 2.11) from an application that uses akka 2.3.9 I get the following error:

{noformat}
2015-04-22 09:01:38,924 - [WARN] - [akka.remote.ReliableDeliverySupervisor] [sparkDriver-akka.actor.default-dispatcher-5] -
Association with remote system [akka.tcp://sparkExecutor@server:59007] has failed, address is now gated for [5000] ms.
Reason is: [akka.actor.Identify; local class incompatible: stream classdesc serialVersionUID = -213377755528332889, local class serialVersionUID = 1].
{noformat}

It looks like akka-actor\_2.11 2.3.4-spark that is used by Spark has been built using Scala compiler 2.11.0 that ignores SerialVersionUID annotations (see https://issues.scala-lang.org/browse/SI-8549).

The following steps can resolve the issue:
- re-build the custom akka library that is used by Spark with the more recent version of Scala compiler (e.g. 2.11.6) 
- deploy a new version (e.g. 2.3.4.1-spark) to a maven repo
- update version of akka used by spark (master and 1.3 branch)

I would also suggest to upgrade to the latest version of akka 2.3.9 (or 2.3.10 that should be released soon).


---

* [SPARK-7026](https://issues.apache.org/jira/browse/SPARK-7026) | *Major* | **LeftSemiJoin can not work when it  has both equal condition and not equal condition.**

Run sql like that 
{panel}
select \*
from
web\_sales ws1
left semi join
web\_sales ws2
on ws1.ws\_order\_number = ws2.ws\_order\_number
and ws1.ws\_warehouse\_sk \<\> ws2.ws\_warehouse\_sk 
{panel}
 then get an exception
{panel}
Couldn't find ws\_warehouse\_sk#287 in {ws\_sold\_date\_sk#237,ws\_sold\_time\_sk#238,ws\_ship\_date\_sk#239,ws\_item\_sk#240,ws\_bill\_customer\_sk#241,ws\_bill\_cdemo\_sk#242,ws\_bill\_hdemo\_sk#243,ws\_bill\_addr\_sk#244,ws\_ship\_customer\_sk#245,ws\_ship\_cdemo\_sk#246,ws\_ship\_hdemo\_sk#247,ws\_ship\_addr\_sk#248,ws\_web\_page\_sk#249,ws\_web\_site\_sk#250,ws\_ship\_mode\_sk#251,ws\_warehouse\_sk#252,ws\_promo\_sk#253,ws\_order\_number#254,ws\_quantity#255,ws\_wholesale\_cost#256,ws\_list\_price#257,ws\_sales\_price#258,ws\_ext\_discount\_amt#259,ws\_ext\_sales\_price#260,ws\_ext\_wholesale\_cost#261,ws\_ext\_list\_price#262,ws\_ext\_tax#263,ws\_coupon\_amt#264,ws\_ext\_ship\_cost#265,ws\_net\_paid#266,ws\_net\_paid\_inc\_tax#267,ws\_net\_paid\_inc\_ship#268,ws\_net\_paid\_inc\_ship\_tax#269,ws\_net\_profit#270,ws\_sold\_date#236}
at scala.sys.package$.error(package.scala:27)
{panel}


---

* [SPARK-7020](https://issues.apache.org/jira/browse/SPARK-7020) | *Critical* | **Restrict module testing based on commit contents**

Currently all builds trigger all tests. This does not need to happen and, to minimize the test window, the {{git}} commit contents should be checked to determine which modules were affected and, for each, only run those tests.


---

* [SPARK-7017](https://issues.apache.org/jira/browse/SPARK-7017) | *Major* | **Refactor dev/run-tests into Python**

This issue is to specifically track the progress of the {{dev/run-tests}} script into Python.


---

* [SPARK-6980](https://issues.apache.org/jira/browse/SPARK-6980) | *Minor* | **Akka timeout exceptions indicate which conf controls them**

If you hit one of the akka timeouts, you just get an exception like

{code}
java.util.concurrent.TimeoutException: Futures timed out after [30 seconds]
{code}

The exception doesn't indicate how to change the timeout, though there is usually (always?) a corresponding setting in {{SparkConf}} .  It would be nice if the exception including the relevant setting.

I think this should be pretty easy to do -- we just need to create something like a {{NamedTimeout}}.  It would have its own {{await}} method, catches the akka timeout and throws its own exception.  We should change {{RpcUtils.askTimeout}} and {{RpcUtils.lookupTimeout}} to always give a {{NamedTimeout}}, so we can be sure that anytime we have a timeout, we get a better exception.

Given the latest refactoring to the rpc layer, this needs to be done in both {{AkkaUtils}} and {{AkkaRpcEndpoint}}.


---

* [SPARK-6973](https://issues.apache.org/jira/browse/SPARK-6973) | *Minor* | **The total stages on the allJobsPage is wrong**

The job has two stages,  map and collect stage. Both two retried two times. The first and second time of map stage is successful, and the third time skipped. Of collect stage, the first and second time is failed, and the third time is successful.
On the allJobs page, the number of total stages is allStages-skippedStages. Mostly it's wright, but here I think total stages should be 2.

The example:
Stage 0: Map Stage
Stage 1: Collect Stage

Stage:     Stage 0 -\> Stage 1 -\> Stage 0(retry 1) -\> Stage 1(retry 1) -\> Stage 0(retry 2) -\> Stage 1(retry 2)
Status  Success -\>     Fail     -\>        Success       -\>             Fail      -\>                Skipped     -\>             Success

Though one of Stage 0 is skipped, actually it's executed. So I think it should be included in the total number.


---

* [SPARK-6964](https://issues.apache.org/jira/browse/SPARK-6964) | *Critical* | **Support Cancellation in the Thrift Server**

There is already a hook in {{ExecuteStatementOperation}}, we just need to connect it to the job group cancellation support we already have and make sure the various drivers support it.


---

* [SPARK-6941](https://issues.apache.org/jira/browse/SPARK-6941) | *Blocker* | **Provide a better error message to explain that tables created from RDDs are immutable**

We should explicitly let users know that tables created from RDDs are immutable and new rows cannot be inserted into it. We can add a better error message and also explain it in the programming guide.


---

* [SPARK-6912](https://issues.apache.org/jira/browse/SPARK-6912) | *Major* | **Throw an AnalysisException when unsupported Java Map\<K,V\> types used in Hive UDF**

The current implementation can't handle Map\<K,V\> as a return type in Hive UDF. 
We assume an UDF below;

public class UDFToIntIntMap extends UDF {
    public Map\<Integer, Integer\> evaluate(Object o);
}

Hive supports this type, and see a link below for details;

https://github.com/kyluka/hive/blob/master/ql/src/java/org/apache/hadoop/hive/ql/udf/generic/GenericUDFBridge.java#L163
https://github.com/l1x/apache-hive/blob/master/hive-0.8.1/src/serde/src/java/org/apache/hadoop/hive/serde2/objectinspector/ObjectInspectorFactory.java#L118


---

* [SPARK-6906](https://issues.apache.org/jira/browse/SPARK-6906) | *Blocker* | **Improve Hive integration support**

Right now Spark SQL is very coupled to a specific version of Hive for two primary reasons.
 - Metadata: we use the Hive Metastore client to retrieve information about tables in a metastore.
 - Execution: UDFs, UDAFs, SerDes, HiveConf and various helper functions for configuration.

Since Hive is generally not compatible across versions, we are currently maintain fairly expensive shim layers to let us talk to both Hive 12 and Hive 13 metastores.  Ideally we would be able to talk to more versions of Hive with less maintenance burden.

This task is proposing that we separate the hive version that is used for communicating with the metastore from the version that is used for execution.  In doing so we can significantly reduce the size of the shim by only providing compatibility for metadata operations.  All execution will be done with single version of Hive (the newest version that is supported by Spark SQL).


---

* [SPARK-6902](https://issues.apache.org/jira/browse/SPARK-6902) | *Major* | **Row() object can be mutated even though it should be immutable**

See the below code snippet, IMHO it shouldn't let you assign {{x.c = 5}} and should just give you an error.

{quote}
Welcome to
      \_\_\_\_              \_\_
     / \_\_/\_\_  \_\_\_ \_\_\_\_\_/ /\_\_
    \_\ \/ \_ \/ \_ `/ \_\_/  '\_/
   /\_\_ / .\_\_/\\_,\_/\_/ /\_/\\_\   version 1.2.0-SNAPSHOT
      /\_/

Using Python version 2.6.6 (r266:84292, Jan 22 2014 09:42:36)
SparkContext available as sc.
\>\>\> from pyspark.sql import \*
\>\>\> x = Row(a=1, b=2, c=3)
\>\>\> x
Row(a=1, b=2, c=3)
\>\>\> x.\_\_dict\_\_
\{'\_\_FIELDS\_\_': ['a', 'b', 'c']\}
\>\>\> x.c
3
\>\>\> x.c = 5
\>\>\> x
Row(a=1, b=2, c=3)
\>\>\> x.\_\_dict\_\_
\{'\_\_FIELDS\_\_': ['a', 'b', 'c'], 'c': 5\}
\>\>\> x.c
5
{quote}


---

* [SPARK-6885](https://issues.apache.org/jira/browse/SPARK-6885) | *Major* | **Decision trees: predict class probabilities**

Under spark.ml, have DecisionTreeClassifier (currently being added) extend ProbabilisticClassifier.


---

* [SPARK-6833](https://issues.apache.org/jira/browse/SPARK-6833) | *Minor* | **Extend `addPackage` so that any given R file can be sourced in the worker before functions are run.**

Similar to how extra python files or packages can be specified (in zip / egg formats), it will be good to support the ability to add extra R files to the executors working directory.

One thing that needs to be investigated is if this will just work out of the box using the spark-submit flag --files ?


---

* [SPARK-6820](https://issues.apache.org/jira/browse/SPARK-6820) | *Critical* | **Convert NAs to null type in SparkR DataFrames**

While converting RDD or local R DataFrame to a SparkR DataFrame we need to handle missing values or NAs.
We should convert NAs to SparkSQL's null type to handle the conversion correctly


---

* [SPARK-6813](https://issues.apache.org/jira/browse/SPARK-6813) | *Major* | **SparkR style guide**

We should develop a SparkR style guide document based on the some of the guidelines we use and some of the best practices in R.
Some examples of R style guide are:
http://r-pkgs.had.co.nz/r.html#style 
http://google-styleguide.googlecode.com/svn/trunk/google-r-style.html
A related issue is to work on a automatic style checking tool. https://github.com/jimhester/lintr seems promising

We could have a R style guide based on the one from google [1], and adjust some of them with the conversation in Spark:
1. Line Length: maximum 100 characters
2. no limit on function name (API should be similar as in other languages)
3. Allow S4 objects/methods


---

* [SPARK-6805](https://issues.apache.org/jira/browse/SPARK-6805) | *Critical* | **MLlib + SparkR integration for 1.5**

--SparkR was merged. So let's have this umbrella JIRA for the ML pipeline API in SparkR. The implementation should be similar to the pipeline API implementation in Python.--

We limited the scope of this JIRA to MLlib + SparkR integration for 1.5.

For Spark 1.5, we want to support linear/logistic regression in SparkR, with basic support for R formula and elastic-net regularization. The design doc can be viewed at https://docs.google.com/document/d/10NZNSEurN2EdWM31uFYsgayIPfCFHiuIu3pCWrUmP\_c/edit?usp=sharing


---

* [SPARK-6797](https://issues.apache.org/jira/browse/SPARK-6797) | *Critical* | **Add support for YARN cluster mode**

SparkR currently does not work in YARN cluster mode as the R package is not shipped along with the assembly jar to the YARN AM. We could try to use the support for archives in YARN to send out the R package as a zip file.


---

* [SPARK-6795](https://issues.apache.org/jira/browse/SPARK-6795) | *Critical* | **Avoid reading Parquet footers on driver side when an global arbitrative schema is available**

With the help of [Parquet MR PR #91\|https://github.com/apache/incubator-parquet-mr/pull/91] which will be included in the official release of Parquet MR 1.6.0, now it's possible to avoid reading footers on the driver side completely when an global arbitrative schema is available.

Currently, the global schema can be either Hive metastore schema or specified via data sources DDL. All tasks should verify Parquet data files and reconcile possible schema conflicts locally against this global schema.

However, when no global schema is available and schema merging is enabled, we still need to read schemas from all data files to infer a valid global schema.


---

* [SPARK-6793](https://issues.apache.org/jira/browse/SPARK-6793) | *Major* | **Implement perplexity for LDA**

LDA should be able to compute perplexity.  This JIRA is for computing it on the training dataset.  See the linked JIRA for computing it on a new corpus: [SPARK-5567]


---

* [SPARK-6785](https://issues.apache.org/jira/browse/SPARK-6785) | *Major* | **DateUtils can not handle date before 1970/01/01 correctly**

{code}
scala\> val d = new Date(100)
d: java.sql.Date = 1969-12-31

scala\> DateUtils.toJavaDate(DateUtils.fromJavaDate(d))
res1: java.sql.Date = 1970-01-01

{code}


---

* [SPARK-6782](https://issues.apache.org/jira/browse/SPARK-6782) | *Minor* | **add sbt-revolver plugin to sbt build**

[sbt-revolver\|https://github.com/spray/sbt-revolver] is a very useful sbt plugin for development.  You can start & stop long-running processes without being forced to kill the entire sbt session.  This can save a lot of time in the development cycle.

With sbt-revolver, you run {{re-start}} to start your app in a forked jvm.   It immediately gives you the sbt shell back, so you can continue to code.  When you want to reload your app with whatever changes you make, you just run {{re-start}} again -- it will kill the forked jvm, recompile your code, and start the process again.  (Or you can run {{re-stop}} at any time to kill the forked jvm.)

I used this a ton while working on adding json support to the UI in https://issues.apache.org/jira/browse/SPARK-3454 (as the history server never stops, without this plugin I had to kill sbt between every time I'd run it manually to play with the behavior.)  I don't write a lot of spark-streaming jobs, but I've also used this plugin in that case, since again my streaming jobs never terminate -- I imagine it would be really useful to anybody that is modifying streaming and wants to test out running some jobs.

I'll post a PR.


---

* [SPARK-6777](https://issues.apache.org/jira/browse/SPARK-6777) | *Critical* | **Implement backwards-compatibility rules in Parquet schema converters**

When converting Parquet schemas to/from  Spark SQL schemas, we should recognize commonly used legacy non-standard representation of complex types. We can follow the pattern used in Parquet's {{AvroSchemaConverter}}.


---

* [SPARK-6749](https://issues.apache.org/jira/browse/SPARK-6749) | *Critical* | **Make metastore client robust to underlying socket connection loss**

Right now, if metastore get restarted, we have to restart the driver to get a new connection to the metastore client because the underlying socket connection is gone. We should make metastore client robust to it.


---

* [SPARK-6747](https://issues.apache.org/jira/browse/SPARK-6747) | *Major* | **Throw an AnalysisException when unsupported Java list types used in Hive UDF**

The current implementation can't handle List\<\> as a return type in Hive UDF and
throws meaningless Match Error.
We assume an UDF below;

public class UDFToListString extends UDF {
    public List\<String\> evaluate(Object o) {
        return Arrays.asList("xxx", "yyy", "zzz");
    }
}

An exception of scala.MatchError is thrown as follows when the UDF used;

scala.MatchError: interface java.util.List (of class java.lang.Class)
	at org.apache.spark.sql.hive.HiveInspectors$class.javaClassToDataType(HiveInspectors.scala:174)
	at org.apache.spark.sql.hive.HiveSimpleUdf.javaClassToDataType(hiveUdfs.scala:76)
	at org.apache.spark.sql.hive.HiveSimpleUdf.dataType$lzycompute(hiveUdfs.scala:106)
	at org.apache.spark.sql.hive.HiveSimpleUdf.dataType(hiveUdfs.scala:106)
	at org.apache.spark.sql.catalyst.expressions.Alias.toAttribute(namedExpressions.scala:131)
	at org.apache.spark.sql.catalyst.planning.PhysicalOperation$$anonfun$collectAliases$1.applyOrElse(patterns.scala:95)
	at org.apache.spark.sql.catalyst.planning.PhysicalOperation$$anonfun$collectAliases$1.applyOrElse(patterns.scala:94)
	at scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:33)
	at scala.collection.TraversableLike$$anonfun$collect$1.apply(TraversableLike.scala:278)
...

To make udf developers more understood, we need to throw a more suitable exception.


---

* [SPARK-6707](https://issues.apache.org/jira/browse/SPARK-6707) | *Major* | **Mesos Scheduler should allow the user to specify constraints based on slave attributes**

Currently, the mesos scheduler only looks at the 'cpu' and 'mem' resources when trying to determine the usablility of a resource offer from a mesos slave node. It may be preferable for the user to be able to ensure that the spark jobs are only started on a certain set of nodes (based on attributes). 

For example, If the user sets a property, let's say {code}spark.mesos.constraints{code} is set to {code}tachyon=true;us-east-1=false{code}, then the resource offers will be checked to see if they meet both these constraints and only then will be accepted to start new executors.


---

* [SPARK-6684](https://issues.apache.org/jira/browse/SPARK-6684) | *Major* | **Add checkpointing to GradientBoostedTrees**

We should add checkpointing to GradientBoostedTrees since it maintains RDDs with long lineages.

keywords: gradient boosting, gbt, gradient boosted trees


---

* [SPARK-6591](https://issues.apache.org/jira/browse/SPARK-6591) | *Major* | **Python data source load options should auto convert common types into strings**

See the discussion at : https://github.com/databricks/spark-csv/pull/39

If the caller invokes
{code}
sqlContext.load("com.databricks.spark.csv", path = "cars.csv", header = True)
{code}

We should automatically turn header into "true" in string form.

We should do this for booleans and numeric values.

cc [~yhuai]


---

* [SPARK-6566](https://issues.apache.org/jira/browse/SPARK-6566) | *Major* | **Update Spark to use the latest version of Parquet libraries**

There are a lot of bug fixes in the latest version of parquet (1.6.0rc7). E.g. PARQUET-136

It would be good to update Spark to use the latest parquet version.

The following changes are required:
{code}
diff --git a/pom.xml b/pom.xml
index 5ad39a9..095b519 100644
--- a/pom.xml
+++ b/pom.xml
@@ -132,7 +132,7 @@
     \<!-- Version used for internal directory structure --\>
     \<hive.version.short\>0.13.1\</hive.version.short\>
     \<derby.version\>10.10.1.1\</derby.version\>
-    \<parquet.version\>1.6.0rc3\</parquet.version\>
+    \<parquet.version\>1.6.0rc7\</parquet.version\>
     \<jblas.version\>1.2.3\</jblas.version\>
     \<jetty.version\>8.1.14.v20131031\</jetty.version\>
     \<orbit.version\>3.0.0.v201112011016\</orbit.version\>
{code}
and
{code}
--- a/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetTableOperations.scala
+++ b/sql/core/src/main/scala/org/apache/spark/sql/parquet/ParquetTableOperations.scala
@@ -480,7 +480,7 @@ private[parquet] class FilteringParquetRowInputFormat
     globalMetaData = new GlobalMetaData(globalMetaData.getSchema,
       mergedMetadata, globalMetaData.getCreatedBy)
 
-    val readContext = getReadSupport(configuration).init(
+    val readContext = ParquetInputFormat.getReadSupportInstance(configuration).init(
       new InitContext(configuration,
         globalMetaData.getKeyValueMetaData,
         globalMetaData.getSchema))

{code}

I am happy to prepare a pull request if necessary.


---

* [SPARK-6489](https://issues.apache.org/jira/browse/SPARK-6489) | *Major* | **Optimize lateral view with explode to not read unnecessary columns**

Currently a query with "lateral view explode(...)" results in an execution plan that reads all columns of the underlying RDD.

E.g. given \*ppl\* table is DF created from Person case class:
{code}
case class Person(val name: String, val age: Int, val data: Array[Int])
{code}
the following SQL:
{code}
select name, sum(d) from ppl lateral view explode(data) d as d group by name
{code}
executes as follows:
{noformat}
== Physical Plan ==
Aggregate false, [name#0], [name#0,SUM(PartialSum#38L) AS \_c1#18L]
 Exchange (HashPartitioning [name#0], 200)
  Aggregate true, [name#0], [name#0,SUM(CAST(d#21, LongType)) AS PartialSum#38L]
   Project [name#0,d#21]
    Generate explode(data#2), true, false
     InMemoryColumnarTableScan [name#0,age#1,data#2], [], (InMemoryRelation [name#0,age#1,data#2], true, 10000, StorageLevel(true, true, false, true, 1), (PhysicalRDD [name#0,age#1,data#2], MapPartitionsRDD[1] at mapPartitions at ExistingRDD.scala:35), Some(ppl))
{noformat}

Note that \*age\* column is not needed to produce the output but it is still read from the underlying RDD.

A sample program to demonstrate the issue:
{code}
case class Person(val name: String, val age: Int, val data: Array[Int])

object ExplodeDemo extends App {
  val ppl = Array(
    Person("A", 20, Array(10, 12, 19)),
    Person("B", 25, Array(7, 8, 4)),
    Person("C", 19, Array(12, 4, 232)))
  

  val conf = new SparkConf().setMaster("local[2]").setAppName("sql")
  val sc = new SparkContext(conf)
  val sqlCtx = new HiveContext(sc)
  import sqlCtx.implicits.\_
  val df = sc.makeRDD(ppl).toDF
  df.registerTempTable("ppl")
  sqlCtx.cacheTable("ppl") // cache table otherwise ExistingRDD will be used that do not support column pruning
  val s = sqlCtx.sql("select name, sum(d) from ppl lateral view explode(data) d as d group by name")
  s.explain(true)
}
{code}


---

* [SPARK-6487](https://issues.apache.org/jira/browse/SPARK-6487) | *Critical* | **Add sequential pattern mining algorithm PrefixSpan to Spark MLlib**

[~mengxr] [~zhangyouhua]
Sequential pattern mining is an important branch in the pattern mining. In the past the actual work, we use the sequence mining (mainly PrefixSpan algorithm) to find the telecommunication signaling sequence pattern, achieved good results. But once the data is too large, the operation time is too long, even can not meet the the service requirements. We are ready to implement the PrefixSpan algorithm in spark, and applied to our subsequent work. 

The related Paper: 

PrefixSpan: 
Pei, Jian, et al. "Mining sequential patterns by pattern-growth: The prefixspan approach." Knowledge and Data Engineering, IEEE Transactions on 16.11 (2004): 1424-1440.

Parallel Algorithm: 
Cong, Shengnan, Jiawei Han, and David Padua. "Parallel mining of closed sequential patterns." Proceedings of the eleventh ACM SIGKDD international conference on Knowledge discovery in data mining. ACM, 2005.

Distributed Algorithm: 
Wei, Yong-qing, Dong Liu, and Lin-shan Duan. "Distributed PrefixSpan algorithm based on MapReduce." Information Technology in Medicine and Education (ITME), 2012 International Symposium on. Vol. 2. IEEE, 2012.

Pattern mining and sequential mining Knowledge: 
Han, Jiawei, et al. "Frequent pattern mining: current status and future directions." Data Mining and Knowledge Discovery 15.1 (2007): 55-86.


---

* [SPARK-6486](https://issues.apache.org/jira/browse/SPARK-6486) | *Major* | **Add BlockMatrix in PySpark**

We should add BlockMatrix to PySpark. Internally, we can use DataFrames and MatrixUDT for serialization. This JIRA should contain conversions between IndexedRowMatrix/CoordinateMatrix to block matrices. But this does NOT cover linear algebra operations of block matrices.


---

* [SPARK-6485](https://issues.apache.org/jira/browse/SPARK-6485) | *Major* | **Add CoordinateMatrix/RowMatrix/IndexedRowMatrix in PySpark**

We should add APIs for CoordinateMatrix/RowMatrix/IndexedRowMatrix in PySpark. Internally, we can use DataFrames for serialization.


---

* [SPARK-6444](https://issues.apache.org/jira/browse/SPARK-6444) | *Major* | **SQL functions (either built-in or UDF) should check for data types of their arguments**

SQL functions should remain unresolved if their arguments don't satisfy their argument type requirements. Take {{Sum}} as an example, the data type of {{Sum(Literal("1"))}} is {{StringType}}, and now it's considered resolved, which may cause problems.

Here is a simplified version of a problematic query reported by [~cenyuhai]. Spark shell session for reproducing this issue:
{code}
import sqlContext.\_

sql("""
    CREATE TABLE IF NOT EXISTS ut (
        c1 STRING,
        c2 STRING
    )
    """)

sql("""
    SELECT SUM(c3) FROM (
        SELECT SUM(c1) AS c3, 0 AS c4 FROM ut     -- (1)
        UNION ALL
        SELECT 0 AS c3, COUNT(c2) AS c4 FROM ut   -- (2)
    ) t
    """).queryExecution.optimizedPlan
{code}
Exception thrown:
{noformat}
java.util.NoSuchElementException: key not found: c3#10
        at scala.collection.MapLike$class.default(MapLike.scala:228)
        at org.apache.spark.sql.catalyst.expressions.AttributeMap.default(AttributeMap.scala:29)
        at scala.collection.MapLike$class.apply(MapLike.scala:141)
        at org.apache.spark.sql.catalyst.expressions.AttributeMap.apply(AttributeMap.scala:29)
        at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$1.applyOrElse(Optimizer.scala:80)
        at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$1.applyOrElse(Optimizer.scala:79)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:187)
        at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$3.apply(TreeNode.scala:187)
        at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:50)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:186)
        at org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:177)
        at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$.pushToRight(Optimizer.scala:79)
        at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$apply$1$$anonfun$applyOrElse$6.apply(Optimizer.scala:101)
        at org.apache.spark.sql.catalyst.optimizer.UnionPushdown$$anonfun$apply$1$$anonfun$applyOrElse$6.apply(Optimizer.scala:101)
        ...
{noformat}
The analyzed plan of the query is:
{noformat}
== Analyzed Logical Plan ==
!Aggregate [], [SUM(CAST(c3#153, DoubleType)) AS \_c0#157]                   (c)
 Union
  Project [CAST(c3#153, StringType) AS c3#164,c4#163L]                      (d)
   Project [c3#153,CAST(c4#154, LongType) AS c4#163L]
    Aggregate [], [SUM(CAST(c1#158, DoubleType)) AS c3#153,0 AS c4#154]     (b)
     MetastoreRelation default, ut, None
  Project [CAST(c3#155, StringType) AS c3#162,c4#156L]                      (a)
   Aggregate [], [0 AS c3#155,COUNT(c2#161) AS c4#156L]
    MetastoreRelation default, ut, None
{noformat}
This case is very interesting. It involves 2 analysis rules, {{WidenTypes}} and {{PromoteStrings}}, and 1 optimizer rule, {{UnionPushdown}}. To see the details, we can turn on TRACE level log and check detailed rule execution process. The TL;DR is:
# Since {{c1}} is STRING, {{SUM(c1)}} is also STRING (which is the root cause of the whole issue).
# {{c3}} in {{(1)}} is STRING, while the one in {{(2)}} is INT. Thus {{WidenTypes}} casts the latter to STRING to ensure both sides of the UNION have the same schema.  See {{(a)}}.
# {{PromoteStrings}} casts {{c1}} in {{SUM(c1)}} to DOUBLE, which consequently changes data type of {{SUM(c1)}} and {{c3}} to DOUBLE.  See {{(b)}}.
# {{c3}} in the top level {{Aggregate}} is resolved as DOUBLE (c)
# Since schemas of the two sides of the UNION are different again, {{WidenTypes}} casts {{SUM(c1) AS c3}} to STRING.  See {{(d)}}.
# Int the top level {{Aggregate}}, {{c3#153}} becomes a missing input attribute because it is hidden by {{(d)}} now.
# In the optimizing phase, {{UnionPushdown}} throws because the top level {{Aggregate}} has missing input attribute.


---

* [SPARK-6419](https://issues.apache.org/jira/browse/SPARK-6419) | *Major* | **GenerateOrdering does not support BinaryType and complex types.**

When user want to order by binary columns or columns with complex types and code gen is enabled, there will be a MatchError ([see here\|https://github.com/apache/spark/blob/v1.3.0/sql/catalyst/src/main/scala/org/apache/spark/sql/catalyst/expressions/codegen/GenerateOrdering.scala#L45]). We can either add supports for these types or have a function to check if we can safely call GenerateOrdering (like the canBeCodeGened for HashAggregation Strategy).


---

* [SPARK-6411](https://issues.apache.org/jira/browse/SPARK-6411) | *Major* | **PySpark DataFrames can't be created if any datetimes have timezones**

I am unable to create a DataFrame with PySpark if any of the {{datetime}} objects that pass through the conversion process have a {{tzinfo}} property set. 

This works fine:

{code}
In [9]: sc.parallelize([(datetime.datetime(2014, 7, 8, 11, 10),)]).toDF().collect()
Out[9]: [Row(\_1=datetime.datetime(2014, 7, 8, 11, 10))]
{code}

as expected, the tuple's schema is inferred as having one anonymous column with a datetime field, and the datetime roundtrips through to the Java side python deserialization and then back into python land upon {{collect}}. This however:

{code}
In [5]: from dateutil.tz import tzutc

In [10]: sc.parallelize([(datetime.datetime(2014, 7, 8, 11, 10, tzinfo=tzutc()),)]).toDF().collect()
{code}

explodes with

{code}
Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.
: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 12, localhost): net.razorvine.pickle.PickleException: invalid pickle data for datetime; expected 1 or 7 args, got 2
	at net.razorvine.pickle.objects.DateTimeConstructor.createDateTime(DateTimeConstructor.java:69)
	at net.razorvine.pickle.objects.DateTimeConstructor.construct(DateTimeConstructor.java:32)
	at net.razorvine.pickle.Unpickler.load\_reduce(Unpickler.java:617)
	at net.razorvine.pickle.Unpickler.dispatch(Unpickler.java:170)
	at net.razorvine.pickle.Unpickler.load(Unpickler.java:84)
	at net.razorvine.pickle.Unpickler.loads(Unpickler.java:97)
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:154)
	at org.apache.spark.api.python.SerDeUtil$$anonfun$pythonToJava$1$$anonfun$apply$1.apply(SerDeUtil.scala:153)
	at scala.collection.Iterator$$anon$13.hasNext(Iterator.scala:371)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:327)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.hasNext(SerDeUtil.scala:119)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.foreach(SerDeUtil.scala:114)
	at scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:48)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:103)
	at scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:47)
	at scala.collection.TraversableOnce$class.to(TraversableOnce.scala:273)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.to(SerDeUtil.scala:114)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toBuffer(SerDeUtil.scala:114)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at org.apache.spark.api.python.SerDeUtil$AutoBatchedPickler.toArray(SerDeUtil.scala:114)
	at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)
	at org.apache.spark.rdd.RDD$$anonfun$17.apply(RDD.scala:813)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1520)
	at org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:1520)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:64)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:203)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1211)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1200)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1199)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:47)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1199)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:693)
	at scala.Option.foreach(Option.scala:236)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:693)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1401)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1362)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)
{code}

By the looks of the error, it would appear as though the java depickler isn't expecting the pickle stream to provide that extra timezone constructor argument.

Here's the disassembled pickle stream for a timezone-less datetime:

{code}
\>\>\> object = datetime.datetime(2014, 7, 8, 11, 10)
\>\>\> stream = pickle.dumps(object)
\>\>\> pickletools.dis(stream)
    0: c    GLOBAL     'datetime datetime'
   19: p    PUT        0
   22: (    MARK
   23: S        STRING     '\x07\xde\x07\x08\x0b\n\x00\x00\x00\x00'
   65: p        PUT        1
   68: t        TUPLE      (MARK at 22)
   69: p    PUT        2
   72: R    REDUCE
   73: p    PUT        3
   76: .    STOP
highest protocol among opcodes = 0
{code}

and then for one with a timezone:

{code}
\>\>\> object = datetime.datetime(2014, 7, 8, 11, 10, tzinfo=tzutc())
\>\>\> stream = pickle.dumps(object)
\>\>\> pickletools.dis(stream)
    0: c    GLOBAL     'datetime datetime'
   19: p    PUT        0
   22: (    MARK
   23: S        STRING     '\x07\xde\x07\x08\x0b\n\x00\x00\x00\x00'
   65: p        PUT        1
   68: c        GLOBAL     'copy\_reg \_reconstructor'
   93: p        PUT        2
   96: (        MARK
   97: c            GLOBAL     'dateutil.tz tzutc'
  116: p            PUT        3
  119: c            GLOBAL     'datetime tzinfo'
  136: p            PUT        4
  139: g            GET        4
  142: (            MARK
  143: t                TUPLE      (MARK at 142)
  144: R            REDUCE
  145: p            PUT        5
  148: t            TUPLE      (MARK at 96)
  149: p        PUT        6
  152: R        REDUCE
  153: p        PUT        7
  156: t        TUPLE      (MARK at 22)
  157: p    PUT        8
  160: R    REDUCE
  161: p    PUT        9
  164: .    STOP
highest protocol among opcodes = 0
{code}

I would bet that the Pyrolite library is missing support for that nested object as a second tuple member in the reconstruction of the datetime object. Has anyone hit this before? Any more information I can provide?


---

* [SPARK-6390](https://issues.apache.org/jira/browse/SPARK-6390) | *Major* | **Add MatrixUDT in PySpark**

After SPARK-6309, we should support MatrixUDT in PySpark too.


---

* [SPARK-6360](https://issues.apache.org/jira/browse/SPARK-6360) | *Major* | **For Spark 1.1 and 1.2, after any RDD transformations, calling saveAsParquetFile over a SchemaRDD with decimal or UDT column throws**

Spark shell session for reproduction (use {{:paste}}):
{noformat}
import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.catalyst.types.decimal.\_
import org.apache.spark.sql.catalyst.types.\_
import org.apache.hadoop.fs.\_

val sqlContext = new SQLContext(sc)
val fs = FileSystem.get(sc.hadoopConfiguration)

fs.delete(new Path("a.parquet"))
fs.delete(new Path("b.parquet"))

import sc.\_
import sqlContext.\_

val r1 = parallelize(1 to 10).map(i =\> Tuple1(Decimal(i, 10, 0))).select('\_1 cast DecimalType(10, 0))

// OK
r1.saveAsParquetFile("a.parquet")

val r2 = parallelize(1 to 10).map(i =\> Tuple1(Decimal(i, 10, 0))).select('\_1 cast DecimalType(10, 0))

val r3 = r2.coalesce(1)

// Error
r3.saveAsParquetFile("b.parquet")
{noformat}
Exception thrown:
{noformat}
java.lang.ClassCastException: scala.math.BigDecimal cannot be cast to org.apache.spark.sql.catalyst.types.decimal.Decimal
        at org.apache.spark.sql.parquet.MutableRowWriteSupport.consumeType(ParquetTableSupport.scala:359)
        at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:328)
        at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:314)
        at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
        at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
        at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:308)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:325)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:325)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
15/03/17 00:04:13 WARN TaskSetManager: Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.ClassCastException: scala.math.BigDecimal cannot be cast to org.apache.spark.sql.catalyst.types.decimal.Decimal
        at org.apache.spark.sql.parquet.MutableRowWriteSupport.consumeType(ParquetTableSupport.scala:359)
        at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:328)
        at org.apache.spark.sql.parquet.MutableRowWriteSupport.write(ParquetTableSupport.scala:314)
        at parquet.hadoop.InternalParquetRecordWriter.write(InternalParquetRecordWriter.java:120)
        at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:81)
        at parquet.hadoop.ParquetRecordWriter.write(ParquetRecordWriter.java:37)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable.org$apache$spark$sql$parquet$InsertIntoParquetTable$$writeShard$1(ParquetTableOperations.scala:308)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:325)
        at org.apache.spark.sql.parquet.InsertIntoParquetTable$$anonfun$saveAsHadoopFile$1.apply(ParquetTableOperations.scala:325)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
        at java.lang.Thread.run(Thread.java:745)
{noformat}
The query plan of {{r1}} is:
{noformat}
== Parsed Logical Plan ==
'Project [CAST('\_1, DecimalType(10,0)) AS c0#60]
 LogicalRDD [\_1#59], MapPartitionsRDD[71] at mapPartitions at ExistingRDD.scala:36

== Analyzed Logical Plan ==
Project [CAST(\_1#59, DecimalType(10,0)) AS c0#60]
 LogicalRDD [\_1#59], MapPartitionsRDD[71] at mapPartitions at ExistingRDD.scala:36

== Optimized Logical Plan ==
Project [CAST(\_1#59, DecimalType(10,0)) AS c0#60]
 LogicalRDD [\_1#59], MapPartitionsRDD[71] at mapPartitions at ExistingRDD.scala:36

== Physical Plan ==
Project [CAST(\_1#59, DecimalType(10,0)) AS c0#60]
 PhysicalRDD [\_1#59], MapPartitionsRDD[71] at mapPartitions at ExistingRDD.scala:36

Code Generation: false
== RDD ==
{noformat}
while {{r3}}'s query plan is:
{noformat}
== Parsed Logical Plan ==
LogicalRDD [c0#61], CoalescedRDD[74] at coalesce at SchemaRDD.scala:456

== Analyzed Logical Plan ==
LogicalRDD [c0#61], CoalescedRDD[74] at coalesce at SchemaRDD.scala:456

== Optimized Logical Plan ==
LogicalRDD [c0#61], CoalescedRDD[74] at coalesce at SchemaRDD.scala:456

== Physical Plan ==
PhysicalRDD [c0#61], CoalescedRDD[74] at coalesce at SchemaRDD.scala:456

Code Generation: false
== RDD ==
{noformat}
The key difference here is that, {{r3}} wraps an existing {{SchemaRDD}} ({{r2}}, beneath the {{CoalescedRDD}}). While evaluating {{r3}}, {{r2.compute}} is called, which calls {{ScalaReflection.convertRowToScala}}. Here, Catalyst {{Decimal}} values are converted into Java {{BigDecimal}}s, and finally causes the exception.

Note that {{DataFrame}} in Spark 1.3 doesn't suffer this issue.


---

* [SPARK-6324](https://issues.apache.org/jira/browse/SPARK-6324) | *Minor* | **Clean up usage code in command-line scripts**

With SPARK-4924, most of the logic to launch Spark classes is in a new Java library. Pretty much the only thing left in scripts are the usage strings for each command; that uses some rather ugly and hacky code to handle, since it requires the library communicating back with the scripts that they should print a usage string instead of executing a command.

The scripts have to process that special command (differently on bash and Windows), and do filtering of the actual output of usage strings to account for different commands.

Instead, the library itself should handle all this by executing the classes with a "help" argument; and the classes should be able to handle that argument to do the right thing. So this would require both changes in the launcher library, and in all the main entry points to make sure they properly respond to the "help" by printing the correct help message.

This would make things a lot cleaner and a lot easier to maintain.


---

* [SPARK-6319](https://issues.apache.org/jira/browse/SPARK-6319) | *Critical* | **Should throw analysis exception when using binary type in groupby/join**

Spark shell session for reproduction:
{noformat}
scala\> import sqlContext.implicits.\_
scala\> import org.apache.spark.sql.types.\_
scala\> Seq(1, 1, 2, 2).map(i =\> Tuple1(i.toString)).toDF("c").select($"c" cast BinaryType).distinct.show()
...
CAST(c, BinaryType)
[B@43f13160
[B@5018b648
[B@3be22500
[B@476fc8a1
{noformat}
Spark SQL uses plain byte arrays to represent binary values. However, arrays are compared by reference rather than by value. On the other hand, the DISTINCT operator uses a {{HashSet}} and its {{.contains}} method to check for duplicated values. These two facts together cause the problem.


---

* [SPARK-6304](https://issues.apache.org/jira/browse/SPARK-6304) | *Major* | **Checkpointing doesn't retain driver port**

In a check-pointed Streaming application running on a fixed driver port, the setting "spark.driver.port" is not loaded when recovering from a checkpoint.

(The driver is then started on a random port.)


---

* [SPARK-6289](https://issues.apache.org/jira/browse/SPARK-6289) | *Major* | **PySpark doesn't maintain SQL date Types**

For the DateType, Spark SQL requires a datetime.date in Python. However, if you collect a row based on that type, you'll end up with a returned value which is type datetime.datetime.

I have tried to reproduce this using the pyspark shell, but have been unable to. This is definitely a problem coming from pyrolite though:

https://github.com/irmen/Pyrolite/

Pyrolite is being used for datetime and date serialization, but appears to not map to date objects, but maps to datetime objects.


---

* [SPARK-6287](https://issues.apache.org/jira/browse/SPARK-6287) | *Major* | **Add support for dynamic allocation in the Mesos coarse-grained scheduler**

Add support inside the coarse-grained Mesos scheduler for dynamic allocation. It amounts to implementing two methods that allow scaling up and down the number of executors:

{code}
def doKillExecutors(executorIds: Seq[String])
def doRequestTotalExecutors(requestedTotal: Int)
{code}


---

* [SPARK-6284](https://issues.apache.org/jira/browse/SPARK-6284) | *Major* | **Support framework authentication and role in Mesos framework**

Support framework authentication and role in both Coarse grain and fine grain mode.


---

* [SPARK-6266](https://issues.apache.org/jira/browse/SPARK-6266) | *Minor* | **PySpark SparseVector missing doc for size, indices, values**

Need to add doc for size, indices, values attributes


---

* [SPARK-6263](https://issues.apache.org/jira/browse/SPARK-6263) | *Major* | **Python MLlib API missing items: Utils**

This JIRA lists items missing in the Python API for this sub-package of MLlib.
This list may be incomplete, so please check again when sending a PR to add these features to the Python API.

Also, please check for major disparities between documentation; some parts of the Python API are less well-documented than their Scala counterparts.  Some items may be listed in the umbrella JIRA linked to this task.

MLUtils
\* appendBias
\* kFold
\* loadVectors


---

* [SPARK-6261](https://issues.apache.org/jira/browse/SPARK-6261) | *Major* | **Python MLlib API missing items: Feature**

This JIRA lists items missing in the Python API for this sub-package of MLlib.
This list may be incomplete, so please check again when sending a PR to add these features to the Python API.

Also, please check for major disparities between documentation; some parts of the Python API are less well-documented than their Scala counterparts.  Some items may be listed in the umbrella JIRA linked to this task.

StandardScalerModel
\* All functionality except predict() is missing.

IDFModel
\* idf

Word2Vec
\* setMinCount

Word2VecModel
\* getVectors


---

* [SPARK-6259](https://issues.apache.org/jira/browse/SPARK-6259) | *Major* | **Python API for LDA**

Add Python API for LDA.

This task may be blocked by ongoing work on LDA which may require API changes:
\* [SPARK-5563]
\* [SPARK-5556]


---

* [SPARK-6246](https://issues.apache.org/jira/browse/SPARK-6246) | *Minor* | **spark-ec2 can't handle clusters with \> 100 nodes**

This appears to be a new restriction, perhaps resulting from our upgrade of boto. Maybe it's a new restriction from EC2. Not sure yet.

We didn't have this issue around the Spark 1.1.0 time frame from what I can remember. I'll track down where the issue is and when it started.

Attempting to launch a cluster with 100 slaves yields the following:

{code}
Spark AMI: ami-35b1885c
Launching instances...
Launched 100 slaves in us-east-1c, regid = r-9c408776
Launched master in us-east-1c, regid = r-92408778
Waiting for AWS to propagate instance metadata...
Waiting for cluster to enter 'ssh-ready' state.ERROR:boto:400 Bad Request
ERROR:boto:\<?xml version="1.0" encoding="UTF-8"?\>
\<Response\>\<Errors\>\<Error\>\<Code\>InvalidRequest\</Code\>\<Message\>101 exceeds the maximum number of instance IDs that can be specificied (100). Please specify fewer than 100 instance IDs.\</Message\>\</Error\>\</Errors\>\<RequestID\>217fd6ff-9afa-4e91-86bc-ab16fcc442d8\</RequestID\>\</Response\>
Traceback (most recent call last):
  File "./ec2/spark\_ec2.py", line 1338, in \<module\>
    main()
  File "./ec2/spark\_ec2.py", line 1330, in main
    real\_main()
  File "./ec2/spark\_ec2.py", line 1170, in real\_main
    cluster\_state='ssh-ready'
  File "./ec2/spark\_ec2.py", line 795, in wait\_for\_cluster\_state
    statuses = conn.get\_all\_instance\_status(instance\_ids=[i.id for i in cluster\_instances])
  File "/path/apache/spark/ec2/lib/boto-2.34.0/boto/ec2/connection.py", line 737, in get\_all\_instance\_status
    InstanceStatusSet, verb='POST')
  File "/path/apache/spark/ec2/lib/boto-2.34.0/boto/connection.py", line 1204, in get\_object
    raise self.ResponseError(response.status, response.reason, body)
boto.exception.EC2ResponseError: EC2ResponseError: 400 Bad Request
\<?xml version="1.0" encoding="UTF-8"?\>
\<Response\>\<Errors\>\<Error\>\<Code\>InvalidRequest\</Code\>\<Message\>101 exceeds the maximum number of instance IDs that can be specificied (100). Please specify fewer than 100 instance IDs.\</Message\>\</Error\>\</Errors\>\<RequestID\>217fd6ff-9afa-4e91-86bc-ab16fcc442d8\</RequestID\>\</Response\>
{code}

This problem seems to be with {{get\_all\_instance\_status()}}, though I am not sure if other methods are affected too.


---

* [SPARK-6212](https://issues.apache.org/jira/browse/SPARK-6212) | *Major* | **The EXPLAIN output of CTAS only shows the analyzed plan**

When you try
{code}
sql("explain extended create table parquet2 as select \* from parquet1").collect.foreach(println)
{code}
The output will be 
{code}
[== Parsed Logical Plan ==]
['CreateTableAsSelect None, parquet2, false, Some(TOK\_CREATETABLE)]
[ 'Project [\*]]
[  'UnresolvedRelation [parquet1], None]
[]
[== Analyzed Logical Plan ==]
[CreateTableAsSelect [Database:default, TableName: parquet2, InsertIntoHiveTable]]
[Project [str#44]]
[ Subquery parquet1]
[  Relation[str#44] ParquetRelation2(List(file:/user/hive/warehouse/parquet1),Map(serialization.format -\> 1, path -\> file:/user/hive/warehouse/parquet1),Some(StructType(StructField(str,StringType,true))),None)]
[]
[]
[== Optimized Logical Plan ==]
[CreateTableAsSelect [Database:default, TableName: parquet2, InsertIntoHiveTable]]
[Project [str#44]]
[ Subquery parquet1]
[  Relation[str#44] ParquetRelation2(List(file:/user/hive/warehouse/parquet1),Map(serialization.format -\> 1, path -\> file:/user/hive/warehouse/parquet1),Some(StructType(StructField(str,StringType,true))),None)]
[]
[]
[== Physical Plan ==]
[ExecutedCommand (CreateTableAsSelect [Database:default, TableName: parquet2, InsertIntoHiveTable]]
[Project [str#44]]
[ Subquery parquet1]
[  Relation[str#44] ParquetRelation2(List(file:/user/hive/warehouse/parquet1),Map(serialization.format -\> 1, path -\> file:/user/hive/warehouse/parquet1),Some(StructType(StructField(str,StringType,true))),None)]
[)]
[]
[Code Generation: false]
[== RDD ==]
{code}

Query Plans of the SELECT clause shown in Optimized Plan and Physical Plan are actually analyzed plan.


---

* [SPARK-6196](https://issues.apache.org/jira/browse/SPARK-6196) | *Minor* | **Add MAPR 4.0.2 support to the build**

Mapr 4.0.2 upgraded to use hadoop 2.5.1 and the current mapr build doesn't support building for 4.0.2
http://doc.mapr.com/display/RelNotes/Version+4.0.2+Release+Notes


---

* [SPARK-6164](https://issues.apache.org/jira/browse/SPARK-6164) | *Minor* | **CrossValidatorModel should keep stats from fitting**

CrossValidator computes stats for each (model, fold) pair, but they are thrown out by the created model.  CrossValidatorModel should keep this info and expose it to users.


---

* [SPARK-6154](https://issues.apache.org/jira/browse/SPARK-6154) | *Major* | **Support Kafka, JDBC in Scala 2.11**

Build v1.3.0-rc2 with Scala 2.11 using instructions in the documentation failed when -Phive-thriftserver is enabled.

[info] Compiling 9 Scala sources to /home/hjs/workspace/spark/sql/hive-thriftserver/target/scala-2.11/classes...
[error] /home/hjs/workspace/spark/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala:2
5: object ConsoleReader is not a member of package jline
[error] import jline.{ConsoleReader, History}
[error]        ^
[warn] Class jline.Completor not found - continuing with a stub.
[warn] Class jline.ConsoleReader not found - continuing with a stub.
[error] /home/hjs/workspace/spark/sql/hive-thriftserver/src/main/scala/org/apache/spark/sql/hive/thriftserver/SparkSQLCLIDriver.scala:1
65: not found: type ConsoleReader
[error]     val reader = new ConsoleReader()


Jianshi


---

* [SPARK-6129](https://issues.apache.org/jira/browse/SPARK-6129) | *Major* | **Create MLlib metrics user guide with algorithm definitions and complete code examples.**

We now have evaluation metrics for binary, multiclass, ranking, and multilabel in MLlib. It would be nice to have a section in the user guide to summarize them.


---

* [SPARK-6126](https://issues.apache.org/jira/browse/SPARK-6126) | *Major* | **Support UDTs in JSON**

This would be nice to do (in Python):
{code}
from pyspark.mllib.util import MLUtils
data = MLUtils.loadLibSVMFile(sc, "data/mllib/sample\_libsvm\_data.txt")
data.toDF().toJSON().count()
{code}

But UDTs are not yet supported in JSON.  It would be nice to add.


---

* [SPARK-6123](https://issues.apache.org/jira/browse/SPARK-6123) | *Critical* | **Parquet reader should use the schema of every file to create converter**

For two parquet files for the same table having an array column, if values of the array in one file was created when containsNull was true and those in another file was created when containsNull was false, the containsNull in the merged schema will be true and we cannot correctly read data from the table created with containsNull=false.


---

* [SPARK-5989](https://issues.apache.org/jira/browse/SPARK-5989) | *Major* | **Model import/export for LDAModel**

Add save/load for LDAModel and its local and distributed variants.
LDA


---

* [SPARK-5962](https://issues.apache.org/jira/browse/SPARK-5962) | *Major* | **[MLLIB] Python support for Power Iteration Clustering**

Add python support for the Power Iteration Clustering feature.  Here is a fragment of the python API as we plan to implement it:

  /\*\*
   \* Java stub for Python mllib PowerIterationClustering.run()
   \*/
  def trainPowerIterationClusteringModel(
      data: JavaRDD[(java.lang.Long, java.lang.Long, java.lang.Double)],
      k: Int,
      maxIterations: Int,
      runs: Int,
      initializationMode: String,
      seed: java.lang.Long): PowerIterationClusteringModel = {
    val picAlg = new PowerIterationClustering()
      .setK(k)
      .setMaxIterations(maxIterations)

    try {
      picAlg.run(data.rdd.persist(StorageLevel.MEMORY\_AND\_DISK))
    } finally {
      data.rdd.unpersist(blocking = false)
    }
  }


---

* [SPARK-5895](https://issues.apache.org/jira/browse/SPARK-5895) | *Major* | **Add VectorSlicer**

`VectorSlicer` takes a vector column and output a vector column with a subset of features.

{code}
val vs = new VectorSlicer()
  .setInputCol("user")
  .setSelectedFeatures("age", "salary")
  .setOutputCol("usefulUserFeatures")
{code}

We should allow specifying selected features by indices and by names. It should preserve the output names.


---

* [SPARK-5768](https://issues.apache.org/jira/browse/SPARK-5768) | *Trivial* | **Spark UI Shows incorrect memory under Yarn**

I am running Spark on Yarn with 2 executors.  The executors are running on separate physical machines.

I have spark.executor.memory set to '40g'.  This is because I want to have 40g of memory used on each machine.  I have one executor per machine.

When I run my application I see from 'top' that both my executors are using the full 40g of memory I allocated to them.

The 'Executors' tab in the Spark UI shows something different.  It shows the memory used as a total of 20GB per executor e.g. x / 20.3GB.  This makes it look like I only have 20GB available per executor when really I have 40GB available.


---

* [SPARK-5707](https://issues.apache.org/jira/browse/SPARK-5707) | *Blocker* | **Enabling spark.sql.codegen throws ClassNotFound exception**

Exception thrown:
{noformat}
org.apache.spark.SparkException: Job aborted due to stage failure: Task 13 in stage 133.0 failed 4 times, most recent failure: Lost task 13.3 in stage 133.0 (TID 3066, cdh52-node2): java.io.IOException: com.esotericsoftware.kryo.KryoException: Unable to find class: \_\_wrapper$1$81257352e1c844aebf09cb84fe9e7459.\_\_wrapper$1$81257352e1c844aebf09cb84fe9e7459$SpecificRow$1
Serialization trace:
hashTable (org.apache.spark.sql.execution.joins.UniqueKeyHashedRelation)
        at org.apache.spark.util.Utils$.tryOrIOException(Utils.scala:1011)
        at org.apache.spark.broadcast.TorrentBroadcast.readBroadcastBlock(TorrentBroadcast.scala:164)
        at org.apache.spark.broadcast.TorrentBroadcast.\_value$lzycompute(TorrentBroadcast.scala:64)
        at org.apache.spark.broadcast.TorrentBroadcast.\_value(TorrentBroadcast.scala:64)
        at org.apache.spark.broadcast.TorrentBroadcast.getValue(TorrentBroadcast.scala:87)
        at org.apache.spark.broadcast.Broadcast.value(Broadcast.scala:70)
        at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$3.apply(BroadcastHashJoin.scala:62)
        at org.apache.spark.sql.execution.joins.BroadcastHashJoin$$anonfun$3.apply(BroadcastHashJoin.scala:61)
        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:601)
        at org.apache.spark.rdd.RDD$$anonfun$13.apply(RDD.scala:601)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.CartesianRDD.compute(CartesianRDD.scala:75)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.CartesianRDD.compute(CartesianRDD.scala:75)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
        at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:263)
        at org.apache.spark.rdd.RDD.iterator(RDD.scala:230)
        at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
        at org.apache.spark.scheduler.Task.run(Task.scala:56)
        at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:196)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{noformat}
SQL:
{code:sql}
INSERT INTO TABLE ${hiveconf:TEMP\_TABLE}
SELECT
  s\_store\_name,
  pr\_review\_date,
  pr\_review\_content
FROM (
  --select store\_name for stores with flat or declining sales in 3 consecutive months.
  SELECT s\_store\_name
  FROM store s
  JOIN (
    -- linear regression part
    SELECT
      temp.cat AS cat,
      --SUM(temp.x)as sumX,
      --SUM(temp.y)as sumY,
      --SUM(temp.xy)as sumXY,
      --SUM(temp.xx)as sumXSquared,
      --count(temp.x) as N,
      --N \* sumXY - sumX \* sumY AS numerator,
      --N \* sumXSquared - sumX\*sumX AS denom
      --numerator / denom as slope,
      --(sumY - slope \* sumX) / N as intercept
      --(count(temp.x) \* SUM(temp.xy) - SUM(temp.x) \* SUM(temp.y)) AS numerator,
      --(count(temp.x) \* SUM(temp.xx) - SUM(temp.x) \* SUM(temp.x)) AS denom
      --numerator / denom as slope,
      --(sumY - slope \* sumX) / N as intercept
      ((count(temp.x) \* SUM(temp.xy) - SUM(temp.x) \* SUM(temp.y)) / (count(temp.x) \* SUM(temp.xx) - SUM(temp.x) \* SUM(temp.x)) ) as slope,
      (SUM(temp.y) - ((count(temp.x) \* SUM(temp.xy) - SUM(temp.x) \* SUM(temp.y)) / (count(temp.x) \* SUM(temp.xx) - SUM(temp.x) \* SUM(temp.x)) ) \* SUM(temp.x)) / count(temp.x) as intercept
    FROM (
SELECT
        s.ss\_store\_sk AS cat,
        s.ss\_sold\_date\_sk  AS x,
        SUM(s.ss\_net\_paid) AS y,
        s.ss\_sold\_date\_sk \* SUM(s.ss\_net\_paid) AS xy,
        s.ss\_sold\_date\_sk\*s.ss\_sold\_date\_sk AS xx
      FROM store\_sales s
      --select date range
      LEFT SEMI JOIN (
        SELECT d\_date\_sk
        FROM date\_dim d
        WHERE d.d\_date \>= '${hiveconf:q18\_startDate}'
        AND   d.d\_date \<= '${hiveconf:q18\_endDate}'
      ) dd ON ( s.ss\_sold\_date\_sk=dd.d\_date\_sk )
      WHERE s.ss\_store\_sk \<= 18
      GROUP BY s.ss\_store\_sk, s.ss\_sold\_date\_sk
    ) temp
    GROUP BY temp.cat
  ) c on s.s\_store\_sk = c.cat
  WHERE c.slope \< 0
) tmp
JOIN  product\_reviews pr on (true)
WHERE instr(pr.pr\_review\_content, tmp.s\_store\_name) \> 0
{code}


---

* [SPARK-5681](https://issues.apache.org/jira/browse/SPARK-5681) | *Major* | **Calling graceful stop() immediately after start() on StreamingContext should not get stuck indefinitely**

Sometimes the receiver will be registered into tracker after ssc.stop is called. Especially when stop() is called immediately after start(). So the receiver doesn't get the StopReceiver message from the tracker. In this case, when you call stop() in graceful mode, stop() would get stuck indefinitely.


---

* [SPARK-5567](https://issues.apache.org/jira/browse/SPARK-5567) | *Major* | **Add prediction methods to LDA**

LDA currently supports prediction on the training set.  E.g., you can call logLikelihood and topicDistributions to get that info for the training data.  However, it should support the same functionality for new (test) documents.

This will require inference but should be able to use the same code, with a few modification to keep the inferred topics fixed.

Note: The API for these methods is already in the code but is commented out.


---

* [SPARK-5562](https://issues.apache.org/jira/browse/SPARK-5562) | *Minor* | **LDA should handle empty documents**

Latent Dirichlet Allocation (LDA) could easily be given empty documents when people select a small vocabulary.  We should check to make sure it is robust to empty documents.

This will hopefully take the form of a unit test, but may require modifying the LDA implementation.


---

* [SPARK-5561](https://issues.apache.org/jira/browse/SPARK-5561) | *Major* | **Generalize PeriodicGraphCheckpointer for RDDs**

PeriodicGraphCheckpointer was introduced for Latent Dirichlet Allocation (LDA), but it could be generalized to work with both Graphs and RDDs.  It should be generalized and moved out of MLlib.

(For those who are not familiar with it, it tries to automatically handle persisting/unpersisting and checkpointing/removing checkpoint files in a lineage of Graphs.)

A generalized version might be immediately useful for:
\* RandomForest
\* Streaming
\* GLMs


---

* [SPARK-5523](https://issues.apache.org/jira/browse/SPARK-5523) | *Major* | **TaskMetrics and TaskInfo have innumerable copies of the hostname string**

 TaskMetrics and TaskInfo objects have the hostname associated with the task. As these are created (directly or through deserialization of RPC messages), each of them have a separate String object for the hostname even though most of them have the same string data in them. This results in thousands of string objects, increasing memory requirement of the driver. 
This can be easily deduped when deserializing a TaskMetrics object, or when creating a TaskInfo object.

This affects streaming particularly bad due to the rate of job/stage/task generation. 

For solution, see how this dedup is done for StorageLevel. 
https://github.com/apache/spark/blob/master/core/src/main/scala/org/apache/spark/storage/StorageLevel.scala#L226


---

* [SPARK-5482](https://issues.apache.org/jira/browse/SPARK-5482) | *Minor* | **Allow individual test suites in python/run-tests**

Add options to run individual test suites in python/run-tests. The usage is as follow.

./python/run-tests \[core\|sql\|mllib\|ml\|streaming\]

When you select none, all test suites are run for backward compatibility.


---

* [SPARK-5479](https://issues.apache.org/jira/browse/SPARK-5479) | *Major* | **PySpark on yarn mode need to support non-local python files**

 In SPARK-5162 [~vgrigor] reports this:
Now following code cannot work:
aws emr add-steps --cluster-id "j-XYWIXMD234" \
--steps Name=SparkPi,Jar=s3://eu-west-1.elasticmapreduce/libs/script-runner/script-runner.jar,Args=[/home/hadoop/spark/bin/spark-submit,--deploy-mode,cluster,--master,yarn-cluster,--py-files,s3://mybucketat.amazonaws.com/tasks/main.py,main.py,param1],ActionOnFailure=CONTINUE

so we need to support non-local python files on yarn client and cluster mode.
before submitting application to Yarn, we need to download non-local files to local or hdfs path.
or spark.yarn.dist.files need to support other non-local files.


---

* [SPARK-5451](https://issues.apache.org/jira/browse/SPARK-5451) | *Critical* | **And predicates are not properly pushed down**

This issue is actually caused by PARQUET-173.

The following {{spark-shell}} session can be used to reproduce this bug:
{code}
import org.apache.spark.sql.SQLContext

val sqlContext = new SQLContext(sc)
import sc.\_
import sqlContext.\_

case class KeyValue(key: Int, value: String)

parallelize(1 to 1024 \* 1024 \* 20).
  flatMap(i =\> Seq.fill(10)(KeyValue(i, i.toString))).
  saveAsParquetFile("large.parquet")

parquetFile("large.parquet").registerTempTable("large")

hadoopConfiguration.set("parquet.task.side.metadata", "false")
sql("SET spark.sql.parquet.filterPushdown=true")

sql("SELECT value FROM large WHERE 1024 \< value AND value \< 2048").collect()
{code}
From the log we can find:
{code}
There were no row groups that could be dropped due to filter predicates
{code}


---

* [SPARK-5423](https://issues.apache.org/jira/browse/SPARK-5423) | *Major* | **ExternalAppendOnlyMap won't delete temp spilled file if some exception happens during using it**

ExternalAppendOnlyMap won't delete temp spilled file if some exception happens during using it.

There is already a TODO in the comment:
{code}
    // TODO: Ensure this gets called even if the iterator isn't drained.
    private def cleanup() {
      batchIndex = batchOffsets.length  // Prevent reading any other batch
      val ds = deserializeStream
      deserializeStream = null
      fileStream = null
      ds.close()
      file.delete()
    }
{code}


---

* [SPARK-5295](https://issues.apache.org/jira/browse/SPARK-5295) | *Major* | **Stabilize data types**

1. We expose all the stuff in data types right now, including NumericTypes, etc. These should be hidden from users. We should only expose the leaf types.

2. Remove DeveloperAPI tag from the common types.

3. Specify the internal type, external scala type, and external java type for each data type.

4. Add conversion functions between internal type, external scala type, and external java type into each type.


---

* [SPARK-5288](https://issues.apache.org/jira/browse/SPARK-5288) | *Major* | **Stabilize Spark SQL data type API followup**

Several issues we need to address before release 1.3

\* Do we want to make all classes in org.apache.spark.sql.types.dataTypes.scala public? Seems we do not need to make those abstract classes public.

\* Seems NativeType is not a very clear and useful concept. Should we just remove it?

\* We need to Stabilize the type hierarchy of our data types. Seems StringType and Decimal Type should not be primitive types.


---

* [SPARK-5161](https://issues.apache.org/jira/browse/SPARK-5161) | *Major* | **Parallelize Python test execution**

[Original discussion here.\|https://github.com/apache/spark/pull/3564#issuecomment-67785676]

As of 1.2.0, Python tests take around 10-12 minutes to run. Once [SPARK-3431] is complete, this will become a significant fraction of the total test time.

There are 2 separate approaches to explore for parallelizing the execution of Python unit tests:
\* Use GNU parallel to run each Python test file in parallel.
\* Use [{{nose}}\|http://nose.readthedocs.org/en/latest/doc\_tests/test\_multiprocess/multiprocess.html] to parallelize all Python tests in a more extensible/configurable way.


---

* [SPARK-5155](https://issues.apache.org/jira/browse/SPARK-5155) | *Major* | **Python API for MQTT streaming**

Python API for MQTT Utils


---

* [SPARK-5133](https://issues.apache.org/jira/browse/SPARK-5133) | *Major* | **Feature Importance for Random Forests**

Add feature importance to random forest models.
If people are interested in this feature I could implement it given a mentor (API decisions, etc). Please find a description of the feature below:

Decision trees intrinsically perform feature selection by selecting appropriate split points. This information can be used to assess the relative importance of a feature. 
Relative feature importance gives valuable insight into a decision tree or tree ensemble and can even be used for feature selection.

More information on feature importance (via decrease in impurity) can be found in ESLII (10.13.1) or here [1].
R's randomForest package uses a different technique for assessing variable importance that is based on permutation tests.

All necessary information to create relative importance scores should be available in the tree representation (class Node; split, impurity gain, (weighted) nr of samples?).

[1] http://scikit-learn.org/stable/modules/ensemble.html#feature-importance-evaluation


---

* [SPARK-5090](https://issues.apache.org/jira/browse/SPARK-5090) | *Major* | **The improvement of python converter for hbase**

The python converter `HBaseResultToStringConverter` provided in the HBaseConverter.scala returns only the value of first column in the result. It limits the utility of this converter, because it returns only one value per row(perhaps there are several version in hbase) and moreover it loses the other information of record, such as column:cell, timestamp. 

Here we would like to propose an improvement about python converter which returns all the records in the results (in a single string) with more complete information. We would like also make some improvements for hbase\_inputformat.py


---

* [SPARK-5016](https://issues.apache.org/jira/browse/SPARK-5016) | *Major* | **GaussianMixtureEM should distribute matrix inverse for large numFeatures, k**

If numFeatures or k are large, GMM EM should distribute the matrix inverse computation for Gaussian initialization.


---

* [SPARK-4988](https://issues.apache.org/jira/browse/SPARK-4988) | *Major* | **"Create table ..as select ..from..order by .. limit 10" report error when one col is a Decimal**

A table 'test' with a decimal type col.
create table test1 as select \* from test order by a limit 10;

org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 2.0 failed 1 times, most recent failure: Lost task 0.0 in stage 2.0 (TID 2, localhost): java.lang.ClassCastException: scala.math.BigDecimal cannot be cast to org.apache.spark.sql.catalyst.types.decimal.Decimal
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$wrapperFor$2.apply(HiveInspectors.scala:339)
	at org.apache.spark.sql.hive.HiveInspectors$$anonfun$wrapperFor$2.apply(HiveInspectors.scala:339)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1$1.apply(InsertIntoHiveTable.scala:111)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1$1.apply(InsertIntoHiveTable.scala:108)
	at scala.collection.Iterator$class.foreach(Iterator.scala:727)
	at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable.org$apache$spark$sql$hive$execution$InsertIntoHiveTable$$writeToFile$1(InsertIntoHiveTable.scala:108)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:87)
	at org.apache.spark.sql.hive.execution.InsertIntoHiveTable$$anonfun$saveAsHiveFile$3.apply(InsertIntoHiveTable.scala:87)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:61)
	at org.apache.spark.scheduler.Task.run(Task.scala:56)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:195)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)


---

* [SPARK-4867](https://issues.apache.org/jira/browse/SPARK-4867) | *Blocker* | **UDF clean up**

Right now our support and internal implementation of many functions has a few issues.  Specifically:
 - UDFS don't know their input types and thus don't do type coercion.
 - We hard code a bunch of built in functions into the parser.  This is bad because in SQL it creates new reserved words for things that aren't actually keywords.  Also it means that for each function we need to add support to both SQLContext and HiveContext separately.

For this JIRA I propose we do the following:
 - Change the interfaces for registerFunction and ScalaUdf to include types for the input arguments as well as the output type.
 - Add a rule to analysis that does type coercion for UDFs.
 - Add a parse rule for functions to SQLParser.
 - Rewrite all the UDFs that are currently hacked into the various parsers using this new functionality.

Depending on how big this refactoring becomes we could split parts 1&2 from part 3 above.


---

* [SPARK-4752](https://issues.apache.org/jira/browse/SPARK-4752) | *Major* | **Classifier based on artificial neural network**

Implement classifier based on artificial neural network (ANN). Requirements:
1) Use the existing artificial neural network implementation https://issues.apache.org/jira/browse/SPARK-2352, https://github.com/apache/spark/pull/1290
2) Extend MLlib ClassificationModel trait, 
3) Like other classifiers in MLlib, accept RDD[LabeledPoint] for training,
4) Be able to return the ANN model


---

* [SPARK-4751](https://issues.apache.org/jira/browse/SPARK-4751) | *Critical* | **Support dynamic allocation for standalone mode**

This is equivalent to SPARK-3822 but for standalone mode.

This is actually a very tricky issue because the scheduling mechanism in the standalone Master uses different semantics. In standalone mode we allocate resources based on cores. By default, an application will grab all the cores in the cluster unless "spark.cores.max" is specified. Unfortunately, this means an application could get executors of different sizes (in terms of cores) if:

1) App 1 kills an executor
2) App 2, with "spark.cores.max" set, grabs a subset of cores on a worker
3) App 1 requests an executor

In this case, the new executor that App 1 gets back will be smaller than the rest and can execute fewer tasks in parallel. Further, standalone mode is subject to the constraint that only one executor can be allocated on each worker per application. As a result, it is rather meaningless to request new executors if the existing ones are already spread out across all nodes.


---

* [SPARK-4598](https://issues.apache.org/jira/browse/SPARK-4598) | *Major* | **Paginate stage page to avoid OOM with \> 100,000 tasks**

In HistoryServer stage page, clicking the task href in Description, it occurs the GC error. The detail error message is:
2014-11-17 16:36:30,851 \| WARN  \| [qtp1083955615-352] \| Error for /history/application\_1416206401491\_0010/stages/stage/ \| org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:590)
java.lang.OutOfMemoryError: GC overhead limit exceeded
2014-11-17 16:36:30,851 \| WARN  \| [qtp1083955615-364] \| handle failed \| org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:697)
java.lang.OutOfMemoryError: GC overhead limit exceeded


---

* [SPARK-4561](https://issues.apache.org/jira/browse/SPARK-4561) | *Major* | **PySparkSQL's Row.asDict() should convert nested rows to dictionaries**

In PySpark, you can call {{.asDict
()}} on a SparkSQL {{Row}} to convert it to a dictionary.  Unfortunately, though, this does not convert nested rows to dictionaries.  For example:

{code}
\>\>\> sqlContext.sql("select results from results").first()
Row(results=[Row(time=3.762), Row(time=3.47), Row(time=3.559), Row(time=3.458), Row(time=3.229), Row(time=3.21), Row(time=3.166), Row(time=3.276), Row(time=3.239), Row(time=3.149)])
\>\>\> sqlContext.sql("select results from results").first().asDict()
{u'results': [(3.762,),
  (3.47,),
  (3.559,),
  (3.458,),
  (3.229,),
  (3.21,),
  (3.166,),
  (3.276,),
  (3.239,),
  (3.149,)]}
{code}


Actually, it looks like the nested fields are just left as Rows (IPython's fancy display logic obscured this in my first example):

{code}
\>\>\> Row(results=[Row(time=1), Row(time=2)]).asDict()
{'results': [Row(time=1), Row(time=2)]}
{code}

Here's the output I'd expect:

{code}
\>\>\> Row(results=[Row(time=1), Row(time=2)])
{'results' : [{'time': 1}, {'time': 2}]}
{code}

I ran into this issue when trying to use Pandas dataframes to display nested data that I queried from Spark SQL.


---

* [SPARK-4485](https://issues.apache.org/jira/browse/SPARK-4485) | *Critical* | **Add broadcast outer join to  optimize left outer join and right outer join**

For now, spark use broadcast join instead of hash join to optimize {{inner join}} when the size of one side data did not reach the {{AUTO\_BROADCASTJOIN\_THRESHOLD}}

However,Spark SQL will perform shuffle operations on each child relations while executing 
{{left outer join}} and {{right outer join}}.   {outer join}} is more suitable for optimiztion with broadcast join. 
We are planning to create a {{BroadcastHashouterJoin}} to implement the broadcast join for {{left outer join}} and {{right outer join}}


---

* [SPARK-4367](https://issues.apache.org/jira/browse/SPARK-4367) | *Major* | **Partial aggregation support the DISTINCT aggregation**

Most of aggregate function(e.g average) with "distinct" value will requires all of the records in the same group to be shuffled into a single node, however, as part of the optimization, those records can be partially aggregated before shuffling, that probably reduces the overhead of shuffling significantly.


---

* [SPARK-4362](https://issues.apache.org/jira/browse/SPARK-4362) | *Minor* | **Make prediction probability available in NaiveBayesModel**

There is currently no way to get the posterior probability of a prediction with Naive Baye's model during prediction. This should be made available along with the label.


---

* [SPARK-4352](https://issues.apache.org/jira/browse/SPARK-4352) | *Critical* | **Incorporate locality preferences in dynamic allocation requests**

Currently, achieving data locality in Spark is difficult unless an application takes resources on every node in the cluster.  preferredNodeLocalityData provides a sort of hacky workaround that has been broken since 1.0.

With dynamic executor allocation, Spark requests executors in response to demand from the application.  When this occurs, it would be useful to look at the pending tasks and communicate their location preferences to the cluster resource manager.


---

* [SPARK-4302](https://issues.apache.org/jira/browse/SPARK-4302) | *Minor* | **Support fixed precision decimal type in JsonParser**

Seems fixed precision decimal type is not supported in the JsonParser (org.apache.spark.sql.json.JacksonParser).


---

* [SPARK-4258](https://issues.apache.org/jira/browse/SPARK-4258) | *Critical* | **NPE with new Parquet Filters**

{code}
Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 4 times, most recent failure: Lost task 0.3 in stage 21.0 (TID 160, ip-10-0-247-144.us-west-2.compute.internal): java.lang.NullPointerException: 
        parquet.io.api.Binary$ByteArrayBackedBinary.compareTo(Binary.java:206)
        parquet.io.api.Binary$ByteArrayBackedBinary.compareTo(Binary.java:162)
        parquet.filter2.statisticslevel.StatisticsFilter.visit(StatisticsFilter.java:100)
        parquet.filter2.statisticslevel.StatisticsFilter.visit(StatisticsFilter.java:47)
        parquet.filter2.predicate.Operators$Eq.accept(Operators.java:162)
        parquet.filter2.statisticslevel.StatisticsFilter.visit(StatisticsFilter.java:210)
        parquet.filter2.statisticslevel.StatisticsFilter.visit(StatisticsFilter.java:47)
        parquet.filter2.predicate.Operators$Or.accept(Operators.java:302)
        parquet.filter2.statisticslevel.StatisticsFilter.visit(StatisticsFilter.java:201)
        parquet.filter2.statisticslevel.StatisticsFilter.visit(StatisticsFilter.java:47)
        parquet.filter2.predicate.Operators$And.accept(Operators.java:290)
        parquet.filter2.statisticslevel.StatisticsFilter.canDrop(StatisticsFilter.java:52)
        parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:46)
        parquet.filter2.compat.RowGroupFilter.visit(RowGroupFilter.java:22)
        parquet.filter2.compat.FilterCompat$FilterPredicateCompat.accept(FilterCompat.java:108)
        parquet.filter2.compat.RowGroupFilter.filterRowGroups(RowGroupFilter.java:28)
        parquet.hadoop.ParquetRecordReader.initializeInternalReader(ParquetRecordReader.java:158)
        parquet.hadoop.ParquetRecordReader.initialize(ParquetRecordReader.java:138)
{code}

This occurs when reading parquet data encoded with the older version of the library for TPC-DS query 34.  Will work on coming up with a smaller reproduction


---

* [SPARK-4234](https://issues.apache.org/jira/browse/SPARK-4234) | *Major* | **Always do paritial aggregation**

Currently, UDAF developer optionally implement a partial aggregation function, However this probably cause performance issue by allowing do that. We actually can always force developers to provide the partial aggregation function as Hive does, hence we will always get the `mapside` aggregation optimization.


---

* [SPARK-4233](https://issues.apache.org/jira/browse/SPARK-4233) | *Major* | **Simplify the Aggregation Function implementation**

Currently, the UDAF implementation is quite complicated, and we have to provide distinct & non-distinct version.


---

* [SPARK-4176](https://issues.apache.org/jira/browse/SPARK-4176) | *Major* | **Support decimals with precision \> 18 in Parquet**

After https://issues.apache.org/jira/browse/SPARK-3929, only decimals with precisions \<= 18 (that can be read into a Long) will be readable from Parquet, so we still need more work to support these larger ones.


---

* [SPARK-4127](https://issues.apache.org/jira/browse/SPARK-4127) | *Major* | **Streaming Linear Regression- Python bindings**

Create python bindings for Streaming Linear Regression (MLlib).
The Mllib file relevant to this issue can be found at : https://github.com/apache/spark/blob/master/examples/src/main/scala/org/apache/spark/examples/mllib/StreamingLinearRegression.scala


---

* [SPARK-4118](https://issues.apache.org/jira/browse/SPARK-4118) | *Major* | **Create python bindings for Streaming KMeans**

Create Python bindings for Streaming K-means
This is in reference to https://issues.apache.org/jira/browse/SPARK-3254
which adds Streaming K-means functionality to MLLib.


---

* [SPARK-4072](https://issues.apache.org/jira/browse/SPARK-4072) | *Critical* | **Storage UI does not reflect memory usage by streaming blocks**

The storage page in the web ui does not show the memory usage of non-RDD, non-Broadcast blocks. In other words, the memory used by data received through Spark Streaming is not shown on the web ui.


---

* [SPARK-3947](https://issues.apache.org/jira/browse/SPARK-3947) | *Major* | **Support Scala/Java UDAF**

Right now only Hive UDAFs are supported. It would be nice to have UDAF similar to UDF through SQLContext.registerFunction.


---

* [SPARK-3865](https://issues.apache.org/jira/browse/SPARK-3865) | *Major* | **Dimension table broadcast shouldn't be eager**

We eagerly broadcast dimension tables in BroadcastJoin. This is bad because even explain would trigger a job to execute the broadcast.


---

* [SPARK-3850](https://issues.apache.org/jira/browse/SPARK-3850) | *Minor* | **Scala style: disallow trailing spaces**

Background discussions:
\* https://github.com/apache/spark/pull/2619
\* http://apache-spark-developers-list.1001551.n3.nabble.com/Extending-Scala-style-checks-td8624.html

If you look at [the PR Cheng opened\|https://github.com/apache/spark/pull/2619], you'll see a trailing white space seemed to mess up some SQL test. That's what spurred the creation of this issue.

[Ted Yu on the dev list\|http://mail-archives.apache.org/mod\_mbox/spark-dev/201410.mbox/%3CCALte62y7a6WyBDUFDcGUwbf8WCpttViE+PAo4pZOR+\_-nB2UTw@mail.gmail.com%3E] suggested using this [{{WhitespaceEndOfLineChecker}}\|http://www.scalastyle.org/rules-0.1.0.html].


---

* [SPARK-3629](https://issues.apache.org/jira/browse/SPARK-3629) | *Minor* | **Improvements to YARN doc**

Right now this doc starts off with a big list of config options, and only then tells you how to submit an app. It would be better to put that part and the packaging part first, and the config options only at the end.

In addition, the doc mentions yarn-cluster vs yarn-client as separate masters, which is inconsistent with the help output from spark-submit (which says to always use "yarn").


---

* [SPARK-3382](https://issues.apache.org/jira/browse/SPARK-3382) | *Minor* | **GradientDescent convergence tolerance**

GradientDescent should support a convergence tolerance setting.  In general, for optimization, convergence tolerance should be preferred over a limit on the number of iterations since it is a somewhat data-adaptive or data-specific convergence criterion.


---

* [SPARK-3258](https://issues.apache.org/jira/browse/SPARK-3258) | *Major* | **Python API for streaming MLlib algorithms**

This is an umbrella JIRA to track Python port of streaming MLlib algorithms.


---

* [SPARK-3190](https://issues.apache.org/jira/browse/SPARK-3190) | *Critical* | **Creation of large graph(\> 2.15 B nodes) seems to be broken:possible overflow somewhere**

While creating a graph with 6B nodes and 12B edges, I noticed that 'numVertices' api returns incorrect result; 'numEdges' reports correct number. For few times(with different dataset \> 2.5B nodes) I have also notices that numVertices is returned as -ive number; so I suspect that there is some overflow (may be we are using Int for some field?).

Here is some details of experiments  I have done so far: 
1. Input: numNodes=6101995593 ; noEdges=12163784626
   Graph returns: numVertices=1807028297 ;  numEdges=12163784626

2. Input : numNodes=2157586441 ; noEdges=2747322705
   Graph Returns: numVertices=-2137380855 ;  numEdges=2747322705

3. Input: numNodes=1725060105 ; noEdges=204176821
   Graph: numVertices=1725060105 ;  numEdges=2041768213

You can find the code to generate this bug here: 

https://gist.github.com/npanj/92e949d86d08715bf4bf

Note: Nodes are labeled are 1...6B .


---

* [SPARK-3071](https://issues.apache.org/jira/browse/SPARK-3071) | *Major* | **Increase default driver memory**

The current default is 512M, which is usually too small because user also uses driver to do some computation. In local mode, executor memory setting is ignored while only driver memory is used, which provides more incentive to increase the default driver memory.

I suggest

1. 2GB in local mode and warn users if executor memory is set a bigger value
2. same as worker memory on an EC2 standalone server


---

* [SPARK-3056](https://issues.apache.org/jira/browse/SPARK-3056) | *Major* | **Sort-based Aggregation**

Currently, SparkSQL only support the hash-based aggregation, which may cause OOM if too many identical keys in the input tuples.


---

* [SPARK-2774](https://issues.apache.org/jira/browse/SPARK-2774) | *Major* | **Set preferred locations for reduce tasks**

Currently we do not set preferred locations for reduce tasks in Spark. This patch proposes setting preferred locations based on the map output sizes and locations tracked by the MapOutputTracker. This is useful in two conditions

1. When you have a small job in a large cluster it can be useful to co-locate map and reduce tasks to avoid going over the network
2. If there is a lot of data skew in the map stage outputs, then it is beneficial to place the reducer close to the largest output.


---

* [SPARK-2645](https://issues.apache.org/jira/browse/SPARK-2645) | *Major* | **Spark driver calls System.exit(50) after calling SparkContext.stop() the second time**

In some cases my application calls SparkContext.stop() after it has already stopped and this leads to stopping JVM that runs spark driver.
E.g
This program should run forever
{code}
JavaSparkContext context = new JavaSparkContext("spark://12.34.21.44:7077", "DummyApp");
        try {
            JavaRDD\<Integer\> rdd = context.parallelize(Arrays.asList(1, 2, 3));
            rdd.count();
        } catch (Throwable e) {
            e.printStackTrace();
        }
        try {
            context.cancelAllJobs();
            context.stop();
            //call stop second time
            context.stop();
        } catch (Throwable e) {
            e.printStackTrace();
        }
        Thread.currentThread().join();
{code}
but it finishes with exit code 50 after calling SparkContext.stop() the second time.
Also it throws an exception like this
{code}
org.apache.spark.ServerStateException: Server is already stopped
	at org.apache.spark.HttpServer.stop(HttpServer.scala:122) ~[spark-core\_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.HttpFileServer.stop(HttpFileServer.scala:48) ~[spark-core\_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:81) ~[spark-core\_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.SparkContext.stop(SparkContext.scala:984) ~[spark-core\_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.scheduler.cluster.SparkDeploySchedulerBackend.dead(SparkDeploySchedulerBackend.scala:92) ~[spark-core\_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.deploy.client.AppClient$ClientActor.markDead(AppClient.scala:178) ~[spark-core\_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.deploy.client.AppClient$ClientActor$$anonfun$registerWithMaster$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(AppClient.scala:96) ~[spark-core\_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.util.Utils$.tryOrExit(Utils.scala:790) ~[spark-core\_2.10-1.0.0.jar:1.0.0]
	at org.apache.spark.deploy.client.AppClient$ClientActor$$anonfun$registerWithMaster$1.apply$mcV$sp(AppClient.scala:91) [spark-core\_2.10-1.0.0.jar:1.0.0]
	at akka.actor.Scheduler$$anon$9.run(Scheduler.scala:80) [akka-actor\_2.10-2.2.3-shaded-protobuf.jar:na]
	at akka.actor.LightArrayRevolverScheduler$$anon$3$$anon$2.run(Scheduler.scala:241) [akka-actor\_2.10-2.2.3-shaded-protobuf.jar:na]
	at akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:42) [akka-actor\_2.10-2.2.3-shaded-protobuf.jar:na]
	at akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:386) [akka-actor\_2.10-2.2.3-shaded-protobuf.jar:na]
	at scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260) [scala-library-2.10.4.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339) [scala-library-2.10.4.jar:na]
	at scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979) [scala-library-2.10.4.jar:na]
	at scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107) [scala-library-2.10.4.jar:na]
{code}
One remark is that this behavior is only reproducible when I call SparkContext.cancellAllJobs() before calling SparkContext.stop()


---

* [SPARK-2205](https://issues.apache.org/jira/browse/SPARK-2205) | *Critical* | **Unnecessary exchange operators in a join on multiple tables with the same join key.**

{code}
hql("select \* from src x join src y on (x.key=y.key) join src z on (y.key=z.key)")

SchemaRDD[1] at RDD at SchemaRDD.scala:100
== Query Plan ==
Project [key#4:0,value#5:1,key#6:2,value#7:3,key#8:4,value#9:5]
 HashJoin [key#6], [key#8], BuildRight
  Exchange (HashPartitioning [key#6], 200)
   HashJoin [key#4], [key#6], BuildRight
    Exchange (HashPartitioning [key#4], 200)
     HiveTableScan [key#4,value#5], (MetastoreRelation default, src, Some(x)), None
    Exchange (HashPartitioning [key#6], 200)
     HiveTableScan [key#6,value#7], (MetastoreRelation default, src, Some(y)), None
  Exchange (HashPartitioning [key#8], 200)
   HiveTableScan [key#8,value#9], (MetastoreRelation default, src, Some(z)), None
{code}

However, this is fine...
{code}
hql("select \* from src x join src y on (x.key=y.key) join src z on (x.key=z.key)")

res5: org.apache.spark.sql.SchemaRDD = 
SchemaRDD[5] at RDD at SchemaRDD.scala:100
== Query Plan ==
Project [key#26:0,value#27:1,key#28:2,value#29:3,key#30:4,value#31:5]
 HashJoin [key#26], [key#30], BuildRight
  HashJoin [key#26], [key#28], BuildRight
   Exchange (HashPartitioning [key#26], 200)
    HiveTableScan [key#26,value#27], (MetastoreRelation default, src, Some(x)), None
   Exchange (HashPartitioning [key#28], 200)
    HiveTableScan [key#28,value#29], (MetastoreRelation default, src, Some(y)), None
  Exchange (HashPartitioning [key#30], 200)
   HiveTableScan [key#30,value#31], (MetastoreRelation default, src, Some(z)), None
{code}


---

* [SPARK-2017](https://issues.apache.org/jira/browse/SPARK-2017) | *Major* | **web ui stage page becomes unresponsive when the number of tasks is large**

{code}
sc.parallelize(1 to 1000000, 1000000).count()
{code}

The above code creates one million tasks to be executed. The stage detail web ui page takes forever to load (if it ever completes).

There are again a few different alternatives:

0. Limit the number of tasks we show.
1. Pagination
2. By default only show the aggregate metrics and failed tasks, and hide the successful ones.


---

* [SPARK-2016](https://issues.apache.org/jira/browse/SPARK-2016) | *Major* | **rdd in-memory storage UI becomes unresponsive when the number of RDD partitions is large**

Try run
{code}
sc.parallelize(1 to 100, 1000000).cache().count()
{code}

And open the storage UI for this RDD. It takes forever to load the page.

When the number of partitions is very large, I think there are a few alternatives:

0. Only show the top 1000.
1. Pagination
2. Instead of grouping by RDD blocks, group by executors


---

* [SPARK-1855](https://issues.apache.org/jira/browse/SPARK-1855) | *Major* | **Provide memory-and-local-disk RDD checkpointing**

Checkpointing is used to cut long lineage while maintaining fault tolerance. The current implementation is HDFS-based. Using the BlockRDD we can create in-memory-and-local-disk (with replication) checkpoints that are not as reliable as HDFS-based solution but faster.

It can help applications that require many iterations.


---

* [SPARK-746](https://issues.apache.org/jira/browse/SPARK-746) | *Major* | **Automatically Use Avro Serialization for Avro Objects**

All generated objects extend org.apache.avro.specific.SpecificRecordBase (or there may be a higher up class as well).

Since Avro records aren't JavaSerializable by default people currently have to wrap their records. It would be good if we could use an implicit conversion to do this for them.



