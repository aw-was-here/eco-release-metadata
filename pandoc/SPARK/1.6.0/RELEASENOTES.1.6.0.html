<!---
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
-->
<h1 id="apache-spark-1.6.0-release-notes">Apache Spark 1.6.0 Release Notes</h1>
<p>These release notes cover new developer and user-facing incompatibilities, important issues, features, and major improvements.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-10716">SPARK-10716</a> | <em>Minor</em> | <strong>spark-1.5.0-bin-hadoop2.6.tgz file doesn't uncompress on OS X due to hidden file</strong></li>
</ul>
<p>Directly downloaded prebuilt binaries of http://d3kbcqa49mib13.cloudfront.net/spark-1.5.0-bin-hadoop2.6.tgz got error when tar xvzf it. Tried download twice and extract twice.</p>
<p>error log: ...... x spark-1.5.0-bin-hadoop2.6/lib/ x spark-1.5.0-bin-hadoop2.6/lib/datanucleus-core-3.2.10.jar x spark-1.5.0-bin-hadoop2.6/lib/datanucleus-api-jdo-3.2.6.jar x spark-1.5.0-bin-hadoop2.6/lib/datanucleus-rdbms-3.2.9.jar x spark-1.5.0-bin-hadoop2.6/lib/spark-examples-1.5.0-hadoop2.6.0.jar x spark-1.5.0-bin-hadoop2.6/lib/spark-assembly-1.5.0-hadoop2.6.0.jar x spark-1.5.0-bin-hadoop2.6/lib/spark-1.5.0-yarn-shuffle.jar x spark-1.5.0-bin-hadoop2.6/README.md tar: copyfile unpack (spark-1.5.0-bin-hadoop2.6/python/test_support/sql/orc_partitioned/SUCCESS.crc) failed: No such file or directory ~ :&gt;</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-10856">SPARK-10856</a> | <em>Major</em> | <strong>SQL Server dialect needs to map java.sql.Timestamp to DATETIME instead of TIMESTAMP</strong></li>
</ul>
<p>When saving a DataFrame to MS SQL Server, en error is thrown if there is more than one TIMESTAMP column:</p>
<p>df.printSchema</p>
<p>root |-- Id: string (nullable = false) |-- TypeInformation_CreatedBy: string (nullable = false) |-- TypeInformation_ModifiedBy: string (nullable = true) |-- TypeInformation_TypeStatus: integer (nullable = false) |-- TypeInformation_CreatedAtDatabase: timestamp (nullable = false) |-- TypeInformation_ModifiedAtDatabase: timestamp (nullable = true)</p>
<p>df.write.mode(&quot;overwrite&quot;).jdbc(url, tablename, props)</p>
<p>com.microsoft.sqlserver.jdbc.SQLServerException: A table can only have one timestamp column. Because table 'DebtorTypeSet1' already has one, the column 'TypeInformation_ModifiedAtDatabase' cannot be added. at com.microsoft.sqlserver.jdbc.SQLServerException.makeFromDatabaseError (SQLServerException.java:217) at com.microsoft.sqlserver.jdbc.SQLServerStatement.getNextResult(SQLServ erStatement.java:1635) at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.doExecutePrep aredStatement(SQLServerPreparedStatement.java:426) at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement$PrepStmtExecC md.doExecute(SQLServerPreparedStatement.java:372) at com.microsoft.sqlserver.jdbc.TDSCommand.execute(IOBuffer.java:6276) at com.microsoft.sqlserver.jdbc.SQLServerConnection.executeCommand(SQLSe rverConnection.java:1793) at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeCommand(SQLSer verStatement.java:184) at com.microsoft.sqlserver.jdbc.SQLServerStatement.executeStatement(SQLS erverStatement.java:159) at com.microsoft.sqlserver.jdbc.SQLServerPreparedStatement.executeUpdate (SQLServerPreparedStatement.java:315)</p>
<p>I tested this on Windows and SQL Server 12 using Spark 1.4.1.</p>
<p>I think this can be fixed in a similar way to Spark-10419.</p>
<p>As a refererence, here is the type mapping according to the SQL Server JDBC driver (basicDT.java, extracted from sqljdbc_4.2.6420.100_enu.exe):</p>
<p>private static void displayRow(String title, ResultSet rs) { try { System.out.println(title); System.out.println(rs.getInt(1) + &quot; , &quot; + // SQL integer type. rs.getString(2) + &quot; , &quot; + // SQL char type. rs.getString(3) + &quot; , &quot; + // SQL varchar type. rs.getBoolean(4) + &quot; , &quot; + // SQL bit type. rs.getDouble(5) + &quot; , &quot; + // SQL decimal type. rs.getDouble(6) + &quot; , &quot; + // SQL money type. rs.getTimestamp(7) + &quot; , &quot; + // SQL datetime type. rs.getDate(8) + &quot; , &quot; + // SQL date type. rs.getTime(9) + &quot; , &quot; + // SQL time type. rs.getTimestamp(10) + &quot; , &quot; + // SQL datetime2 type. ((SQLServerResultSet)rs).getDateTimeOffset(11)); // SQL datetimeoffset type.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-11023">SPARK-11023</a> | <em>Major</em> | <strong>Error initializing SparkContext. java.net.URISyntaxException</strong></li>
</ul>
<p>Simliar to SPARK-10326. [https://issues.apache.org/jira/browse/SPARK-10326?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&amp;focusedCommentId=14949470#comment-14949470]</p>
<p>C:32&gt;pyspark --master yarn-client Python 2.7.10 |Anaconda 2.3.0 (64-bit)| (default, Sep 15 2015, 14:26:14) [MSC v.1500 64 bit (AMD64)] Type &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information. IPython 4.0.0 â€“ An enhanced Interactive Python. ? -&gt; Introduction and overview of IPython's features. %quickref -&gt; Quick reference. help -&gt; Python's own help system. object? -&gt; Details about 'object', use 'object??' for extra details. 15/10/08 09:28:05 WARN MetricsSystem: Using default name DAGScheduler for source because spark.app.id is not set. 15/10/08 09:28:06 WARN : Your hostname, PC-509512 resolves to a loopback/non-reachable address: fe80:0:0:0:0:5efe:a5f:c318%net3, but we couldn't find any external IP address! 15/10/08 09:28:08 WARN BlockReaderLocal: The short-circuit local reads feature cannot be used because UNIX Domain sockets are not available on Windows. 15/10/08 09:28:08 ERROR SparkContext: Error initializing SparkContext. java.net.URISyntaxException: Illegal character in opaque part at index 2: C:...zip at java.net.URI$Parser.fail(Unknown Source) at java.net.URI$Parser.checkChars(Unknown Source) at java.net.URI$Parser.parse(Unknown Source) at java.net.URI.&lt;init&gt;(Unknown Source) at org.apache.spark.deploy.yarn.Client$$anonfun$setupLaunchEnv$7.apply(Client.scala:558) at org.apache.spark.deploy.yarn.Client$$anonfun$setupLaunchEnv$7.apply(Client.scala:557) at scala.collection.immutable.List.foreach(List.scala:318) at org.apache.spark.deploy.yarn.Client.setupLaunchEnv(Client.scala:557) at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:628) at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:119) at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56) at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:523) at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:61) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source) at java.lang.reflect.Constructor.newInstance(Unknown Source) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379) at py4j.Gateway.invoke(Gateway.java:214) at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79) at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68) at py4j.GatewayConnection.run(GatewayConnection.java:207) at java.lang.Thread.run(Unknown Source) 15/10/08 09:28:08 ERROR Utils: Uncaught exception in thread Thread-2 java.lang.NullPointerException at org.apache.spark.network.netty.NettyBlockTransferService.close(NettyBlockTransferService.scala:152) at org.apache.spark.storage.BlockManager.stop(BlockManager.scala:1228) at org.apache.spark.SparkEnv.stop(SparkEnv.scala:100) at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1749) at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1185) at org.apache.spark.SparkContext.stop(SparkContext.scala:1748) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:593) at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:61) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source) at java.lang.reflect.Constructor.newInstance(Unknown Source) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379) at py4j.Gateway.invoke(Gateway.java:214) at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79) at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68) at py4j.GatewayConnection.run(GatewayConnection.java:207) at java.lang.Thread.run(Unknown Source) --------------------------------------------------------------------------- Py4JJavaError Traceback (most recent call last) C:...py in &lt;module&gt;() 41 SparkContext.setSystemProperty(&quot;spark.executor.uri&quot;, os.environ[&quot;SPARK_EXECUTOR_URI&quot;]) 42 ---&gt; 43 sc = SparkContext(pyFiles=add_files) 44 atexit.register(lambda: sc.stop()) 45 C:.pyc in _init_(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls) 111 try: 112 self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer, --&gt; 113 conf, jsc, profiler_cls) 114 except: 115 # If an error occurs, clean up in order to allow future SparkContext creation: C:.pyc in _do_init(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls) 168 169 # Create the Java SparkContext through Py4J --&gt; 170 self._jsc = jsc or self._initialize_context(self._conf._jconf) 171 172 # Create a single Accumulator in Java that we'll send all our updates through; C:.pyc in _initialize_context(self, jconf) 222 Initialize SparkContext in function to allow subclass specific initialization 223 &quot;&quot;&quot; --&gt; 224 return self._jvm.JavaSparkContext(jconf) 225 226 @classmethod C:4j-0.8.2.1-src.zip4j_gateway.py in _call_(self, *args) 699 answer = self._gateway_client.send_command(command) 700 return_value = get_return_value(answer, self._gateway_client, None, --&gt; 701 self._fqn) 702 703 for temp_arg in temp_args: C:4j-0.8.2.1-src.zip4j.py in get_return_value(answer, gateway_client, target_id, name) 298 raise Py4JJavaError( 299 'An error occurred while calling {0} {1} {2} .'. --&gt; 300 format(target_id, '.', name), value) 301 else: 302 raise Py4JError( Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext. : java.net.URISyntaxException: Illegal character in opaque part at index 2: C:...zip at java.net.URI$Parser.fail(Unknown Source) at java.net.URI$Parser.checkChars(Unknown Source) at java.net.URI$Parser.parse(Unknown Source) at java.net.URI.&lt;init&gt;(Unknown Source) at org.apache.spark.deploy.yarn.Client$$anonfun$setupLaunchEnv$7.apply(Client.scala:558) at org.apache.spark.deploy.yarn.Client$$anonfun$setupLaunchEnv$7.apply(Client.scala:557) at scala.collection.immutable.List.foreach(List.scala:318) at org.apache.spark.deploy.yarn.Client.setupLaunchEnv(Client.scala:557) at org.apache.spark.deploy.yarn.Client.createContainerLaunchContext(Client.scala:628) at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:119) at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:56) at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:144) at org.apache.spark.SparkContext.&lt;init&gt;(SparkContext.scala:523) at org.apache.spark.api.java.JavaSparkContext.&lt;init&gt;(JavaSparkContext.scala:61) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source) at java.lang.reflect.Constructor.newInstance(Unknown Source) at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:234) at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:379) at py4j.Gateway.invoke(Gateway.java:214) at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:79) at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:68) at py4j.GatewayConnection.run(GatewayConnection.java:207) at java.lang.Thread.run(Unknown Source) In [1]: Reply</p>
<p>Marcelo Vanzin added a comment - 10 hours ago Ah, that's similar but not the same bug; it's a different part of the code that only affects pyspark. Could you file a separate bug for that? This is the</p>
<p>{code} (pySparkArchives ++ pyArchives).foreach { path =&gt; val uri = new URI(path) {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-11481">SPARK-11481</a> | <em>Major</em> | <strong>orderBy with multiple columns in WindowSpec does not work properly</strong></li>
</ul>
<p>When using multiple columns in the orderBy of a WindowSpec the order by seems to work only for the first column.</p>
<p>A possible workaround is to sort previosly the DataFrame and then apply the window spec over the sorted DataFrame</p>
<p>e.g. THIS NOT WORKS: window_sum = Window.partitionBy('user_unique_id').orderBy('creation_date', 'mib_id', 'day').rowsBetween(-sys.maxsize, 0)</p>
<p>df = df.withColumn('user_version', func.sum(df.group_counter).over(window_sum))</p>
<p>THIS WORKS WELL: df = df.sort('user_unique_id', 'creation_date', 'mib_id', 'day') window_sum = Window.partitionBy('user_unique_id').orderBy('creation_date', 'mib_id', 'day').rowsBetween(-sys.maxsize, 0)</p>
<p>df = df.withColumn('user_version', func.sum(df.group_counter).over(window_sum))</p>
<p>Also, can anybody confirm that this is a true workaround?</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/SPARK-11700">SPARK-11700</a> | <em>Critical</em> | <strong>Memory leak at SparkContext jobProgressListener stageIdToData map</strong></li>
</ul>
<p>it seems that there is A SparkContext jobProgressListener memory leak.*. Bellow i describe the steps i do to reproduce that.</p>
<p>I have created a java webapp trying to abstractly Run some Spark Sql jobs that read data from HDFS (join them) and Write them To ElasticSearch using ES hadoop connector. After a Lot of consecutive runs i noticed that my heap space was full so i got an out of heap space error.</p>
<p>At the attached file {code} AbstractSparkJobRunner {code} the {code} public final void run(T jobConfiguration, ExecutionLog executionLog) throws Exception {code} runs each time an Spark Sql Job is triggered. So tried to reuse the same SparkContext for a number of consecutive runs. If some rules apply i try to clean up the SparkContext by first calling {code} killSparkAndSqlContext {code}. This code eventually runs {code} synchronized (sparkContextThreadLock) { if (javaSparkContext != null) { LOGGER.info(&quot;!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!! CLEARING SPARK CONTEXT!!!!!!!!!!!!!!!!!!!!!!!!!!!&quot;); javaSparkContext.stop(); javaSparkContext = null; sqlContext = null;</p>
<pre><code>            System.gc();
        }
        numberOfRunningJobsForSparkContext.getAndSet(0);
    }</code></pre>
<p>{code}.</p>
<p>So at some point in time i suppose that if no other SparkSql job should run i should kill the sparkContext (The AbstractSparkJobRunner.killSparkAndSqlContext runs) and this should be garbage collected from garbage collector. However this is not the case, Even if in my debugger shows that my JavaSparkContext object is null see attached picture {code} SparkContextPossibleMemoryLeakIDEA_DEBUG.png {code}.</p>
<p>The jvisual vm shows an incremental heap space even when the garbage collector is called. See attached picture {code} SparkHeapSpaceProgress.png {code}.</p>
<p>The memory analyser Tool shows that a big part of the retained heap to be assigned to _jobProgressListener see attached picture {code} SparkMemoryAfterLotsOfConsecutiveRuns.png {code} and summary picture {code} SparkMemoryLeakAfterLotsOfRunsWithinTheSameContext.png {code}. Although at the same time in Singleton Service the JavaSparkContext is null.</p>
