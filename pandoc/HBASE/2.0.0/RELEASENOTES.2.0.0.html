<!---
# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
-->
<h1 id="apache-hbase-2.0.0-release-notes">Apache HBase 2.0.0 Release Notes</h1>
<p>These release notes cover new developer and user-facing incompatibilities, important issues, features, and major improvements.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-9049">HBASE-9049</a> | <em>Major</em> | <strong>Generalize ServerCallable creation to support custom callables</strong></li>
</ul>
<p>Support custom RpcRetryingCaller via a configurable factory.</p>
<hr />
<ul>
<li><p><a href="https://issues.apache.org/jira/browse/HBASE-9745">HBASE-9745</a> | <em>Major</em> | <strong>Append HBASE_CLASSPATH to end of Java classpath and use another env var for prefix</strong></p></li>
<li>The additional JAR files specified with HBASE_CLASSPATH are appended to the system JARs.</li>
<li><p>The additional JAR files specified with HBASE_CLASSPATH_PREFIX are prefixed to the system JARs.</p></li>
</ul>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11059">HBASE-11059</a> | <em>Major</em> | <strong>ZK-less region assignment</strong></li>
</ul>
<p>This patch makes it possible to do region assignment without ZK. By default, it is off (i.e. ZK is used for region assignment as before).</p>
<p>Two setting &quot;hbase.assignment.usezk&quot;, &quot;hbase.assignment.usezk.migrating&quot; are introduced to control migration from using ZK for assignment to not using ZK for assignment.</p>
<p>For rolling upgrade from using ZK to not using ZK, it can be done in two steps:</p>
<ol>
<li><p>Set both hbase.assignment.usezk and hbase.assignment.usezk.migrating to true, do a rolling upgrade so that both masters and regionservers have the new code. Either master or regionserver can be upgraded at first. The order is not important for this step. If you want to keep using ZK for assignment, you'd better set hbase.assignment.usezk to true, and hbase.assignment.usezk.migrating to false so that region states are not persisted in meta table.</p></li>
<li><p>Set hbase.assignment.usezk to false, do a rolling restart so that region assignments don't use ZK any more. For this step, masters should be restarted after regionservers have all restarted at first so that they won't update meta table directly and master doesn't know about it.</p></li>
</ol>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11372">HBASE-11372</a> | <em>Major</em> | <strong>Remove SlabCache</strong></li>
</ul>
<p>SlabCache is no longer support. For off-heap blockcache needs, please see BucketCache.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11344">HBASE-11344</a> | <em>Major</em> | <strong>Hide row keys and such from the web UIs</strong></li>
</ul>
<p>Configure &quot;hbase.display.keys&quot; to false (default: true) in the master/regionservers if the row-keys should be hidden in the webUIs (like in the webUI for table details).</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-10885">HBASE-10885</a> | <em>Blocker</em> | <strong>Support visibility expressions on Deletes</strong></li>
</ul>
<p>Deletes can be specified with Cell Visibility as done for puts. Cells covered by the delete is found by doing pattern matching. A deleteFamily issued for row1, f1 with Cell Visibility (A &amp; B) would delete only those cells of row1 under family f1 which has cell visibility A&amp;B or B&amp;A. A delete without any cell visibility would only delete those cells that does not have any cell visibility. In case of delete specific column with latest version only the latest cell with the specified cell visibility will be covered by the delete marker. In case there is no such cell with the specified cell visibility then no cell gets deleted.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11315">HBASE-11315</a> | <em>Major</em> | <strong>Keeping MVCC for configurable longer time</strong></li>
</ul>
<p>After this patch, MVCC value of KVs will be kept for a configurable period &quot;hbase.hstore.compaction.keep.mvcc.period&quot;(default value is 5 days). The minimum setting of &quot;hbase.hstore.compaction.keep.mvcc.period&quot; is also set to 5 days and any user setting less than the min setting will be ignored.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11240">HBASE-11240</a> | <em>Major</em> | <strong>Print hdfs pipeline when hlog's sync is slow</strong></li>
</ul>
<p>Adds emission of a log message that lists datanodes in HDFS pipeline when sync is slow. Set when to log with &quot;hbase.regionserver.hlog.slowsync.ms&quot;. Defaults logging if sync takes &gt; 100ms.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11118">HBASE-11118</a> | <em>Blocker</em> | <strong>non environment variable solution for &quot;IllegalAccessError: class com.google.protobuf.ZeroCopyLiteralByteString cannot access its superclass com.google.protobuf.LiteralByteString&quot;</strong></li>
</ul>
<p>The workaround documented in HBASE-10304 for running a mapreduce job with a &quot;fat jar&quot; is no longer required. Such applications can be launched without adding hbase-protocol.jar to the HADOOP_CLASSPATH.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11520">HBASE-11520</a> | <em>Major</em> | <strong>Simplify offheap cache config by removing the confusing &quot;hbase.bucketcache.percentage.in.combinedcache&quot;</strong></li>
</ul>
<p>Remove &quot;hbase.bucketcache.percentage.in.combinedcache&quot;. Simplifies config of block cache. If you are using this config., after this patch goes in, it will be ignored. The L1 LruBlockCache will be whatever hfile.block.cache.size is set to and the L2 BucketCache will be whatever hbase.bucketcache.size is set to.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-2251">HBASE-2251</a> | <em>Major</em> | <strong>PE defaults to 1k rows - uncommon use case, and easy to hit benchmarks</strong></li>
</ul>
<p>Added a --valueZipf to PE.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-2217">HBASE-2217</a> | <em>Major</em> | <strong>VM OPTS for shell only</strong></li>
</ul>
<p>Set HBASE_SHELL_OPTS in the environment or in hbase-env.sh to supply extra VM arguments when launching the HBase command shell.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11534">HBASE-11534</a> | <em>Minor</em> | <strong>Remove broken JAVA_HOME autodetection in hbase-config.sh</strong></li>
</ul>
<p>This change removes old and increasingly useless JAVA_HOME autodetection that looks for Java 6 runtimes in various locations. Ensure that JAVA_HOME is correctly set in the environment before launching HBase daemons.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11548">HBASE-11548</a> | <em>Trivial</em> | <strong>[PE] Add 'cycling' test N times and unit tests for size/zipf/valueSize calculations</strong></li>
</ul>
<p>Adds --cycles=N argument.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11556">HBASE-11556</a> | <em>Major</em> | <strong>Move HTablePool to hbase-thrift module.</strong></li>
</ul>
<p>HTablePool was deprecated in 0.98.1 but was still present and usable by apps built against versions before HBase 2.0. It has been moved and is not intended to be used by user applications, and is now an internal part of the thrift2 proxy server only.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11492">HBASE-11492</a> | <em>Critical</em> | <strong>Hadoop configuration overrides some ipc parameters including tcpNoDelay</strong></li>
</ul>
<p>If the Hadoop configuration is read after the HBase configuration, Hadoop's settings can override HBase's settings if the names of the settings are the same. To avoid the risk of override, HBase has renamed the following settings (by prepending 'hbase.') so that you can set them independent of your setting for Hadoop. If you do not use the HBase-specific variants, the Hadoop settings will be used.</p>
<h1 id="old-name----new-name">Old Name --&gt; New Name</h1>
<p>ipc.server.listen.queue.size --&gt; hbase.ipc.server.listen.queue.size ipc.server.max.callqueue.size --&gt; hbase.ipc.server.max.callqueue.size ipc.server.callqueue.handler.factor --&gt; hbase.ipc.server.callqueue.handler.factor ipc.server.callqueue.read.share --&gt; hbase.ipc.server.callqueue.read.share ipc.server.callqueue.type --&gt; hbase.ipc.server.callqueue.type ipc.server.queue.max.call.delay --&gt; hbase.ipc.server.queue.max.call.delay ipc.server.max.callqueue.length --&gt; hbase.ipc.server.max.callqueue.length ipc.server.read.threadpool.size --&gt; hbase.ipc.server.read.threadpool.size ipc.server.tcpkeepalive --&gt; hbase.ipc.server.tcpkeepalive ipc.server.tcpnodelay --&gt; hbase.ipc.server.tcpnodelay ipc.client.call.purge.timeout --&gt; hbase.ipc.client.call.purge.timeout ipc.client.connection.maxidletime --&gt; hbase.ipc.client.connection.maxidletime ipc.client.idlethreshold --&gt; hbase.ipc.client.idlethreshold ipc.client.kill.max --&gt; hbase.ipc.client.kill.max ipc.server.scan.vtime.weight --&gt; hbase.ipc.server.scan.vtime.weight</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11600">HBASE-11600</a> | <em>Trivial</em> | <strong>DataInputputStream and DoubleOutputStream are no longer being used</strong></li>
</ul>
<p>Removed unused classes</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-3135">HBASE-3135</a> | <em>Major</em> | <strong>Make our MR jobs implement Tool and use ToolRunner so can do -D trickery, etc.</strong></li>
</ul>
<p>All MR jobs implement Tool Interface, http://hadoop.apache.org/docs/current/api/org/apache/hadoop/util/Tool.html, so now you can pass properties on command line with the -D flag, etc.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11606">HBASE-11606</a> | <em>Minor</em> | <strong>Enable ZK-less region assignment by default</strong></li>
</ul>
<p>By default, we don't use ZK for region assignment now. To fall back to the old way, you can set hbase.assignment.usezk to true.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11558">HBASE-11558</a> | <em>Major</em> | <strong>Caching set on Scan object gets lost when using TableMapReduceUtil in 0.95+</strong></li>
</ul>
<p>TableMapReduceUtil now restores the option to set scanner caching by setting it on the Scan object that is passe in. The priority order for choosing the scanner caching is as follows:</p>
<ol>
<li>Caching set on the scan object.</li>
<li>Caching specified via the config &quot;hbase.client.scanner.caching&quot;, which can either be set manually on the conf or via the helper method TableMapReduceUtil.setScannerCaching().</li>
<li>The default value HConstants.DEFAULT_HBASE_CLIENT_SCANNER_CACHING, which is set to 100 currently.</li>
</ol>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11640">HBASE-11640</a> | <em>Major</em> | <strong>Add syntax highlighting support to HBase Ref Guide programlistings</strong></li>
</ul>
<p>This got committed, so I guess it is safe to resolve it?</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11323">HBASE-11323</a> | <em>Major</em> | <strong>BucketCache all the time!</strong></li>
</ul>
<p>Use the LruBlockCache default if your data fits the blockcache. If block cache churn or you want a block cache that is immune to the vagaries of BC, deploy the offheap bucketcache. See http://people.apache.org/~stack/bc/</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11658">HBASE-11658</a> | <em>Major</em> | <strong>Piped commands to hbase shell should return non-zero if shell command failed.</strong></li>
</ul>
<p>Adds a noninteractive mode (-n or --noninteractive) to the hbase shell that exits with a non-zero error code on failed or invalid shell command executions, and exits with a zero error code upon successful execution.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11527">HBASE-11527</a> | <em>Major</em> | <strong>Cluster free memory limit check should consider L2 block cache size also when L2 cache is onheap.</strong></li>
</ul>
<p>The sum of the heap size % used by Memstore (Configured using 'hbase.regionserver.global.memstore.size' and defaults to 40%) and the that of block cache (Configured using 'hfile.block.cache.size' and defaults to 40%) should leave enough heap size for other normal operations of RS. This is 20%. Also to be noted that when the L2 block cache (ie. Bucket cache) is configured to be on heap, the size of that also comes into this size math. So in that case the sum of memstore heap size, L1 block cache size and L2 cache size (Configured using 'hbase.bucketcache.size')can be at max 80%. When automatic tuning of heap memory is enabled (HBASE-5349) at point of time this 80% cap should hold.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11696">HBASE-11696</a> | <em>Major</em> | <strong>Make CombinedBlockCache resizable.</strong></li>
</ul>
<p>CombinedBlockCache is made resizable. See HBASE-5349 for auto resizing feature. On resize of this block cache, the L1 cache (ie. LRU cache) will get resized</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11657">HBASE-11657</a> | <em>Major</em> | <strong>Put HTable region methods in an interface</strong></li>
</ul>
<p>Add a RegionLocator Interface. Encapsulates 'region' operations. Implemented by HTable and you can get one from an HConnection.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11610">HBASE-11610</a> | <em>Major</em> | <strong>Enhance remote meta updates</strong></li>
</ul>
<p>Adds &quot;hbase.regionstatestore.meta.connection&quot; configuration and new MultiHConnection class. Is set to 1 by default. Set it higher if you want to run with more than one connection to the meta table. Useful if you have a very large meta table -- e.g. 1M regions -- otherwise, stick to the default.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11821">HBASE-11821</a> | <em>Major</em> | <strong>[ImportTSV] Abstract labels tags creation into pluggable Interface</strong></li>
</ul>
<p>Added a facade org.apache.hadoop.hbase.mapreduce.CellCreator to create Cells for HFileOutputFormat. When working with HFileOutputFormat (with or with out visibility labels) in bulk load one can use this facade to make Cells with visibility tags. Also we allow plugin an implementation of org.apache.hadoop.hbase.mapreduce.VisibilityExpressionResolver by configuring the the FQCN with key &quot;hbase.mapreduce.visibility.expression.resolver.class&quot;. The default implementation will tag Cells with visibility label ordinal.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11878">HBASE-11878</a> | <em>Major</em> | <strong>TestVisibilityLabelsWithDistributedLogReplay#testAddVisibilityLabelsOnRSRestart sometimes fails due to VisibilityController not yet initialized</strong></li>
</ul>
<p>A new exception, VisibilityControllerNotReadyException, is introduced for the case where VisibilityController is being initialized. Client can respond to this exception by retrying prior operation. VisibilityControllerNotReadyException isn't a DoNotRetryIOException.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11440">HBASE-11440</a> | <em>Major</em> | <strong>Make KeyValueCodecWithTags as the default codec for replication in trunk</strong></li>
</ul>
<p>By default in master branch the replication would use a new type of codec that would always replicate Cells with tags. {code} {code} &lt;property&gt; &lt;name&gt;hbase.replication.rpc.codec&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.codec.KeyValueCodecWithTags&lt;/value&gt; &lt;/property&gt; {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11919">HBASE-11919</a> | <em>Major</em> | <strong>Remove the deprecated pre/postGet CP hook</strong></li>
</ul>
<p>Removed the below 2 hooks from RegionObserver CP. These were deprecated since 0.96 and having replacement 1. preGet(final ObserverContext&lt;RegionCoprocessorEnvironment&gt; c, final Get get, final List&lt;KeyValue&gt; result) 2. postGet(final ObserverContext&lt;RegionCoprocessorEnvironment&gt; c, final Get get, final List&lt;KeyValue&gt; result) These are replaced with pre/postGetOp hooks</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11331">HBASE-11331</a> | <em>Major</em> | <strong>[blockcache] lazy block decompression</strong></li>
</ul>
<p>When hbase.block.data.cachecompressed=true, DATA (and ENCODED_DATA) blocks are cached in the BlockCache in their on-disk format. This is different from the default behavior, which decompresses and decrypts a block before caching.</p>
<p>For a region server hosting more data than fits into cache, enabling this feature with SNAPPY compression results in 50% increase in throughput and 30% improvement in mean latency while increasing GC by 80% and increasing overall CPU load by 2%.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11845">HBASE-11845</a> | <em>Minor</em> | <strong>HFile tool should implement Tool, disable blockcache by default</strong></li>
</ul>
<p>HFileTool now accepts configuration overrides in the usual way. Blockcache is disabled by default.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11897">HBASE-11897</a> | <em>Minor</em> | <strong>Add append and remove peer table-cfs cmds for replication</strong></li>
</ul>
<p>Adds two new shell commands for operators to add and remove peers at the columnfamily scope.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11847">HBASE-11847</a> | <em>Minor</em> | <strong>HFile tool should be able to print block headers</strong></li>
</ul>
<p>The HFileTool (<code>hbase hfile</code>) can print block headers. The flag is -h,--printblockheaders.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11805">HBASE-11805</a> | <em>Major</em> | <strong>KeyValue to Cell Convert in WALEdit APIs</strong></li>
</ul>
<p>The KeyValue based APIs in WALEdit is been removed and replaced with Cell based APIs void add(KeyValue) -&gt; void add(Cell) ArrayList&lt;KeyValue&gt; getKeyValues() -&gt; ArrayList&lt;Cell&gt; getCells()</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11911">HBASE-11911</a> | <em>Major</em> | <strong>Break up tests into more fine grained categories</strong></li>
</ul>
<p>Adds new test categories besides the class smalltests, mediumtests, and largetests. Adds:</p>
<p>ClientTests CoprocessorTests FilterTests FlakeyTests IOTests MapReduceTests MasterTests MiscTests RegionServerTests ReplicationTests RestTests SecurityTests VerySlowMapReduceTests VerySlowRegionServerTests</p>
<p>See description for examples on how to use them.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-7767">HBASE-7767</a> | <em>Major</em> | <strong>Get rid of ZKTable, and table enable/disable state in ZK</strong></li>
</ul>
<p>Keeps table enabled/disabled state in HDFS rather than up in ZooKeeper. Auto-migrates any existing zk state.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11873">HBASE-11873</a> | <em>Minor</em> | <strong>Hbase Version CLI enhancement</strong></li>
</ul>
<p>Calculates on startup checksum of running code. Emits in logs on startup and shows in UI.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11917">HBASE-11917</a> | <em>Major</em> | <strong>Deprecate / Remove HTableUtil</strong></li>
</ul>
<p>HTableUtil removed in master branch (2.0) and deprecated in 1.0.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12046">HBASE-12046</a> | <em>Major</em> | <strong>HTD/HCD setters should be builder-style</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12048">HBASE-12048</a> | <em>Major</em> | <strong>Remove deprecated APIs from Filter</strong></li>
</ul>
<p>The following APIs are removed from Filter KeyValue transform(KeyValue) KeyValue getNextKeyHint(KeyValue) and replaced with Cell transformCell(Cell) Cell getNextCellHint(Cell) respectively. If a custom Filter implementation have overridden any of these methods, we will no longer call them. User has to change the custom Filter to override cell based methods as shown above</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12054">HBASE-12054</a> | <em>Major</em> | <strong>bad state after NamespaceUpgrade with reserved table names</strong></li>
</ul>
<p>Prior to HBase 0.98.7, if you have 0.94 tables named (data, archive, corrupt, lib, WALs, splitWAl, oldWALs) upgrading would &quot;silently&quot; fail, corrupting the state of those tables.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12084">HBASE-12084</a> | <em>Major</em> | <strong>Remove deprecated APIs from Result</strong></li>
</ul>
<p>The below KeyValue based APIs are removed from Result KeyValue[] raw() List&lt;KeyValue&gt; list() List&lt;KeyValue&gt; getColumn(byte [] family, byte [] qualifier) KeyValue getColumnLatest(byte [] family, byte [] qualifier) KeyValue getColumnLatest(byte [] family, int foffset, int flength, byte [] qualifier, int qoffset, int qlength)</p>
<p>They are replaced with Cell[] rawCells() List&lt;Cell&gt; listCells() List&lt;Cell&gt; getColumnCells(byte [] family, byte [] qualifier) Cell getColumnLatestCell(byte [] family, byte [] qualifier) Cell getColumnLatestCell(byte [] family, int foffset, int flength, byte [] qualifier, int qoffset, int qlength) respectively</p>
<p>Also the constructors which were taking KeyValues also removed Result(KeyValue [] cells) Result(List&lt;KeyValue&gt; kvs)</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12052">HBASE-12052</a> | <em>Major</em> | <strong>BulkLoad Failed due to no write permission on input files</strong></li>
</ul>
<p>SecureBulkLoadEndPoint can be used in un-secure env to bulk load data without hitting &quot;Permission denied&quot; for hbase user.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12111">HBASE-12111</a> | <em>Major</em> | <strong>Remove deprecated APIs from Mutation(s)</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-6290">HBASE-6290</a> | <em>Minor</em> | <strong>Add a function a mark a server as dead and start the recovery the process</strong></li>
</ul>
<p>Adds a script to mark a server as dead.</p>
<p>Usage: considerAsDead.sh --hostname serverName</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12099">HBASE-12099</a> | <em>Major</em> | <strong>TestScannerModel fails if using jackson 1.9.13</strong></li>
</ul>
<p>The XML element used for the visibility labels in the REST API has been renamed from &quot;label&quot; lo &quot;labels&quot; in order to support old and the new versions of jackson.</p>
<p>Developers that use visibility labels via REST XML need to modify their applications in order to use the new XML element name.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11804">HBASE-11804</a> | <em>Major</em> | <strong>Raise default heap size if unspecified</strong></li>
</ul>
<p>When run without a specified heap size the HBase scripts will now let the jvm choose the defaults. Usually this will mean that the max heap size will be the larger of 1gig or 1/4 of physical memory.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12145">HBASE-12145</a> | <em>Major</em> | <strong>Fix javadoc and findbugs so new folks aren't freaked when they see them</strong></li>
</ul>
<p>Fix javadoc warnings.</p>
<p>Fixup findbugs warnings mostly by adding annotations saying 'working as expected'.</p>
<p>In RpcRetryingCallerWithReadReplicas made following change which findbugs spotted:</p>
<ul>
<li><pre><code>  if (completed == null) tasks.wait();</code></pre></li>
<li><pre><code> while (completed == null) tasks.wait();</code></pre></li>
</ul>
<p>In RecoverableZooKeeper, made all zk accesses synchronized -- we were doing it half-ways previously.</p>
<p>In RatioBasedCompactionPolicy we were making an instance of Random on each invocation of getNextMajorCompactionTime</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12133">HBASE-12133</a> | <em>Minor</em> | <strong>Add FastLongHistogram for metric computation</strong></li>
</ul>
<p>Adds Histogram and AtomicLong Utils</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-10153">HBASE-10153</a> | <em>Major</em> | <strong>improve VerifyReplication to compute BADROWS more accurately</strong></li>
</ul>
<p>VerifyReplicaiton reports the following counters besides the existing ones:</p>
<p>ONLY_IN_SOURCE_TABLE_ROWS: number of rows found only in source ONLY_IN_PEER_TABLE_ROWS: number of rows found only in peer CONTENT_DIFFERENT_ROWS: number of rows whose contents are different between source and peer</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11907">HBASE-11907</a> | <em>Minor</em> | <strong>Use the joni byte[] regex engine in place of j.u.regex in RegexStringComparator</strong></li>
</ul>
<p>The RegexStringComparator comparator now include an optional EngineType constructor parameter. Use EngineType.JONI to select the joni regex engine, which can be twice as fast as the Java regex engine and will be more efficient, producing less object churn while scanning, because it operates natively on byte arrays. RegexStringComparator still uses the Java regex engine by default.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-8808">HBASE-8808</a> | <em>Trivial</em> | <strong>Use Jacoco to generate Unit Test coverage reports</strong></li>
</ul>
<p>Adds jacoco plugin. Call it by defining argLine with necessary arguments. By default outputs to target dir.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12016">HBASE-12016</a> | <em>Minor</em> | <strong>Reduce number of versions in Meta table. Make it configurable</strong></li>
</ul>
<p>Clients fetch META table descriptor using RPC. That gives an opportunity to change META table parameters on running cluster. Prior this change all clients used statically compiled META table descriptor and to apply new parameters new code need to be deployed. META table versions can be configured by 'hbase.meta.versions' and now has 3 versions by default (was 10). Block size for META table can be configured by 'hbase.meta.blocksize' (default 8k)</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11997">HBASE-11997</a> | <em>Minor</em> | <strong>CopyTable with bulkload</strong></li>
</ul>
<p>CopyTable now can generate HFiles and bulkload to the destination table.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11990">HBASE-11990</a> | <em>Major</em> | <strong>Make setting the start and stop row for a specific prefix easier</strong></li>
</ul>
<p>Added new utility method, setRowPrefixFilter, to Scan to easily scan for a specific row prefix</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12220">HBASE-12220</a> | <em>Major</em> | <strong>Add hedgedReads and hedgedReadWins metrics</strong></li>
</ul>
<p>Adds metrics hedgedReads and hedgedReadWins counts.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12189">HBASE-12189</a> | <em>Major</em> | <strong>Fix new issues found by coverity static analysis</strong></li>
</ul>
<p>Fixes up the worst offenders reported by coverity. Many hundreds more of issues to address but seem minor relatively on review.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12197">HBASE-12197</a> | <em>Major</em> | <strong>Move REST</strong></li>
</ul>
<p>HBase Rest has moved to it's own module. If you previously were using the rest sever code in your project you will now need to depend upon hbase-rest module in addition to the hbase-server module.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12241">HBASE-12241</a> | <em>Critical</em> | <strong>The crash of regionServer when taking deadserver's replication queue breaks replication</strong></li>
</ul>
<p>This fix includes our enabling useMulti flag as default. multi is a zk method only available in later versions of zookeeper. This change means HBase 1.0 requires a zookeeper that is at least version 3.4+. See HBASE-6775 for background.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12250">HBASE-12250</a> | <em>Minor</em> | <strong>Adding an endpoint for updating the regionserver config</strong></li>
</ul>
<p>Adds Admin#updateConfiguration(ServerName) and Admin#updateConfiguration() for loading config. dynamically.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12286">HBASE-12286</a> | <em>Major</em> | <strong>[shell] Add server/cluster online load of configuration changes</strong></li>
</ul>
<p>Adds shell commands:</p>
<p>update_config 'servername' update_all_config</p>
<p>These call new Admin methods added in HBASE-12147 to change server config. Only a small subset of configs are changeable without restart currently mostly to do with compaction configuration. See the refguide on dynamic configuration for a list.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12317">HBASE-12317</a> | <em>Minor</em> | <strong>Run IntegrationTestRegionReplicaPerf w.o mapred</strong></li>
</ul>
<p>IntegrationTestRegionReplicaPerf no longer accepts the --nomapred flag. Instead, --nomapred is always used.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12314">HBASE-12314</a> | <em>Major</em> | <strong>Add chaos monkey policy to execute two actions concurrently</strong></li>
</ul>
<p>Adds a new chaos monkey noKill that won't kill services. Instead it runs two or more ddl commands at the same time.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-2609">HBASE-2609</a> | <em>Major</em> | <strong>Harmonize the Get and Delete operations</strong></li>
</ul>
<p>Align Delete class with Get class. Change all delete* method names to add* instead. For example, change deleteColumn to addColumn (as in add-this-column to the Delete specification). Deprecate the old.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11912">HBASE-11912</a> | <em>Major</em> | <strong>Catch some bad practices at compile time with error-prone</strong></li>
</ul>
<p>Errors from error-prone will fail the build in the compile phase. Warnings look like Javac warnings and are counted as such by test-patch etc</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11562">HBASE-11562</a> | <em>Major</em> | <strong>CopyTable should provide an option to shuffle the mapper tasks</strong></li>
</ul>
<p>Adds new 'shuffle' option to CopyTable that shuffles the maps order; use if you need some random on what tasks of a table copy run when.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12075">HBASE-12075</a> | <em>Major</em> | <strong>Preemptive Fast Fail</strong></li>
</ul>
<p>The Interceptor Framework can be used to modify the behavior of retrying client calls on the client side without altering the core of the client code. This framework gives reasonable flexibility with little intrusion.</p>
<p>An example use case of the interceptor is to have a custom sleep strategy within retries. We can consider a simple accounting system where we have a time spent quota on the client side for the servers. And we would want to error some of the calls if we have consumed the server's quota already.</p>
<p>Interceptor.intercept() { // would keep track the start time. server = getServer() // Throw if we already spent a lot of time on a server. timeSpentOnServer = getTimeSpentSinceLastReset(server) if ( timeSpentOnServer &gt; THRESHOLD) throw new Exception(&quot;We spend too long on this server&quot;) setServerStartTime(server) }</p>
<p>Interceptor.handleFailure() { server = getServer() updateTimeSpentOnServer(server) }</p>
<p>Interceptor.updateFailureInfo() { server = getServer() updateTimeSpentOnServer(server) }</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12297">HBASE-12297</a> | <em>Major</em> | <strong>Support DBB usage in Bloom and HFileIndex area</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12354">HBASE-12354</a> | <em>Major</em> | <strong>Update dependencies in time for 1.0 release</strong></li>
</ul>
<p>Updated dependencies. Of note, went from hadoop 2.2 to 2.5.1.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12381">HBASE-12381</a> | <em>Minor</em> | <strong>Add maven enforcer rules for build assumptions</strong></li>
</ul>
<p>Enforces maven &gt;= 3.0.3 (based on the oldest version we have building on jenkins) and java &gt;= the source compilation target variable, which is 1.7 on master and branch-1 (based on java compat doc) and 1.6 before this.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12335">HBASE-12335</a> | <em>Major</em> | <strong>IntegrationTestRegionReplicaPerf is flaky</strong></li>
</ul>
<p>This change alters PerformanceEvaluation to expose read request histograms back to consumers. IntegrationTestRegionReplicaPerf takes advantage of this by examining the histograms to determine the overall effectiveness of the feature under test. In this case, specific summary statistics are aggregated across all threads and all test iterations to determine success. These summaries are used to make the test more robust in the face of run-to-run variation.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12142">HBASE-12142</a> | <em>Minor</em> | <strong>Truncate command does not preserve ACLs table</strong></li>
</ul>
<p>Prior to this change, the truncate shell command could not preserve ACLs on the table being truncated. In the 0.98 branch, this change also backports HBASE-8332, which adds a master handler for table truncation and new HBaseAdmin APIs for same.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12389">HBASE-12389</a> | <em>Minor</em> | <strong>Reduce the number of versions configured for the ACL table</strong></li>
</ul>
<p>This change reduces the number of versions kept in the ACL table from 10 to 1. This is a fully compatible change. Existing ACL tables will not be affected, only newly initialized ones. Furthermore, old versions of ACL table entries were never considered by the AccessController in decision making, and any and all entries in the ACL table are not user accessible.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-8707">HBASE-8707</a> | <em>Minor</em> | <strong>Add LongComparator for filter</strong></li>
</ul>
<p>Adds a LongComparator for use by Filters.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12329">HBASE-12329</a> | <em>Minor</em> | <strong>Table create with duplicate column family names quietly succeeds</strong></li>
</ul>
<p>Tables created with duplicate column family names use to succeed silently. This removes fix removes ambiguity and will change the behavior of code that depended upon this behavior. Instead of just using addFamil, a change to the CF definition now requires a modifyFamily call.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12432">HBASE-12432</a> | <em>Minor</em> | <strong>RpcRetryingCaller should log after fixed number of retries like AsyncProcess</strong></li>
</ul>
<p>hbase.client.start.log.errors.counter now also applies to Scanners.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12455">HBASE-12455</a> | <em>Major</em> | <strong>Add 'description' to bean and attribute output when you do /jmx?description=true</strong></li>
</ul>
<p>When you click on 'Metrics Dump', it takes you to the servlet at /jmx. This servlet dumps all metrics by name and their value. If you add a query string of '?description=true', the dump will include descriptions on all metric mbeans and attributes that are non-null and that do not have a description that matches the attribute name exactly.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-10483">HBASE-10483</a> | <em>Major</em> | <strong>Provide API for retrieving info port when hbase.master.info.port is set to 0</strong></li>
</ul>
<p>Adds new method to Admin so clients can ask the master's info port. Also fixes info port display in UI to use what master is actually using rather than display what is Configuration</p>
<p>{code} + /** + * Get the info port of the current master if one is available. + * @return master info port + * @throws IOException + */ + public int getMasterInfoPort() throws IOException; {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12411">HBASE-12411</a> | <em>Major</em> | <strong>Optionally enable p-reads and private readers for compactions</strong></li>
</ul>
<p>This introduces two new configuration options:</p>
<h1 id="hbase.storescanner.use.pread-enables-postional-reads-for-all-scanners-except-compactions.-default-off-i.e.-try-to-use-seekread-as-before">hbase.storescanner.use.pread enables postional reads for all scanners (except compactions). Default off (i.e. try to use seek+read as before)</h1>
<h1 id="hbase.regionserver.compaction.private.readers-has-compaction-run-with-their-own-readers.-default-off-i.e.-share-readers-with-all-other-scanners-as-before">hbase.regionserver.compaction.private.readers has compaction run with their own readers. Default off (i.e. share readers with all other scanners as before)</h1>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12277">HBASE-12277</a> | <em>Major</em> | <strong>Refactor bulkLoad methods in AccessController to its own interface</strong></li>
</ul>
<p>Adds new BulkLoadObserver Interface</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12536">HBASE-12536</a> | <em>Major</em> | <strong>Reduce the effective scope of GLOBAL CREATE and ADMIN permission</strong></li>
</ul>
<p>This change removes implicit write access to the META and ACL tables for any user with GLOBAL CREATE or ADMIN privilege. Users with GLOBAL CREATE will not be able to elevate their privileges unexpectedly through direct access to the ACL table. A GLOBAL ADMIN will still correctly be allowed to grant themselves any desired privilege.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-8572">HBASE-8572</a> | <em>Major</em> | <strong>Enhance delete_snapshot.rb to call snapshot deletion API with regex</strong></li>
</ul>
<p>Adds a new shell command that allows you delete snapshots that match a regex:</p>
<p>hbase&gt; delete_all_snapshot 's.*'</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-10378">HBASE-10378</a> | <em>Major</em> | <strong>Divide HLog interface into User and Implementor specific interfaces</strong></li>
</ul>
<p>HBase internals for the write ahead log have been refactored. Advanced users of HBase should be aware of the following changes.</p>
<p>Public Audience - The Admin API for asking a region server to roll WAL files has changed from a synchronous command that returns a set of regions the WAL implementation would like flushed into an asynchronous command that returns nothing. Older clients relying on the former behavior will still be able to interact with newer servers, but the response body will always contain an empty list of regions to flush. - The shell command &quot;hlog_roll&quot; has been deprecated. Operators should use the &quot;wal_roll&quot; command instead. This command is subject to the changes described above for the Admin API to roll WAL files. - The command for analyzing write ahead logs has been renamed from 'hlog' to 'wal'. The old usage is deprecated and will be removed in a future version. - Some utility methods in the HBaseTesetingUtility related to testing write-ahead-logs were changed in incompatible ways. No functionality has been removed, but method names and arguments have changed. See the HBaseTestingUtility javadoc for details. - The WALPlayer utility has deprecated the configuration keys used for advanced customization. Users should switch to the updated configuration keys. See the usage information on the WALPlayer tool for details. - The HLogInputFormat utility class for processing logs with MapReduce has been deprecated and will be removed in a future version. Users should switch to the WALInputFormat. - The labeling of server metrics on the region server status pages changed. Previously, the number of backing files for the write ahead log was labeled 'Num. HLog Files'. If you wish to see this statistic now, please look for the label 'Num. WAL Files.' If you rely on JMX for these metrics, their location has not changed.</p>
<p>LimitedPrivate(COPROC) Audience, LimitedPrivate(PHOENIX) - The RegionObserver API has been updated. The changes are both binary and source backwards compatible for coprocessors that use the BaseRegionObserver class. For those that implement RegionObserver directly the changes are binary backwards compatible. Depending on the internals of future HBase versions, coprocessors using the deprecated API may not see all WAL related events. Users are strongly encouraged to update their use of the API; see the RegionObserver javadoc for details. - Classes related to reading WAL entries (ReaderBase, ProtobufLogReader, SequenceFileLogReader) have changed in a backwards incompatible way. Users who referenced HLog.Reader directly or HLog.Entry will have to update. These changes do not impact compatibility with extant wal files. - The WALObserver API has been updated. The changes are both binary and source backwards compatible for coprocessors that use the BaseWALObserver class. For those that implement WALObserver directly the changes are binary backwards compatible. Depending on the internals of future HBase versions, coprocessors using the deprecated API may not see all WAL related events. Users are strongly encouraged to update their use of the API; see the WALObserver javadoc for details. - The WALCoprocessorEnvironment has changed in a backwards incompatible way. WALObserver coprocessors that relied on retrieving an object representing the write ahead log instance will have to be updated.</p>
<p>LimitedPrivate(REPLICATION) Audience - The WALEntryFilter API has changed in a backwards incompatible way. Implementers will have to be updated. - The ReplicationEndpoint.ReplicateContext API has changed in a backwards incompatible way. Implementers who use this interface will have to be updated. These changes do not impact wire compatibility for replicating between clusters. - The HLogKey API is deprecated in favor of the WALKey API. Additionally, the HLogKey API has changed in a backwards incompatible way by changing from implementing WriteableComparable&lt;HLogKey&gt; to implementing Writeable and Comparable&lt;WALKey&gt;.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12346">HBASE-12346</a> | <em>Major</em> | <strong>Scan's default auths behavior under Visibility labels</strong></li>
</ul>
<p>When no Authorizations passed in Scan , will take user's Auth labels for the Scan and return results accordingly. Prior to this Jira, we were returning only those cells with out any visibility in such a case.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12569">HBASE-12569</a> | <em>Minor</em> | <strong>Control MaxDirectMemorySize in the same manner as heap size</strong></li>
</ul>
<p>Adds new HBASE_OFFHEAPSIZE environment variable to ./bin/hbase. Set the max offheap memory java will request with this one variable. It combines with HBASE_HEAPSIZE to determine the max amount of ram that the JVM can request.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12017">HBASE-12017</a> | <em>Major</em> | <strong>Use Connection.createTable() instead of HTable constructors.</strong></li>
</ul>
<p>This was fixed by patches for other issues.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-10536">HBASE-10536</a> | <em>Major</em> | <strong>ImportTsv should fail fast if any of the column family passed to the job is not present in the table</strong></li>
</ul>
<p>Added new Feature to check for column family name of the destination table before bulk load runs.</p>
<p>By default it checks whether column family matches the destination table. To disable the check provide the option: -Dno.strict=true</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12559">HBASE-12559</a> | <em>Major</em> | <strong>Provide LoadBalancer with online configuration capability</strong></li>
</ul>
<p>updateConfiguration(ServerName server) method of Admin now updates config for HMaster as well. Specifically, config update would be taken by load balancer.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11639">HBASE-11639</a> | <em>Major</em> | <strong>[Visibility controller] Replicate the visibility of Cells as strings</strong></li>
</ul>
<p>Allows the visibility tags to be replicated as Strings to the peer cluster. Useful in cases where the peer cluster and the source cluster does not have a common global LABELs table. In order to enable this feature configure the below property in hbase-site.xml &lt;property&gt; &lt;name&gt; hbase.coprocessor.regionserver.classes &lt;/name&gt; &lt;value&gt; org.apache.hadoop.hbase.security.visibility.VisibilityController$VisibilityReplication &lt;/value&gt; &lt;property&gt;</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12597">HBASE-12597</a> | <em>Major</em> | <strong>Add RpcClient interface and enable changing of RpcClient implementation</strong></li>
</ul>
<p>Adds an RpcClient Interface and an RpcClientFactory to get RpcClient implementation.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12665">HBASE-12665</a> | <em>Major</em> | <strong>When aborting, dump metrics</strong></li>
</ul>
<p>When regionserver aborts, we dump subset of metrics into the log (We used to do this too pre-0.96 but was lost when we refactored metrics)</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12373">HBASE-12373</a> | <em>Minor</em> | <strong>Provide a command to list visibility labels</strong></li>
</ul>
<p>Adds new list_labels command to shell.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12644">HBASE-12644</a> | <em>Major</em> | <strong>Visibility Labels: issue with storing super users in labels table</strong></li>
</ul>
<p>The system visibility label authorization for super users will no longer be persisted in hbase:labels table. Super users will be determined at server startup time. They will have all the permissions for Visibility labels. If you have a prior deployment that had super users' system label persisted in hbase:labels, you can clean up by invoking the shell command 'clear_auths'. For example: clear_auths 'old_superuser', 'system' This is particularly necessary when you change super users, i.e. a previous super user is no longer a super user.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-10201">HBASE-10201</a> | <em>Major</em> | <strong>Port 'Make flush decisions per column family' to trunk</strong></li>
</ul>
<p>Adds new flushing policy mechanism. Default, org.apache.hadoop.hbase.regionserver.FlushLargeStoresPolicy, will try to avoid flushing out the small column families in a region, those whose memstores are &lt; hbase.hregion.percolumnfamilyflush.size.lower.bound. To restore the old behavior of flushes writing out all column families, set hbase.regionserver.flush.policy to org.apache.hadoop.hbase.regionserver.FlushAllStoresPolicy either in hbase-default.xml or on a per-table basis by setting the policy to use with HTableDescriptor.getFlushPolicyClassName().</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12121">HBASE-12121</a> | <em>Minor</em> | <strong>maven release plugin does not allow for customized goals</strong></li>
</ul>
<p>Allows user to add goals to release procedure mvn release:perform -Dgoal=&lt;goal-name&gt; If no goal is specified default behavior is used</p>
<p>Example use case: Release to private repository and upload artifacts but also want the tarball to be uploaded, not just the jars. the following could be added to the release command -Dgoal=&quot;-X package install site assembly:single deploy -DskipTests&quot; This will execute the release procedure and also upload the tarball along with all jars.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12718">HBASE-12718</a> | <em>Major</em> | <strong>Convert TestAcidGuarantees from a unit test to an integration test</strong></li>
</ul>
<p>TestAcidGuarantees used to be runnable from the command line. It has been renamed to IntegrationTestAcidGuarantees and allows for ChaosMonkey parameters.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-5699">HBASE-5699</a> | <em>Critical</em> | <strong>Run with &gt; 1 WAL in HRegionServer</strong></li>
</ul>
<p>HBase's write-ahead-log (WAL) can now be configured to use multiple HDFS pipelines in parallel to provide better write throughput for clusters by using additional disks. By default, HBase will still use only a single HDFS-based WAL.</p>
<p>To run with multiple WALs, alter the hbase-site.xml property &quot;hbase.wal.provider&quot; to have the value &quot;multiwal&quot;. To return to having HBase determine what kind of WAL implementation to use either remove the property all together or set it to &quot;defaultProvider&quot;.</p>
<p>Altering the WAL provider used by a particular RegionServer requires restarting that instance. RegionServers using the original WAL implementation and those using the &quot;multiwal&quot; implementation can each handle recovery of either set of WALs, so a zero-downtime configuration update is possible through a rolling restart.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11412">HBASE-11412</a> | <em>Minor</em> | <strong>Minimize a number of hbase-client transitive dependencies</strong></li>
</ul>
<p>Removes some transitive dependencies from the hbase-client module.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12695">HBASE-12695</a> | <em>Critical</em> | <strong>JDK 1.8 compilation broken</strong></li>
</ul>
<p>Use the -Pjavac maven profile in order to compile HBase using the compiler provided by the JDK instead of the default error-prone compiler plugin. This is useful for now if you are building HBase with JDK 1.8 or a JDK that doesn't support error-prone.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12775">HBASE-12775</a> | <em>Major</em> | <strong>CompressionTest ate my HFile (sigh!)</strong></li>
</ul>
<p>CompressionTest will now abort when the target path exists.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12028">HBASE-12028</a> | <em>Major</em> | <strong>Abort the RegionServer, when it's handler threads die</strong></li>
</ul>
<p>Adds a configuration property &quot;hbase.regionserver.handler.abort.on.error.percent&quot; for aborting the region server when some of it's handler threads die. The default value is 0.5 causing an abort in the RS when half of it's handler threads die. A handler thread only dies in case of a serious software bug, or a non-recoverable Error (StackOverflow, OOM, etc) is thrown. These are possible values for the configuration: * -1 =&gt; Disable aborting * 0 =&gt; Abort if even a single handler has died * 0.x =&gt; Abort only when this percent of handlers have died * 1 =&gt; Abort only all of the handers have died</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12268">HBASE-12268</a> | <em>Major</em> | <strong>Add support for Scan.setRowPrefixFilter to shell</strong></li>
</ul>
<p>Added new option, ROWPREFIXFILTER, to the scan command in the HBase shell to easily scan for a specific row prefix.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12798">HBASE-12798</a> | <em>Major</em> | <strong>Map Reduce jobs should not create Tables in setConf()</strong></li>
</ul>
<p>TableInputFormatBase#initialize() has been added:</p>
<p>/** * This method will be called when any of the following are referenced, but not yet initialized: * admin, regionLocator, table. Subclasses will have the opportunity to call * {@link #initializeTable(Connection, TableName)} */ protected void initialize() {</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11144">HBASE-11144</a> | <em>Major</em> | <strong>Filter to support scanning multiple row key ranges</strong></li>
</ul>
<p>MultiRowRangeFilter is a filter to support scanning multiple row key ranges. If the number of the ranges is small, using multiple scans can also do the same thing and can work well. But when the number of ranges are quite big (e.g. millions), use the MultiRowRangeFilter will be nice. In this filter, the ranges will be sorted and merged, so users do not have to take care of ranges are not continuous. And if users are using something like rest, thrift or pig to access the data the filter might be the practical solution.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12848">HBASE-12848</a> | <em>Major</em> | <strong>Utilize Flash storage for WAL</strong></li>
</ul>
<p>For users on a version of Hadoop that supports tiered storage policies (i.e. Apache Hadoop 2.6.0+), HBase now allows users to opt-in to having the write ahead log placed on the SSD tier. Users on earlier versions of Hadoop will be unable to take advantage of this feature.</p>
<p>Use of tiered storage is controlled by a new RegionServer config, hbase.wal.storage.policy. It defaults to the value 'NONE', which will rely on HDFS defaults for a policy decision.</p>
<p>User can specify ONE_SSD or ALL_SSD as the value: ONE_SSD: place only one replica of WAL files in SSD and the remaining in default storage ALL_SSD: all replica for WAL files are placed on SSD</p>
<p>See [the HDFS docs on storage policy|http://hadoop.apache.org/docs/r2.6.0/hadoop-project-dist/hadoop-hdfs/ArchivalStorage.html]</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12774">HBASE-12774</a> | <em>Minor</em> | <strong>Fix the inconsistent permission checks for bulkloading.</strong></li>
</ul>
<p>Bulk load permissions are changed from requiring both C and W to only require C.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12731">HBASE-12731</a> | <em>Major</em> | <strong>Heap occupancy based client pushback</strong></li>
</ul>
<p>This feature incorporates reported regionserver heap occupancy in the (optional) client pushback calculations. If client pushback is enabled, the exponential backoff policy will take heap occupancy into account, should it exceed &quot;hbase.heap.occupancy.low_water_mark&quot; percentage of the heap (default 0.95). Once above the low water mark, heap occupancy is an additional factor scaling from 0.1 up to 1.0 at &quot;hbase.heap.occupancy.high_water_mark&quot; (default 0.98). At or above the high water mark the client will use the maximum configured backoff.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12899">HBASE-12899</a> | <em>Major</em> | <strong>HBase should prefix htrace configuration keys with &quot;hbase.htrace&quot; rather than just &quot;hbase.&quot;</strong></li>
</ul>
<p>All htrace related configuration options are renamed to have &quot;hbase.htrace&quot; prefix instead of just &quot;htrace&quot;.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12902">HBASE-12902</a> | <em>Major</em> | <strong>Post-asciidoc conversion fix-ups</strong></li>
</ul>
<p>Pushed to master. Shout if there are any issues.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12728">HBASE-12728</a> | <em>Blocker</em> | <strong>buffered writes substantially less useful after removal of HTablePool</strong></li>
</ul>
<p>In our pre-1.0 API, HTable is considered a light-weight object that consumed by a single thread at a time. The HTablePool class provided a means of sharing multiple HTable instances across a number of threads. As an optimization, HTable managed a &quot;write buffer&quot;, accumulating edits and sending a &quot;batch&quot; all at once. By default the batch was sent as the last step in invocations of put(Put) and put(List&lt;Put&gt;). The user could disable the automatic flushing of the write buffer, retaining edits locally and only sending the whole &quot;batch&quot; once the write buffer has filled or when the flushCommits() method in invoked explicitly. Explicit or implicit batch writing was controlled by the setAutoFlushTo(boolean) method. A value of true (the default) had the write buffer flushed at the completion of a call to put(Put) or put(List&lt;Put&gt;). A value of false allowed for explicit buffer management. HTable also exposed the buffer to consumers via getWriteBuffer().</p>
<p>The combination of HTable with setAutoFlushTo(false) and the HTablePool provided a convenient mechanism by which multiple &quot;Put-producing&quot; threads could share a common write buffer. Both HTablePool and HTable are deprecated, and they are officially replaced in The new 1.0 API by Table and BufferedMutator. Table, which replaces HTable, no longer exposes explicit write-buffer management. Instead, explicit buffer management is exposed via BufferedMutator. BufferedMutator is made safe for concurrent use. Where code would previously retrieve and return HTables from an HTablePool, now that code creates and shares a single BufferedMutator instance across all threads.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-8410">HBASE-8410</a> | <em>Major</em> | <strong>Basic quota support for namespaces</strong></li>
</ul>
<p>Namespace auditor provides basic quota support for namespaces in terms of number of tables and number of regions. In order to use namespace quotas, quota support must be enabled by setting &quot;hbase.quota.enabled&quot; property to true in hbase-site.xml file.</p>
<p>The users can add quota information to namespace, while creating new namespaces or by altering existing ones.</p>
<p>Examples: 1. create_namespace 'ns1', {'hbase.namespace.quota.maxregions'=&gt;'10'} 2. create_namespace 'ns2', {'hbase.namespace.quota.maxtables'=&gt;'2','hbase.namespace.quota.maxregions'=&gt;'5'} 3. alter_namespace 'ns3', {METHOD =&gt; 'set', 'hbase.namespace.quota.maxtables'=&gt;'5','hbase.namespace.quota.maxregions'=&gt;'25'}</p>
<p>The quotas can be modified/added to namespace at any point of time. To remove quotas, the following command can be used:</p>
<p>alter_namespace 'ns3', {METHOD =&gt; 'unset', NAME =&gt; 'hbase.namespace.quota.maxtables'} alter_namespace 'ns3', {METHOD =&gt; 'unset', NAME =&gt; 'hbase.namespace.quota.maxregions'}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12684">HBASE-12684</a> | <em>Major</em> | <strong>Add new AsyncRpcClient</strong></li>
</ul>
<p>Retrofit a new, netty-based rpc transport on the client. This client is slightly slower if little contention given the extra tier or so that netty adds and that we block on a Future waiting on the call to finish. This client opens the way for HBase having a native Async API.</p>
<p>This client is on by default in master branch (2.0 hbase). It is off in branch-1.0 (hbase-1.1.x). To enable it, set &quot;hbase.rpc.client.impl&quot; to &quot;org.apache.hadoop.hbase.ipc.AsyncRpcClient&quot;</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12745">HBASE-12745</a> | <em>Major</em> | <strong>Visibility Labels: support visibility labels for user groups.</strong></li>
</ul>
<p>VisibilityClient API and shell commands can be used to grant and clear visibility authorizations of a group. e.g. set_auths '@group1', ['SECRET','PRIVATE'] get_auths '@group1' clear_auths '@group1', ['SECRET','PRIVATE']</p>
<p>When checking visibility authorizations of a user, the server will include the visibility authorizations of the groups of which the user is a member, together with the user's own.</p>
<p>On the other hand, get_auths 'user1' will only get user1's own visibility authorizations. clear_auths 'user1' will only clear user1's own visibility authorizations. The visibility authorizations of a group can be changed by invoking the API or command on the '@group1' itself.</p>
<p>Note:</p>
<p>The following two methods have been deprecated in VisibilityLabelService from 0.98.10 and will be removed in 2.0+ releases. getAuths(byte[], boolean) havingSystemAuth(byte[])</p>
<p>Use the following methods instead: getUserAuths(byte[], boolean) getGroupAuths(String[], boolean) havingSystemAuth(User)</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12808">HBASE-12808</a> | <em>Major</em> | <strong>Use Java API Compliance Checker for binary/source compatibility</strong></li>
</ul>
<p>Adds a dev-support/check_compatibility.sh script for comparing versions. Run the script to see usage.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12831">HBASE-12831</a> | <em>Major</em> | <strong>Changing the set of vis labels a user has access to doesn't generate an audit log event</strong></li>
</ul>
<p>Auditing of visibility label administration.</p>
<p>Administrative actions on visibility labels, such as creation of a label or changing the set of labels a user or group may access, are now sent to the audit log. The audit messages should be similar to those already tracked by the access controller.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-6778">HBASE-6778</a> | <em>Major</em> | <strong>Deprecate Chore; its a thread per task when we should have one thread to do all tasks</strong></li>
</ul>
<p>Corresponding usages for new ScheduledChore vs. Deprecated Chore: Chore.interrupt() -&gt; ScheduledChore.cancel(mayInterruptWhileRunning = true) Threads.setDaemonThreadRunning(Chore) -&gt; ChoreService.scheduleChore(ScheduledChore) Chore.isAlive -&gt; ScheduledChore.isScheduled() Chore.getSleeper().skipSleepCycle() -&gt; ScheduledChore.triggerNow()</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-8329">HBASE-8329</a> | <em>Major</em> | <strong>Limit compaction speed</strong></li>
</ul>
<p>Adds compaction throughput limit mechanism(the word &quot;throttle&quot; is already used when choosing compaction thread pool, so use a different word here to avoid ambiguity). Default is org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController, will limit throughput as follow: 1. In off peak hours, use a fixed limitation &quot;hbase.hstore.compaction.throughput.offpeak&quot; (default is Long.MAX_VALUE which means no limitation). 2. In normal hours, the limitation is tuned between &quot;hbase.hstore.compaction.throughput.lower.bound&quot;(default 10MB/sec) and &quot;hbase.hstore.compaction.throughput.higher.bound&quot;(default 20MB/sec), using the formula &quot;lower + (higer - lower) * param&quot; where param is in range [0.0, 1.0] and calculate based on store files count on this regionserver. 3. If some stores have too many store files(storefilesCount &gt; blockingFileCount), then there is no limitation no matter peak or off peak. You can set &quot;hbase.regionserver.throughput.controller&quot; to org.apache.hadoop.hbase.regionserver.throttle.NoLimitThroughputController to disable throughput controlling. And we have implemented ConfigurationObserver which means you can change all configurations above and do not need to restart cluster.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12108">HBASE-12108</a> | <em>Minor</em> | <strong>HBaseConfiguration: set classloader before loading xml files</strong></li>
</ul>
<p>This patch fixes hbase configuration not found issues when HBase jars are loaded in a child class loader whose parent has already loaded hadoop classes. Setting appropriate classloader in Configuration should fix this issue.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-7332">HBASE-7332</a> | <em>Minor</em> | <strong>[webui] HMaster webui should display the number of regions a table has.</strong></li>
</ul>
<p>Adds counts for various regions states to the table listing on main page. See attached screenshot.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12961">HBASE-12961</a> | <em>Minor</em> | <strong>Negative values in read and write region server metrics</strong></li>
</ul>
<p>Change read and write request count in ServerLoad from int to long</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12976">HBASE-12976</a> | <em>Major</em> | <strong>Set default value for hbase.client.scanner.max.result.size</strong></li>
</ul>
<p>With this setting defaulted now, scanner caching should be set to large value unless the caller knows how many rows it needs/expects back.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12897">HBASE-12897</a> | <em>Major</em> | <strong>Minimum memstore size is a percentage</strong></li>
</ul>
<p>This change lowers the minimum acceptable configured memstore heap percentage from 5% (0.05f) (10% (0.1f) in 0.98) to 0% (0.0f), and in branch 0.98 lowers the maximum acceptable percentage from 90% (0.9f) to 80% (0.8f).</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-9910">HBASE-9910</a> | <em>Major</em> | <strong>TestHFilePerformance and HFilePerformanceEvaluation should be merged in a single HFile performance test class.</strong></li>
</ul>
<p>Add support for codec and cipher in HFilePerformanceEvaluation</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12978">HBASE-12978</a> | <em>Blocker</em> | <strong>Region goes permanently offline (WAS: hbase:meta has a row missing hregioninfo and it causes my long-running job to fail)</strong></li>
</ul>
<p>Fixes a bug where an optimization that keeps our hfile indexes files small chanced upon an entry that has special meaning in the meta table making it so we failed to find a region entry in an hbase:meta HFile (though the entry was present). Once we'd happened upon this condition, the region would be unfindable thereafter making it so the region was considered offline. The bug is in branch-1+ hbase only.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13008">HBASE-13008</a> | <em>Minor</em> | <strong>Better default for hbase.regionserver.regionSplitLimit parameter.</strong></li>
</ul>
<p>The configuration parameter &quot;hbase.regionserver.regionSplitLimit&quot;, the number of regions a regionserver can open before splitting on the server is stopped to limit further region count growth, was lowered from Integer.MAX_VALUE (2147483647) to a much more reasonable 1000, and documented in hbase-defaults.xml.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13030">HBASE-13030</a> | <em>Major</em> | <strong>[1.0.0 polish] Make ScanMetrics public again and align Put 'add' with Get, Delete, etc., addColumn</strong></li>
</ul>
<p>Miscellaneous cleanup. Make ScanMetrics audience public -- was mistakenly made private -- and make their access more amenable. Make Put like Delete and Get adding addColumn method (deprecating add). Make the ByteBuffer returned by Cell read only. Add to CellUtil a method to get a ByteRange on a value.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13028">HBASE-13028</a> | <em>Blocker</em> | <strong>Cleanup mapreduce API changes</strong></li>
</ul>
<p>The bindings for reading data from HBase using both the Hadoop MapReduce mapred and mapreduce libraries now are initialized the same way. Subclasses wishing to build on TableInputFormatBase that use the new initialize method will have access to job configuration information.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13044">HBASE-13044</a> | <em>Minor</em> | <strong>Configuration option for disabling coprocessor loading</strong></li>
</ul>
<p>This change adds two new configuration options: - &quot;hbase.coprocessor.enabled&quot; controls globally if any coprocessors will be loaded. Set to &quot;false&quot; to disable. Defaults to &quot;true&quot; for compatibility with previous releases. - &quot;hbase.coprocessor.user.enabled&quot; controls if any user (aka table) coprocessors will be loaded. Set to &quot;false&quot; to disable. Defaults to &quot;true&quot; for compatibility with previous releases.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13047">HBASE-13047</a> | <em>Trivial</em> | <strong>Add &quot;HBase Configuration&quot; link missing on the table details pages</strong></li>
</ul>
<p>Add a '/conf' link to UI</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12869">HBASE-12869</a> | <em>Major</em> | <strong>Add a REST API implementation of the ClusterManager interface</strong></li>
</ul>
<p>Adds an implementation of ClusterManager to control REST API-managed HBase clusters.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13142">HBASE-13142</a> | <em>Major</em> | <strong>[PERF] Reuse the IPCUtil#buildCellBlock buffer</strong></li>
</ul>
<p>Adds buffer reuse sending Cell results. It is on by default and should not need configuration. Improves GC profile and ups throughput. The benefit gets better the larger the row size returned.</p>
<p>The buffer reservoir is bounded at a maximum count after which we will start logging at WARN level that the reservoir is running at capacity (returned buffers will be discarded and not added back to the reservoir pool). Default maximum is twice the handler count: i.e. 2 * hbase.regionserver.handler.count. This should be more than enough. Set the maximum with the new configuration: hbase.ipc.server.reservoir.max</p>
<p>The reservoir will not cache buffers in excess of hbase.ipc.server.reservoir.max.buffer.size The default is 10MB. This means that if a row is very large, then we will allocate a buffer of the average size that is currently in the pool and we will then resize it till we can accommodate the return. These resizes are expensive. The resultant buffer will be used and then discarded.</p>
<p>To check how the reservoir is doing, enable trace level logging for a few seconds on a regionserver. You can do this from the regionserver UI. See 'Log Level'. Set org.apache.hadoop.hbase.io.BoundedByteBufferPool to TRACE. The BoundedByteBufferPool will spew report to the log. Disable the TRACE level and then check the log. You'll see allocation rate, size of pool, size of buffers in pool, etc.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12706">HBASE-12706</a> | <em>Critical</em> | <strong>Support multiple port numbers in ZK quorum string</strong></li>
</ul>
<p>hbase.zookeeper.quorum configuration now allows servers together with client ports consistent with the way Zookeeper java client accepts the quorum string. In this case, using hbase.zookeeper.clientPort is not needed. eg. hbase.zookeeper.quorum=myserver1:2181,myserver2:20000,myserver3:31111</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12586">HBASE-12586</a> | <em>Major</em> | <strong>Task 6 &amp; 7 from HBASE-9117, delete all public HTable constructors and delete ConnectionManager#{delete,get}Connection</strong></li>
</ul>
<p>HTable class has been marked as private API before, and now it's no longer directly instantiable from client code (all public constructors have been removed). All clients should use Connection#getTable() and Connection#getRegionLocator() when appropriate to obtain Table and RegionLocator implementations to work with.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13198">HBASE-13198</a> | <em>Major</em> | <strong>Remove HConnectionManager</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12867">HBASE-12867</a> | <em>Major</em> | <strong>Shell does not support custom replication endpoint specification</strong></li>
</ul>
<p>Adds support to add_peer in hbase shell to add a custom replication endpoint from HBASE-12254.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13273">HBASE-13273</a> | <em>Major</em> | <strong>Make Result.EMPTY_RESULT read-only; currently it can be modified</strong></li>
</ul>
<p>The Result.EMPTY_RESULT object is now immutable. In previous releases, the object could be modified by a caller to no longer be empty. Code that relies on this behavior will now receive an UnsupportedOperationException.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13331">HBASE-13331</a> | <em>Blocker</em> | <strong>Exceptions from DFS client can cause CatalogJanitor to delete referenced files</strong></li>
</ul>
<p>Fixes an issue where files from a split region that were still referenced were erroneously deleted leading to data loss.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13248">HBASE-13248</a> | <em>Major</em> | <strong>Make HConnectionImplementation top-level class.</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13298">HBASE-13298</a> | <em>Critical</em> | <strong>Clarify if Table.{set|get}WriteBufferSize() is deprecated or not</strong></li>
</ul>
<p>Deprecate said methods. They were mistakenly included in Table Interface.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13316">HBASE-13316</a> | <em>Minor</em> | <strong>Reduce the downtime on planned moves of regions</strong></li>
</ul>
<p>When issuing an Admin.move command the RegionServer that receive the region will try and open the StoreFiles of that region to prime the block cache with index blocks.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13170">HBASE-13170</a> | <em>Major</em> | <strong>Allow block cache to be external</strong></li>
</ul>
<p>HBase can use memcached as an external block cache. To use this change your config to set hbase.blockcache.use.external to true and hbase.cache.memcached.servers to contain the list of memcached servers to use.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-10728">HBASE-10728</a> | <em>Major</em> | <strong>get_counter value is never used.</strong></li>
</ul>
<p>for 0.98 and 1.0 changes are compatible (due to mitigation by HBASE-13433):</p>
<p>* The &quot;get_counter&quot; command no longer requires a dummy 4th argument. Downstream users are encouraged to migrate code to not pass this argument because it will result in an error for HBase 1.1+. * The &quot;incr&quot; command now outputs the current value of the counter to stdout. ex: {code} jruby-1.6.8 :005 &gt; incr 'counter_example', 'r1', 'cf1:foo', 10 COUNTER VALUE = 1772 0 row(s) in 0.1180 seconds {code}</p>
<p>for 1.1+ changes are incompatible:</p>
<p>* The &quot;get_counter&quot; command no longer accepts a dummy 4th argument. Downstream users will need to update their code to not pass this argument. ex: {code} jruby-1.6.8 :006 &gt; get_counter 'counter_example', 'r1', 'cf1:foo' COUNTER VALUE = 1772</p>
<p>{code} * The &quot;incr&quot; command now outputs the current value of the counter to stdout. ex: {code} jruby-1.6.8 :005 &gt; incr 'counter_example', 'r1', 'cf1:foo', 10 COUNTER VALUE = 1772 0 row(s) in 0.1180 seconds {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13361">HBASE-13361</a> | <em>Minor</em> | <strong>Remove or undeprecate {get|set}ScannerCaching in HTable</strong></li>
</ul>
<p>Removed getScannerCaching and setScannerCaching from Table</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13187">HBASE-13187</a> | <em>Critical</em> | <strong>Add ITBLL that exercises per CF flush</strong></li>
</ul>
<p>Pass the -D flag generator.multiple.columnfamilies on the command-line if you want the generator to write three column families rather than the default one. When set, we will write the usual 'meta' column family and use it checking linked-list is wholesome but we will also write a 'tiny' column family and a 'big' column family to provoke uneven flushing; good for testing the flush-by-columnfamily feature.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12954">HBASE-12954</a> | <em>Minor</em> | <strong>Ability impaired using HBase on multihomed hosts</strong></li>
</ul>
<p>The following config is added by this JIRA:</p>
<p>hbase.regionserver.hostname</p>
<p>This config is for experts: don't set its value unless you really know what you are doing. When set to a non-empty value, this represents the (external facing) hostname for the underlying server. See https://issues.apache.org/jira/browse/HBASE-12954 for details.</p>
<p>Caution: please make sure rolling upgrade succeeds before turning on this feature.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13252">HBASE-13252</a> | <em>Major</em> | <strong>Get rid of managed connections and connection caching</strong></li>
</ul>
<p>For a long time, HBase supported 2 types of connections - managed, which were cached and closed automatically when not needed, and unmanaged, where user is responsible for closing the connections by calling #close() on them.</p>
<p>The concept of managed connections in HBase (deprecated before) has now been extinguished completely, and now all callers are responsible for managing the lifecycle of connections they acquire.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13373">HBASE-13373</a> | <em>Major</em> | <strong>Squash HFileReaderV3 together with HFileReaderV2 and AbstractHFileReader; ditto for Scanners and BlockReader, etc.</strong></li>
</ul>
<p>Marking as incompatible change. Requires hfiles be major version &gt;= 2 and &gt;= minor version 3. Version 3 files are enabled by default in 1.0. 0.98 writes version 2 minor version 3. You cannot go to 1.0 from anything before 0.98.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13289">HBASE-13289</a> | <em>Major</em> | <strong>typo in splitSuccessCount metric</strong></li>
</ul>
<p>In hbase 1.0.0, 0.98.10, 0.98.10.1, 0.98.11, and 0.98.12 'splitSuccessCount' was misspelled as 'splitSuccessCounnt'</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11864">HBASE-11864</a> | <em>Minor</em> | <strong>Enhance HLogPrettyPrinter to print information from WAL Header</strong></li>
</ul>
<p>Enhance WALPrettyPrinter to print information (writer classnames and cell codec classname) from WAL Header</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11544">HBASE-11544</a> | <em>Critical</em> | <strong>[Ergonomics] hbase.client.scanner.caching is dogged and will try to return batch even if it means OOME</strong></li>
</ul>
<p>Results returned from RPC calls may now be returned as partials</p>
<p>When is a Result marked as a partial? When the server must stop the scan because the max size limit has been reached. Means that the LAST Result returned within the ScanResult's Result array may be marked as a partial if the scan's max size limit caused it to stop in the middle of a row.</p>
<p>Incompatible Change: The return type of InternalScanners#next and RegionScanners#nextRaw has been changed to NextState from boolean The previous boolean return value can be accessed via NextState#hasMoreValues() Provides more context as to what happened inside the scanner</p>
<p>Scan caching default has been changed to Integer.Max_Value This value works together with the new maxResultSize value from HBASE-12976 (defaults to 2MB) Results returned from server on basis of size rather than number of rows Provides better use of network since row size varies amongst tables</p>
<p>Protobuf models have changed for Result, ScanRequest, and ScanResponse to support new partial Results</p>
<p>Partial Results should be invisible to application layer unless Scan#setAllowPartials is set</p>
<p>Scan#setAllowPartials has been added to allow the application to request to see the partial Results returned by the server rather than have the ClientScanner form the complete Result prior to returning it to the application</p>
<p>To disable the use of partial Results on the server, set ScanRequest.Builder#setClientHandlesPartials() to be false in the ScanRequest issued to server</p>
<p>Partial Results should allow the server to return large rows in parts rather than accumulate all the cells for that particular row and run out of memory</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13362">HBASE-13362</a> | <em>Major</em> | <strong>Set max result size from client only (like scanner caching).</strong></li>
</ul>
<p>This introduces a new config option: hbase.server.scanner.max.result.size This setting enforces a maximum result size (in bytes), when reached the server will return the results is has so far. This is a safety setting and should be kept large. The default is inifinite in 0.98 and 1.0.x and 100mb in 1.1 and later.</p>
<p>Use hbase.client.scanner.max.result.size instead to enforce practical chunk sizes of a few mb (defaults to 2mb)</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13270">HBASE-13270</a> | <em>Major</em> | <strong>Setter for Result#getStats is #addResults; confusing!</strong></li>
</ul>
<p>Deprecates Result#addResults in favor of Result#setStatistics</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13118">HBASE-13118</a> | <em>Major</em> | <strong>[PE] Add being able to write many columns</strong></li>
</ul>
<p>Adds a --columns option to PE so you can write more than one column (changes default qualifier from 'data' to '0').</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13275">HBASE-13275</a> | <em>Major</em> | <strong>Setting hbase.security.authorization to false does not disable authorization</strong></li>
</ul>
<p>Prior to this change the configuration setting 'hbase.security.authorization' had no effect if security coprocessor were installed. The act of installing the security coprocessors was assumed to indicate active authorizaton was desired and required. Now it is possible to install the security coprocessors yet have them operate in a passive state with active authorization disabled by setting 'hbase.security.authorization' to false. This can be useful but is probably not what you want. For more information, consult the Security section of the HBase online manual.</p>
<p>'hbase.security.authorization' defaults to true for backwards comptatible behavior.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13419">HBASE-13419</a> | <em>Major</em> | <strong>Thrift gateway should propagate text from exception causes.</strong></li>
</ul>
<p>Compose thrift exception text from the text of the entire cause chain of the underlying exception.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13453">HBASE-13453</a> | <em>Critical</em> | <strong>Master should not bind to region server ports</strong></li>
</ul>
<p>In 1.0.x, master by default binds to the region server ports (both rpc and info). This change brings back the usage of old master rpc and info ports in 1.1+ and master (2.0) branches. The motivation for this change is to ease the life of the user so that he does not need to do anything to bring up a RS on the same host and also to make the migration from 0.98 to 1.1 hassle free. However, the users going from 1.0 to 1.1 would see the change in the master ports.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13307">HBASE-13307</a> | <em>Major</em> | <strong>Making methods under ScannerV2#next inlineable, faster</strong></li>
</ul>
<p>Made methods smaller under Scanner#next so inlinable and compilable (was getting 'too big to compile' from hotspot). Use of unsafe to parse shorts rather than use BB#getShort... faster, etc.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13090">HBASE-13090</a> | <em>Major</em> | <strong>Progress heartbeats for long running scanners</strong></li>
</ul>
<p>Previously, there was no way to enforce a time limit on scan RPC requests. The server would receive a scan RPC request and take as much time as it needed to accumulate enough results to reach a limit or exhaust the region. The problem with this approach was that, in the case of a very selective scan, the processing of the scan could take too long and cause timeouts client side.</p>
<p>With this fix, the server will now enforce a time limit on the execution of scan RPC requests. When a scan RPC request arrives to the server, a time limit is calculated to be half of whichever timeout value is more restictive between the configurations (&quot;hbase.client.scanner.timeout.period&quot; and &quot;hbase.rpc.timeout&quot;). When the time limit is reached, the server will return whatever results it has accumulated up to that point. The results may be empty.</p>
<p>To ensure that timeout checks do not occur too often (which would hurt the performance of scans), the configuration &quot;hbase.cells.scanned.per.heartbeat.check&quot; has been introduced. This configuration controls how often System.currentTimeMillis() is called to update the progress towards the time limit. Currently, the default value of this configuration value is 10000. Specifying a smaller value will provide a tighter bound on the time limit, but may hurt scan performance due to the higher frequency of calls to System.currentTimeMillis().</p>
<p>Protobuf models for ScanRequest and ScanResponse have been updated so that heartbeat support can be communicated. Support for heartbeat messages is specified in the request sent to the server via ScanRequest.Builder#setClientHandlesHeartbeats. Only when the server sees that ScanRequest#getClientHandlesHeartbeats() is true will it send heartbeat messages back to the client. A response is marked as a heartbeat message via the boolean flag ScanResponse#getHeartbeatMessage</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13481">HBASE-13481</a> | <em>Major</em> | <strong>Master should respect master (old) DNS/bind related configurations</strong></li>
</ul>
<p>Master now honors configuration options as was before 1.0.0 releases: hbase.master.ipc.address hbase.master.dns.interface hbase.master.dns.nameserver hbase.master.info.bindAddress This jira also adds hbase.master.hostname parameter as an extension to HBASE-12954.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13149">HBASE-13149</a> | <em>Blocker</em> | <strong>HBase MR is broken on Hadoop 2.5+ Yarn</strong></li>
</ul>
<p>In HBase 1.1.0 and above we have upgraded the version of Jackson dependencies (jackson-core-asl, jackson-mapper-asl, jackson-jaxrs and jackson-xc) from 1.8.8 to 1.9.13. This is to follow the upgrade to Jackson 1.9.13 in Hadoop 2.5 and above which causes Jackson class incompatibility for HBase as reported in HBASE-13149. Refer to HADOOP-10104 and YARN-2092 for additional information. Jackson1.9.13 is not completely backward compatible with the prior version 1.8.8 used in HBase. See the Compatibility reports attached in HBASE-13149 and http://svn.codehaus.org/jackson/trunk/release-notes/VERSION for more information.</p>
<p>This upgrade does not have direct impact on HBase users and HBase applications in most cases. In the rare case where your HBase application uses Jackson directly AND your application has compatibility issue with Jackson 1.9.13, you can do the following to mitigate the problem.</p>
<ol>
<li>If you are on Hadoop 2.5 or above, and your HBase application involves running Yarn jobs, we recommend you update your application to use Jackson 1.9.13. You may be able to explore classpath isolation options (e.g. HADOOP-10893) or have your own classpath isolation strategy that works for you, but the general recommendation is that you upgrade to Jackson 1.9.13.</li>
<li>You may choose to continue using Jackson 1.8.8 and not to use Jackson 1.9.13 in your classpath. You can also choose to replace the Jackson 1.9.13 jars in $HBASE_HOME/lib with 1.8.8 jars. It can work for you in the following cases:</li>
</ol>
<p>a) You are on a Hadoop version earlier than Hadoop 2.5, or b) You are on Hadoop 2.5 or above, but your HBase application does not involve running Yarn jobs. 3. You may experiment with further isolation using the shaded jars introduced with 1.1.0 via HBASE-13517.</p>
<p>Note that it may not be tested or guaranteed that using Jackson 1.8.8 in $HBASE_HOME/lib will work in future HBase releases. It is recommended that your HBase application matches the Jackson version provided in HBase.</p>
<p>In HBase 0.98.x and HBase 1.0.x, we have NOT upgraded the version of Jackson dependencies. If you are on Hadoop 2.5 or above, and your HBase application involves running Yarn jobs, you may encounter Jackson class incomparability issue, as reported in HBASE-13149.</p>
<p>You can do the following to mitigate the problem: 1. Use 'hadoop jar' command to run your HBase jobs. 2. Explore classpath isolation options (e.g. HADOOP-10893) or have your own classpath isolation strategy that works for you. 3. You can also choose to replace the Jackson 1.8.8 jars in $HBASE_HOME/lib with 1.9.13 jars from your Hadoop lib directory. We have tested HBase 0.98 with Jackson 1.9.13.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13517">HBASE-13517</a> | <em>Major</em> | <strong>Publish a client artifact with shaded dependencies</strong></li>
</ul>
<p>HBase now provides added convenience artifacts that shade most dependencies. These jars hbase-shaded-client and hbase-shaded-server are meant to be used when dependency conflicts can not be solved any other way. The normal jars hbase-client and hbase-server should still be preferred when possible.</p>
<p>Do not use hbase-shaded-server or hbase-shaded-client inside of a co-processor as bad things will happen.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13537">HBASE-13537</a> | <em>Major</em> | <strong>Procedure V2 - Change the admin interface for async operations to return Future (incompatible with branch-1.x)</strong></li>
</ul>
<p>As we made changes to return types in asynchronous methods of Admin API, this change is going to break binary compatibility. The source compatibility is kept intact though. The applications running against this change needs to be recompiled to keep things working.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13564">HBASE-13564</a> | <em>Major</em> | <strong>Master MBeans are not published</strong></li>
</ul>
<p>To use the coprocessor-based JMX implementation provided by HBase for Master. Add below property in hbase-site.xml file:</p>
<p>&lt;property&gt; &lt;name&gt;hbase.coprocessor.master.classes&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.JMXListener&lt;/value&gt; &lt;/property&gt;</p>
<p>NOTE: DO NOT set <code>com.sun.management.jmxremote.port</code> for Java VM at the same time.</p>
<p>By default, the JMX listens on TCP port 10101 for Master, we can further configure the port using below properties:</p>
<p>&lt;property&gt; &lt;name&gt;master.rmi.registry.port&lt;/name&gt; &lt;value&gt;61110&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;master.rmi.connector.port&lt;/name&gt; &lt;value&gt;61120&lt;/value&gt; &lt;/property&gt; ----</p>
<p>The registry port can be shared with connector port in most cases, so you only need to configure master.rmi.registry.port. However if you want to use SSL communication, the 2 ports must be configured to different values.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-10800">HBASE-10800</a> | <em>Major</em> | <strong>Use CellComparator instead of KVComparator</strong></li>
</ul>
<p>From 2.0 branch onwards KVComparator and its subclasses MetaComparator, RawBytesComparator are all deprecated. All the comparators are moved to CellComparator. MetaCellComparator, a subclass of CellComparator, will be used to compare hbase:meta cells.<br />
Previously exposed static instances KeyValue.COMPARATOR, KeyValue.META_COMPARATOR and KeyValue.RAW_COMPARATOR are deprecated instead use CellComparator.COMPARATOR and CellComparator.META_COMPARATOR. Also note that there will be no RawBytesComparator. Where ever we need to compare raw bytes use Bytes.BYTES_RAWCOMPARATOR. CellComparator will always operate on cells and its components, abstracting the fact that a cell can be backed by a single byte[] as opposed to how KVComparators were working.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13625">HBASE-13625</a> | <em>Major</em> | <strong>Use HDFS for HFileOutputFormat2 partitioner's path</strong></li>
</ul>
<p>Introduces a new config hbase.fs.tmp.dir which is a directory in HDFS (or default file system) to use as a staging directory for HFileOutputFormat2. This is also used as the default for hbase.bulkload.staging.dir</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13533">HBASE-13533</a> | <em>Trivial</em> | <strong>section on configuring ~/.m2/settings.xml has no anchor</strong></li>
</ul>
<p>Correct setting.xml anchor in book</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13071">HBASE-13071</a> | <em>Major</em> | <strong>Hbase Streaming Scan Feature</strong></li>
</ul>
<p>MOTIVATION</p>
<p>A pipelined scan API is introduced for speeding up applications that combine massive data traversal with compute-intensive processing. Traditional HBase scans save network trips through prefetching the data to the client side cache. However, they prefetch synchronously: the fetch request to regionserver is invoked only when the entire cache is consumed. This leads to a stop-and-wait access pattern, in which the client stalls until the next chunk of data is fetched. Applications that do significant processing can benefit from background data prefetching, which eliminates this bottleneck. The pipelined scan implementation overlaps the cache population at the client side with application processing. Namely, it issues a new scan RPC when the iteration retrieves 50% of the cache. If the application processing (that is, the time between invocations of next()) is substantial, the new chunk of data will be available before the previous one is exhausted, and the client will not experience any delay. Ideally, the prefetch and the processing times should be balanced.</p>
<p>API AND CONFIGURATION</p>
<p>Asynchronous scanning can be configured either globally for all tables and scans, or on per-scan basis via a new Scan class API.</p>
<p>Configuration in hbase-site.xml: hbase.client.scanner.async.prefetch, default false:</p>
<p>&lt;property&gt; &lt;name&gt;hbase.client.scanner.async.prefetch&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt;</p>
<p>API - Scan#setAsyncPrefetch(boolean)</p>
<pre><code>  Scan scan = new Scan();
  scan.setCaching(1000);
  scan.setMaxResultSize(BIG\_SIZE);
  scan.setAsyncPrefetch(true);
    ...
  ResultScanner scanner = table.getScanner(scan);</code></pre>
<p>IMPLEMENTATION NOTES</p>
<p>Pipelined scan is implemented by a new ClientAsyncPrefetchScanner class, which is fully API-compatible with the synchronous ClientSimpleScanner. ClientAsyncPrefetchScanner is not instantiated in case of small (Scan#setSmall) and reversed (Scan#setReversed) scanners. The application is responsible for setting the prefetch size in a way that the prefetch time and the processing times are balanced. Note that due to double buffering, the client side cache can use twice as much memory as the synchronous scanner.</p>
<p>Generally, this feature will put more load on the server (higher fetch rate -- which is the whole point). Also, YMMV.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13636">HBASE-13636</a> | <em>Major</em> | <strong>Remove deprecation for HBASE-4072 (Reading of zoo.cfg)</strong></li>
</ul>
<p>Purge support for parsing zookeepers zoo.cfg deprecated since hbase-0.96.0</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13698">HBASE-13698</a> | <em>Major</em> | <strong>Add RegionLocator methods to Thrift2 proxy.</strong></li>
</ul>
<p>Added getRegionLocation and getAllRegionLocations to the thrift2 interface.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-5980">HBASE-5980</a> | <em>Minor</em> | <strong>Scanner responses from RS should include metrics on rows/KVs filtered</strong></li>
</ul>
<p>Adds scan metrics to the result. In the shell, set the ALL_METRICS attribute to true on your scan to see dump of metrics after results (see the scan help for examples).</p>
<p>If you would prefer to see only a subset of the metrics, the METRICS array can be defined to include the names of only the metrics you care about.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13375">HBASE-13375</a> | <em>Major</em> | <strong>Provide HBase superuser higher priority over other users in the RPC handling</strong></li>
</ul>
<p>This JIRA modifies the signature of PriorityFunction#getPriority() method to also take request user as a parameter; all RPC requests sent by super users (as determined by cluster configuration) are executed with Admin QoS.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13754">HBASE-13754</a> | <em>Major</em> | <strong>Allow non KeyValue Cell types also to oswrite</strong></li>
</ul>
<p>This jira has removed the already deprecated method KeyValue#oswrite(final KeyValue kv, final OutputStream out)</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13816">HBASE-13816</a> | <em>Major</em> | <strong>Build shaded modules only in release profile</strong></li>
</ul>
<p>hbase-shaded-client and hbase-shaded-server modules will not build the actual jars unless -Prelease is supplied in mvn.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13686">HBASE-13686</a> | <em>Major</em> | <strong>Fail to limit rate in RateLimiter</strong></li>
</ul>
<p>As per this jira contribution. We now support two kinds of RateLimiter. 1) org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter : This limiter will refill resources at every TimeUnit/resources interval. Example: For a limiter configured with 10resources/second, then 1resource will be refilled after every 100ms.</p>
<p>2) org.apache.hadoop.hbase.quotas.FixedIntervalRateLimiter: This limiter will refill resources only after a given fixed interval of time.</p>
<p>Client can configure anyone of this rate limiter for the cluster by setting the value for the property &quot;hbase.quota.rate.limiter&quot; in the hbase-site.xml. org.apache.hadoop.hbase.quotas.AverageIntervalRateLimiter is the default value. Note: Client needs to restart the cluster for the configuration to take into effect.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13843">HBASE-13843</a> | <em>Trivial</em> | <strong>Fix internal constant text in ReplicationManager.java</strong></li>
</ul>
<p>In previous versions of HBase, the ReplicationAdmin utility erroneously used the string key &quot;columnFamlyName&quot; when listing replicated column families. It now uses the corrected spelling of &quot;columnFamilyName&quot; (note the added &quot;i&quot;).</p>
<p>Downstream code that parsed the replication entries returned from listReplicated will need to be updated to use the new key. Previously compiled code that relied on the static CFNAME member of ReplicationAdmin will need to be recompiled in order to see the updated value.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13900">HBASE-13900</a> | <em>Minor</em> | <strong>duplicate methods between ProtobufMagic and ProtobufUtil</strong></li>
</ul>
<p>Use ProtobufMagic methods in ProtobufUtil</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13103">HBASE-13103</a> | <em>Major</em> | <strong>[ergonomics] add region size balancing as a feature of master</strong></li>
</ul>
<p>This patch adds optional ability for HMaster to normalize regions in size (disabled by default, change hbase.normalizer.enabled property to true to turn it on). If enabled, HMaster periodically (every 30 minutes by default) monitors tables for which normalization is enabled in table configuration and performs splits/merges as seems appropriate. Users may implement their own normalization strategies by implementing RegionNormalizer interface and configuring it in hbase-site.xml.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13898">HBASE-13898</a> | <em>Minor</em> | <strong>correct additional javadoc failures under java 8</strong></li>
</ul>
<p>Correct Javadoc generation errors</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13938">HBASE-13938</a> | <em>Major</em> | <strong>Deletes done during the region merge transaction may get eclipsed</strong></li>
</ul>
<p>Use the master's timestamp when sending hbase:meta edits on region merge to ensure proper ordering of new region addition and old region deletes.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13869">HBASE-13869</a> | <em>Trivial</em> | <strong>Fix typo in HBase book</strong></li>
</ul>
<p>Fix typo in HBase book</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13214">HBASE-13214</a> | <em>Major</em> | <strong>Remove deprecated and unused methods from HTable class</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13930">HBASE-13930</a> | <em>Major</em> | <strong>Exclude Findbugs packages from shaded jars</strong></li>
</ul>
<p>Exclude Findbugs packages from shaded jars</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13959">HBASE-13959</a> | <em>Critical</em> | <strong>Region splitting uses a single thread in most common cases</strong></li>
</ul>
<p>The performance of region splitting has been improved by using a thread pool to split the store files concurrently. Prior to this change, the store files were always split sequentially in a single thread, so a region with multiple store files ended up taking several seconds. The thread pool is sized dynamically with the aim of getting maximum concurrency, without exceeding the number of cores available for HBase Java process. A lower limit for the thread pool can be explicitly set using the property hbase.regionserver.region.split.threads.max.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13747">HBASE-13747</a> | <em>Critical</em> | <strong>Promote Java 8 to &quot;yes&quot; in support matrix</strong></li>
</ul>
<p>Java 8 is considered supported and tested as of HBase 1.2+</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13983">HBASE-13983</a> | <em>Minor</em> | <strong>Doc how the oddball HTable methods getStartKey, getEndKey, etc. will be removed in 2.0.0</strong></li>
</ul>
<p>Adds extra doc on getStartKeys, getEndKeys, and getStartEndKeys in HTable explaining that they will be removed in 2.0.0 (these methods did not get the proper full major version deprecation cycle).</p>
<p>In this issue, we actually also remove these methods in master/2.0.0 branch.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13895">HBASE-13895</a> | <em>Critical</em> | <strong>DATALOSS: Region assigned before WAL replay when abort</strong></li>
</ul>
<p>If the master went to assign a region concurrent with a RegionServer abort, the returned RegionServerAbortedException was being handled as though the region had been cleanly offlined so assign was allowed proceed. If the region was opened in its new location before WAL replay completion, the replayed edits were ignored, worst case, or were later played over the top of edits that had come in since open and so susceptible to overwrite. In either case, DATALOSS.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13639">HBASE-13639</a> | <em>Major</em> | <strong>SyncTable - rsync for HBase tables</strong></li>
</ul>
<p>Tool to sync two tables that tries to send the differences only like rsync.</p>
<p>Adds two new MapReduce jobs, SyncTable and HashTable. See usage for these jobs on how to use. See design doc for generally overview: https://docs.google.com/document/d/1-2c9kJEWNrXf5V4q_wBcoIXfdchN7Pxvxv1IO6PW0-U/edit</p>
<p>From comments below, &quot;It can be challenging to run against a table getting live writes, if those writes are updates/overwrites. In general, you can run it against a time range to ignore new writes, but if those writes update existing cells, then the time range scan may or may not see older versions of those cells depending on whether major compaction has happened, which may be different in remote clusters.&quot;</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13646">HBASE-13646</a> | <em>Major</em> | <strong>HRegion#execService should not try to build incomplete messages</strong></li>
</ul>
<p>When RegionServerCoprocessors throw an exception we will no longer attempt to build an incomplete RPC response message. Instead, the response message will be null.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13849">HBASE-13849</a> | <em>Major</em> | <strong>Remove restore and clone snapshot from the WebUI</strong></li>
</ul>
<p>The HBase master status web page no longer allows operators to clone snapshots nor restore snapshots.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14029">HBASE-14029</a> | <em>Major</em> | <strong>getting started for standalone still references hadoop-version-specific binary artifacts</strong></li>
</ul>
<p>HBASE-14029 Correct documentation for Hadoop version specific artifacts</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14047">HBASE-14047</a> | <em>Major</em> | <strong>Cleanup deprecated APIs from Cell class</strong></li>
</ul>
<p>The following API from Cell (which were deprecated since past few major versions) are removed now. getRow getFamily getQualifier getValue getMvccVersion The above apis can be replaced with their respective CellUtil#cloneXXX (allocates a copy) or Cell#getXXXArray (essentially just returns a pointer) based on the use case.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-7782">HBASE-7782</a> | <em>Minor</em> | <strong>HBaseTestingUtility.truncateTable() not acting like CLI</strong></li>
</ul>
<p>HBaseTestingUtility now uses the truncate API added in HBASE-8332 so that calls to HBTU.truncateTable will behave like the shell command: effectively dropping the table and recreating a new one with the same split points.</p>
<p>Previously, HBTU.truncateTable instead issued deletes for all the data already in the table. If you wish to maintain the same behavior, you should use the newly added HBTU.deleteTableData method.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14027">HBASE-14027</a> | <em>Major</em> | <strong>Clean up netty dependencies</strong></li>
</ul>
<p>HBase's convenience binary artifact no longer contains the netty 3.2.4 jar . This jar was not directly used by HBase, but may have been relied on by downstream applications.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14045">HBASE-14045</a> | <em>Major</em> | <strong>Bumping thrift version to 0.9.2.</strong></li>
</ul>
<p>This changes upgrades thrift dependency of HBase to 0.9.2. Though this doesn't break any HBase compatibility promises, it might impact any downstream projects that share thrift dependency with HBase.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13954">HBASE-13954</a> | <em>Major</em> | <strong>Remove HTableInterface#getRowOrBefore related server side code</strong></li>
</ul>
<p>Removed Table#getRowOrBefore, Region#getClosestRowBefore, Store#getRowKeyAtOrBefore, RemoteHTable#getRowOrBefore apis and Thrift support for getRowOrBefore. Also removed two coprocessor hooks preGetClosestRowBefore and postGetClosestRowBefore. User using this api can instead use reverse scan something like below, {code} Scan scan = new Scan(row); scan.setSmall(true); scan.setCaching(1); scan.setReversed(true); scan.addFamily(family); {code} pass this scan object to the scanner and retrieve the first Result from scanner output.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11339">HBASE-11339</a> | <em>Major</em> | <strong>HBase MOB</strong></li>
</ul>
<p>The Moderate Object Storage (MOB) feature (HBASE-11339[1]) is modified I/O and compaction path that allows individual moderately sized values (100KB-10MB) to be stored in a way that write amplification is reduced when compared to the normal I/O path. MOB is defined in the column family and it is almost isolated with other components, the features and performance cannot be effected in normal columns.</p>
<p>For more details on how to use the feature please consult the HBase Reference Guide</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12295">HBASE-12295</a> | <em>Major</em> | <strong>Prevent block eviction under us if reads are in progress from the BBs</strong></li>
</ul>
<p>We try to delay the eviction of the block till the cellblocks are formed at the Rpc layer. A simple reference counting mechanism is introduced when ever a block is accessed from the Bucket cache. Once a scanner completes using a block the reference count is decremented. The eviction of the block happens only when the reference count of that block is 0. We also introduce a concept of ShareableMemory based on the type of blocks we create from the Block cache. The blocks from the ByteBufferIOEngine directly refer to the buckets in offheap and such blocks are marked SHARED memory type. The blocks from LRU, HDFS and file mode of Bucket cache are all marked EXCLUSIVE because these blocks have their own exclusive memory. For the CP case, any cell coming out of SHARED memory block is copied before returning the results, because CPs can use the results as its state so that eviction cannot corrupt the results.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13992">HBASE-13992</a> | <em>Major</em> | <strong>Integrate SparkOnHBase into HBase</strong></li>
</ul>
<p>This release includes initial support for running Spark against HBase with a richer feature set than was previously possible with MapReduce bindings:</p>
<p>* Support for Spark and Spark Streaming against Spark 1.3 * RDD/DStream formation from scan operations * convenience methods for interacting with HBase from an HBase backed RDD / DStream instance * examples in both the Spark Java API and Spark Scala API * support for running against a secure HBase cluster</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13865">HBASE-13865</a> | <em>Trivial</em> | <strong>Increase the default value for hbase.hregion.memstore.block.multipler from 2 to 4 (part 2)</strong></li>
</ul>
<p>Increase default hbase.hregion.memstore.block.multiplier from 2 to 4 in the code to match the default value in the config files.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14081">HBASE-14081</a> | <em>Minor</em> | <strong>(outdated) references to SVN/trunk in documentation</strong></li>
</ul>
<p>HBASE-14081 Remove (outdated) references to SVN/trunk from documentation</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14201">HBASE-14201</a> | <em>Major</em> | <strong>hbck should not take a lock unless fixing errors</strong></li>
</ul>
<p>HBCK no longer takes a lock until there are changes to the cluster being made.</p>
<p>The old behavior can be achieved by passing the -exclusive flag.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13985">HBASE-13985</a> | <em>Minor</em> | <strong>Add configuration to skip validating HFile format when bulk loading</strong></li>
</ul>
<p>A new config, hbase.loadincremental.validate.hfile , is introduced - default to true When set to false, checking hfile format is skipped during bulkloading.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14054">HBASE-14054</a> | <em>Major</em> | <strong>Acknowledged writes may get lost if regionserver clock is set backwards</strong></li>
</ul>
<p>In {{checkAndPut}} write path use max(max timestamp for the row, System.currentTimeMillis()) in the, instead of blindly taking System.currentTimeMillis() to ensure that checkAndPut() cannot do writes which is already eclipsed. This is similar to what has been done in HBASE-12449 for increment and append.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13706">HBASE-13706</a> | <em>Minor</em> | <strong>CoprocessorClassLoader should not exempt Hive classes</strong></li>
</ul>
<p>Starting from HBase 2.0, CoprocessorClassLoader will not exempt hadoop classes or zookeeper classes. This means that if the custom coprocessor jar contains hadoop or zookeeper packages and classes, they will be loaded by the CoprocessorClassLoader. Only hbase packages and classes are exempted from the CoprocessorClassLoader. They (and their dependencies) are loaded by the parent server class loader.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13966">HBASE-13966</a> | <em>Minor</em> | <strong>Limit column width in table.jsp</strong></li>
</ul>
<p>Wraps region, start key, end key columns if too long.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-10844">HBASE-10844</a> | <em>Major</em> | <strong>Coprocessor failure during batchmutation leaves the memstore datastructs in an inconsistent state</strong></li>
</ul>
<p>Promotes an -ea assert to logged FATAL and RS abort when memstore is found to be in an inconsistent state.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14148">HBASE-14148</a> | <em>Major</em> | <strong>Web UI Framable Page</strong></li>
</ul>
<p>Security fix: Adds protection from clickjacking using X-Frame-Options header. This will prevent use of HBase UI in frames. To disable this feature, set the configuration 'hbase.http.filter.xframeoptions.mode' to 'ALLOW' (default is 'DENY').</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13127">HBASE-13127</a> | <em>Major</em> | <strong>Add timeouts on all tests so less zombie sightings</strong></li>
</ul>
<p>Use junit facility to impose timeout on test. Use test category to chose which timeout to apply: small tests timeout after 30 seconds, medium tests after 180 seconds, and large tests after ten minutes.</p>
<p>Updated junit version from 4.11 to 4.12. 4.12 has support for feature used here.</p>
<p>Add this at the head of your junit4 class to add a category-based timeout:</p>
<p>{code} @Rule public final TestRule timeout = CategoryBasedTimeout.builder().withTimeout(this.getClass()). withLookingForStuckThread(true).build(); {code}</p>
<p>For example:</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14224">HBASE-14224</a> | <em>Critical</em> | <strong>Fix coprocessor handling of duplicate classes</strong></li>
</ul>
<p>Prevent Coprocessors being doubly-loaded; a particular coprocessor can only be loaded once.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13339">HBASE-13339</a> | <em>Blocker</em> | <strong>Update default Hadoop version to latest for master</strong></li>
</ul>
<p>Master/2.0.0 now builds on the latest stable hadoop by default.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14313">HBASE-14313</a> | <em>Critical</em> | <strong>After a Connection sees ConnectionClosingException it never recovers</strong></li>
</ul>
<p>HConnection could get stuck when talking to a host that went down and then returned. This has been fixed by closing the connection in all paths.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14309">HBASE-14309</a> | <em>Major</em> | <strong>Allow load balancer to operate when there is region in transition by adding force flag</strong></li>
</ul>
<p>This issue adds boolean parameter, force, to 'balancer' command so that admin can force region balancing even when there is region (other than hbase:meta) in transition - assuming RIT being transient. If hbase:meta is in transition, balancer command returns false.</p>
<p>WARNING: For experts only. Forcing a balance may do more damage than repair when assignment is confused Note: enclose the force parameter in double quotes</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14261">HBASE-14261</a> | <em>Major</em> | <strong>Enhance Chaos Monkey framework by adding zookeeper and datanode fault injections.</strong></li>
</ul>
<p>This change augments existing chaos monkey framework with actions for restarting underlying zookeeper quorum and hdfs nodes of distributed hbase cluster. One assumption made while creating zk actions are that zookeper ensemble is an independent external service and won't be managed by hbase cluster. For these actions to work as expected, the following parameters need to be configured appropriately.</p>
<p>{code} &lt;property&gt; &lt;name&gt;hbase.it.clustermanager.hadoop.home&lt;/name&gt; &lt;value&gt;$HADOOP_HOME&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.it.clustermanager.zookeeper.home&lt;/name&gt; &lt;value&gt;$ZOOKEEPER_HOME&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.it.clustermanager.hbase.user&lt;/name&gt; &lt;value&gt;hbase&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.it.clustermanager.hadoop.hdfs.user&lt;/name&gt; &lt;value&gt;hdfs&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hbase.it.clustermanager.zookeeper.user&lt;/name&gt; &lt;value&gt;zookeeper&lt;/value&gt; &lt;/property&gt; {code}</p>
<p>The service user related configurations are newly introduced since in prod/test environments each service is managed by different user. Once the above parameters are configured properly, you can start using them as needed. An example usage for invoking these new actions is:</p>
<p>{{./hbase org.apache.hadoop.hbase.IntegrationTestAcidGuarantees -m serverAndDependenciesKilling}}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14317">HBASE-14317</a> | <em>Blocker</em> | <strong>Stuck FSHLog: bad disk (HDFS-8960) and can't roll WAL</strong></li>
</ul>
<p>Tighten up WAL-use semantic.</p>
<ol>
<li>If an append or a sync throws an exception, all subsequent attempts at using the log will also throw this same exception. The WAL is now a lame-duck until you roll it.</li>
<li>If a successful append, and then we fail to sync the append, this is a fatal exception. The container must abort to replay the WAL logs even though we have told the client that the appends failed.</li>
</ol>
<p>The above rules have been applied laxly up to this; it used to be possible to get a good sync to go in over the top of a failed append. This has been fixed in this patch.</p>
<p>Also fixed a hang in the WAL subsystem if a request to pause the write pipeline took on a failed sync. before the roll requests sync got scheduled.</p>
<p>TODO: Revisit our WAL system. HBASE-12751 helps rationalize our write pipeline. In particular, it manages sequenceid inside mvcc which should make it so we can purge mechanism that writes empty, unflushed appends just to get the next sequenceid... problematic when WAL goes lame-duck. Lets get it in. TODO: A successful append followed by a failed sync probably only needs us replace the WAL (if we have signalled the client that the appends failed). Bummer is that replicating, these last appends might make it to the sink cluster or get replayed during recovery. HBase should keep its own WAL length? Or sequenceid of last successful sync should be passed when doing recovery and replication?</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14314">HBASE-14314</a> | <em>Major</em> | <strong>Metrics for block cache should take region replicas into account</strong></li>
</ul>
<p>The following metrics for primary region replica are added:</p>
<p>blockCacheHitCountPrimary blockCacheMissCountPrimary blockCacheEvictionCountPrimary</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-6617">HBASE-6617</a> | <em>Major</em> | <strong>ReplicationSourceManager should be able to track multiple WAL paths</strong></li>
</ul>
<p>ReplicationSourceManager now could track multiple wal paths. Notice that although most changes are internal and all metrics names remain the same, signature of below methods in MetricsSource are changed:</p>
<ol>
<li>refreshAgeOfLastShippedOp now requires a String parameter which indicates the wal group id of the reporter</li>
<li>setAgeOfLastShippedOp also adds a String parameter for wal group id</li>
</ol>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14306">HBASE-14306</a> | <em>Major</em> | <strong>Refine RegionGroupingProvider: fix issues and make it more scalable</strong></li>
</ul>
<p>In HBASE-14306 we've changed default strategy of RegionGroupingProvider from &quot;identify&quot; to &quot;bounded&quot;, so it's required to explicitly set &quot;hbase.wal.regiongrouping.strategy&quot; to &quot;identify&quot; if user still wants to use one WAL per region</p>
<p>Please also notice that in the new framework there will be one WAL per group, and the region-group mapping is decided by RegionGroupingStrategy. Accordingly, we've removed BoundedRegionGroupingProvider and added BoundedRegionGroupingStrategy as a replacement. If you already have a customized class for hbase.wal.regiongrouping.strategy, please check the new logic and make updates if necessary.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14400">HBASE-14400</a> | <em>Critical</em> | <strong>Fix HBase RPC protection documentation</strong></li>
</ul>
<p>To use rpc protection in HBase, set the value of 'hbase.rpc.protection' to: 'authentication' : simple authentication using kerberos 'integrity' : authentication and integrity 'privacy' : authentication and confidentiality</p>
<p>Earlier, HBase reference guide erroneously mentioned in some places to set the value to 'auth-conf'. This patch fixes the guide and adds temporary support for erroneously recommended values.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14433">HBASE-14433</a> | <em>Major</em> | <strong>Set down the client executor core thread count from 256 in tests</strong></li>
</ul>
<p>Tests run with client executors that have core thread count of 4 and a keepalive of 3 seconds. They used to default to 256 core threads and 60 seconds for keepalive.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14334">HBASE-14334</a> | <em>Major</em> | <strong>Move Memcached block cache in to it's own optional module.</strong></li>
</ul>
<p>Move external block cache to it's own module. This will reduce dependencies for people who use hbase-server. Currently Memcached is the reference implementation for external block cache. External block caches allow HBase to take advantage of other more complex caches that can live longer than the HBase regionserver process and are not necessarily tied to a single computer life time. However external block caches add in extra operational overhead.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14280">HBASE-14280</a> | <em>Minor</em> | <strong>Bulk Upload from HA cluster to remote HA hbase cluster fails</strong></li>
</ul>
<p>Patch will effectively work with Hadoop version 2.6 or greater with a launch of &quot;internal.nameservices&quot;. There will be no change in versions older than 2.6.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14495">HBASE-14495</a> | <em>Major</em> | <strong>TestHRegion#testFlushCacheWhileScanning goes zombie</strong></li>
</ul>
<p>The WAL append was changed by HBASE-12751. Every append now sets a latch on an edit. The latch needs to be cleared or else the WAL will hang. The original failures in TestHRegion turned up 'holes' where we were failing to throw the latch if we skipped out early because we were interrupted. Other 'holes' were found where we had mocked up a WAL so the latch would just stay in place. Futher holes were found appending WAL markers... here we were skipping the mvcc completely for a few edits. A clean up of WALUtils made all markers take the same code paths.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14230">HBASE-14230</a> | <em>Minor</em> | <strong>replace reflection in FSHlog with HdfsDataOutputStream#getCurrentBlockReplication()</strong></li>
</ul>
<p>Remove calling getNumCurrentReplicas on HdfsDataOutputStream via reflection. getNumCurrentReplicas showed up in hadoop 1+ and hadoop 0.2x. In hadoop-2 it was deprecated.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14475">HBASE-14475</a> | <em>Major</em> | <strong>Region split requests are always audited with &quot;hbase&quot; user rather than request user</strong></li>
</ul>
<p>Region observer notifications w.r.t. split request are now audited with request user through proper scope of doAs() calls.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14367">HBASE-14367</a> | <em>Major</em> | <strong>Add normalization support to shell</strong></li>
</ul>
<p>This patch adds shell support for region normalizer (see HBASE-13103).</p>
<p>3 commands have been added to hbase shell 'tools' command group (modeled on how the balancer works):</p>
<ul>
<li>'normalizer_enabled' checks whether region normalizer is turned on</li>
<li>'normalizer_switch' allows user to turn normalizer on and off</li>
<li>'normalize' runs region normalizer if it's turned on.</li>
</ul>
<p>Also 'alter' command has been extended to allow user to enable/disable region normalization per table (disabled by default). Use it as</p>
<p>alter 'testtable', {NORMALIZATION_MODE =&gt; 'true'}</p>
<p>Here is the help for the normalize command:</p>
<p>{code} hbase(main):008:0&gt; help 'normalize' Trigger region normalizer for all tables which have NORMALIZATION_MODE flag set. Returns true if normalizer ran successfully, false otherwise. Note that this command has no effect if region normalizer is disabled (make sure it's turned on using 'normalizer_switch' command).</p>
<p>Examples:</p>
<p>hbase&gt; normalize {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14544">HBASE-14544</a> | <em>Major</em> | <strong>Allow HConnectionImpl to not refresh the dns on errors</strong></li>
</ul>
<p>By setting hbase.resolve.hostnames.on.failure to false you can reduce the number of dns name resolutions that a client will do. However if machines leave and come back with different ip's the changes will not be noticed by the clients. So only set hbase.resolve.hostnames.on.failure to false if your cluster dns is not changing while clients are connected.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14502">HBASE-14502</a> | <em>Major</em> | <strong>Purge use of jmock and remove as dependency</strong></li>
</ul>
<p>HBASE-14502 Purge use of jmock and remove as dependency</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14529">HBASE-14529</a> | <em>Major</em> | <strong>Respond to SIGHUP to reload config</strong></li>
</ul>
<p>HBase daemons can now be signaled to reload their config by sending SIGHUP to the java process. Not all config parameters can be reloaded.</p>
<p>In order for this new feature to work the hbase-daemon.sh script was changed to use disown rather than nohup. Functionally this shouldn't change anything but the processes will have a different parent when being run from a connected login shell.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12911">HBASE-12911</a> | <em>Major</em> | <strong>Client-side metrics</strong></li>
</ul>
<p>Introduces collection and reporting of various client-perceived metrics. Metrics are exposed via JMX under &quot;org.apache.hadoop.hbase.client.MetricsConnection&quot;. Metrics are scoped according to connection instance, so multiple connection objects (ie, to different clusters) will report their metrics separately. Metrics are disabled by default, must be enabled by configuring &quot;hbase.client.metrics.enable=true&quot;.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14517">HBASE-14517</a> | <em>Minor</em> | <strong>Show regionserver's version in master status page</strong></li>
</ul>
<p>Adds server version to the listing of regionservers on the master home page.</p>
<p>if a cluster where the versions deviate, at the bottom of the 'Version' column on the master home page listing of 'Region Servers', you will see a note in red that says something like: 'Total:10 9 nodes with inconsistent version'</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13819">HBASE-13819</a> | <em>Major</em> | <strong>Make RPC layer CellBlock buffer a DirectByteBuffer</strong></li>
</ul>
<p>For master branch(2.0 version), the BoundedByteBufferPool always create Direct (off heap) ByteBuffers and return that. For branch-1(1.3 version), byte default the buffers returned will be off heap. This can be changed to return on heap ByteBuffers by configuring 'hbase.ipc.server.reservoir.direct.buffer' to false.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14521">HBASE-14521</a> | <em>Major</em> | <strong>Unify the semantic of hbase.client.retries.number</strong></li>
</ul>
<p>After this change, hbase.client.reties.number universally means the number of retry which is one less than total tries number, for both non-batch operations like get/scan/increment etc. which uses RpcRetryingCallerImpl#callWithRetries to submit the call or batch operations like put through AsyncProcess#submit.</p>
<p>Note that previously this property means total tries number for puts, so please adjust the setting of its value if necessary. Please also be cautious when setting it to zero since retry is necessary for client cache update when region move happens.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14658">HBASE-14658</a> | <em>Major</em> | <strong>Allow loading a MonkeyFactory by class name</strong></li>
</ul>
<p>You can specify one of the predefined set of Monkeys when you run Integration Tests by passing the -m|--monkey arguments on the command line; e.g -m CALM or -m SLOW_DETERMINISTIC</p>
<p>This patch makes it so you can pass the name of a class as the monkey to run: e.g. -m org.example.KingKong</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14257">HBASE-14257</a> | <em>Major</em> | <strong>Periodic flusher only handles hbase:meta, not other system tables</strong></li>
</ul>
<p>Memstore periodic flusher used to flush META table every 5 minutes but not any other system tables. This jira extends it to flush all system tables within this time period.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14700">HBASE-14700</a> | <em>Major</em> | <strong>Support a &quot;permissive&quot; mode for secure clusters to allow &quot;simple&quot; auth clients</strong></li>
</ul>
<p>Secure HBase now supports a permissive mode to allow mixed secure and insecure clients. This allows clients to be incrementally migrated over to a secure configuration. To enable clients to continue to connect using SIMPLE authentication when the cluster is configured for security, set &quot;hbase.ipc.server.fallback-to-simple-auth-allowed&quot; equal to &quot;true&quot; in hbase-site.xml. NOTE: This setting should ONLY be used as a temporary measure while converting clients over to secure authentication. It MUST BE DISABLED for secure operation.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12822">HBASE-12822</a> | <em>Minor</em> | <strong>Option for Unloading regions through region_mover.rb without Acknowledging</strong></li>
</ul>
<p>Incorporated in HBASE-13014.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14387">HBASE-14387</a> | <em>Major</em> | <strong>Compaction improvements: Maximum off-peak compaction size</strong></li>
</ul>
<p>New configuration option: hbase.hstore.compaction.max.size.offpeak - maximum selection size eligible for minor compaction during off peak hours. hbase.hstore.compaction.max.size - this is default maximum if no off-peak hours are defined or if no maximum off-peak maximum size is defined.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14793">HBASE-14793</a> | <em>Major</em> | <strong>Allow limiting size of block into L1 block cache.</strong></li>
</ul>
<p>Very large blocks can fragment the heap and cause bad issues for the garbage collector, especially the G1GC. Now there is a maximum size that a block can be and still stick in the LruBlockCache. That size defaults to 16mb but can be controlled by changing &quot;hbase.lru.max.block.size&quot;</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12751">HBASE-12751</a> | <em>Major</em> | <strong>Allow RowLock to be reader writer</strong></li>
</ul>
<p>Locks on row are now reader/writer rather than exclusive.</p>
<p>Moves sequenceid out of HRegion and into MVCC class; MVCC is now in charge. A WAL append is still stamped in same way (we pass MVCC context in a few places where we previously we did not).</p>
<p>MVCC methods cleaned up. Make a bit more sense now. Less of them.</p>
<p>Simplifies our update of MemStore/WAL. Now we update memstore AFTER we add to WAL (but before we sync). This fixes possible dataloss when two edits came in with same coordinates; we could order the edits in memstore differently to how they arrived in the WAL.</p>
<p>Marked as an incompatible change because it breaks Distributed Log Replay, a feature we'd determined already was unreliable and to be removed (See http://search-hadoop.com/m/YGbbhTJpoal8GD1).</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14799">HBASE-14799</a> | <em>Critical</em> | <strong>Commons-collections object deserialization remote command execution vulnerability</strong></li>
</ul>
<p>This issue resolves a potential security vulnerability. For all versions we update our commons-collections dependency to the release that fixes the reported vulnerability in that library. In 0.98 we additionally disable by default a feature of code carried from 0.94 for backwards compatibility that is not needed.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14821">HBASE-14821</a> | <em>Major</em> | <strong>CopyTable should allow overriding more config properties for peer cluster</strong></li>
</ul>
<p>Configuration properties for org.apache.hadoop.hbase.mapreduce.TableOutputFormat can now be overridden by prefixing the property keys with &quot;hbase.mapred.output.&quot;. When the configuration is applied to TableOutputFormat, these entries will be rewritten with the prefix removed -- ie. &quot;hbase.mapred.output.hbase.security.authentication&quot; becomes &quot;hbase.security.authentication&quot;. This can be useful when directing output to a peer cluster with different security configuration, for example.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14825">HBASE-14825</a> | <em>Minor</em> | <strong>HBase Ref Guide corrections of typos/misspellings</strong></li>
</ul>
<p>Corrections to content of &quot;book.html&quot;, which is pulled from various *.adoc files and *.xml files. -- corrects typos/misspellings -- corrects incorrectly formatted links</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14926">HBASE-14926</a> | <em>Major</em> | <strong>Hung ThriftServer; no timeout on read from client; if client crashes, worker thread gets stuck reading</strong></li>
</ul>
<p>Adds a timeout to server read from clients. Adds new configs hbase.thrift.server.socket.read.timeout for setting read timeout on server socket in milliseconds. Default is 60000;</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14605">HBASE-14605</a> | <em>Blocker</em> | <strong>Split fails due to 'No valid credentials' error when SecureBulkLoadEndpoint#start tries to access hdfs</strong></li>
</ul>
<p>When split is requested by non-super user, split related notifications for Coprocessor are executed using the login of the request user. Previously the notifications were carried out as super user.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14631">HBASE-14631</a> | <em>Blocker</em> | <strong>Region merge request should be audited with request user through proper scope of doAs() calls to region observer notifications</strong></li>
</ul>
<p>Region observer notifications w.r.t. merge request are now audited with request user through proper scope of doAs() calls.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14655">HBASE-14655</a> | <em>Blocker</em> | <strong>Narrow the scope of doAs() calls to region observer notifications for compaction</strong></li>
</ul>
<p>Region observer notifications w.r.t. compaction request are now audited with request user through proper scope of doAs() calls.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-7171">HBASE-7171</a> | <em>Major</em> | <strong>Initial web UI for region/memstore/storefiles details</strong></li>
</ul>
<p>HBASE-7171 adds 2 new pages to the region server Web UI to ease debugging and provide greater insight into the physical data layout.</p>
<p>Region names in UI table listing all regions (on the RS status page) are now hyperlinks leading to region detail page which shows some aggregate memstore information (currently just memory used) along with the list of all Store Files (HFiles) in the region. Names of Store Files are also hyperlinks leading to Store File detail page, which currently runs 'hbase hfile' command behind the scene and displays statistics about store file.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13153">HBASE-13153</a> | <em>Major</em> | <strong>Bulk Loaded HFile Replication</strong></li>
</ul>
<p>This enhances the HBase replication to support replication of bulk loaded data. This is configurable, by default it is set to false which means it will not replicate the bulk loaded data to its peer(s). To enable it set &quot;hbase.replication.bulkload.enabled&quot; to true.</p>
<p>Following are the additional configurations added for this enhancement, a. hbase.replication.cluster.id - This is manadatory to configure in cluster where replication for bulk loaded data is enabled. A source cluster is uniquely identified by sink cluster using this id. This should be configured in the source cluster configuration file for all the RS. b. hbase.replication.conf.dir - This represents the directory where all the active cluster's file system client configurations are defined in subfolders corresponding to their respective replication cluster id in peer cluster. This should be configured in the peer cluster configuration file for all the RS. Default is HBASE_CONF_DIR. c. hbase.replication.source.fs.conf.provider - This represents the class which provides the source cluster file system client configuration to peer cluster. This should be configured in the peer cluster configuration file for all the RS. Default is org.apache.hadoop.hbase.replication.regionserver.DefaultSourceFSConfigurationProvider</p>
<p>For example: If source cluster FS client configurations are copied in peer cluster under directory /home/user/dc1/ then hbase.replication.cluster.id should be configured as dc1 and hbase.replication.conf.dir as /home/user</p>
<p>Note: a. Any modification to source cluster FS client configuration files in peer cluster side replication configuration directory then it needs to restart all its peer(s) cluster RS with default hbase.replication.source.fs.conf.provider. b. Only 'xml' type files will be loaded by the default hbase.replication.source.fs.conf.provider.</p>
<p>As part of this we have made following changes to LoadIncrementalHFiles class which is marked as Public and Stable class, a. Raised the visibility scope of LoadQueueItem class from package private to public. b. Added a new method loadHFileQueue, which loads the queue of LoadQueueItem into the table as per the region keys provided.</p>
<hr />
<ul>
<li><p><a href="https://issues.apache.org/jira/browse/HBASE-14769">HBASE-14769</a> | <em>Major</em> | <strong>Remove unused functions and duplicate javadocs from HBaseAdmin</strong></p></li>
<li>Removes functions from HBaseAdmin which require table name parameter as either byte[] or String. Use their counterparts which take TableName instead.</li>
<li>Removes redundant javadocs from HBaseAdmin as they will be automatically inherited from Admin interface.</li>
<li><p>HBaseAdmin is marked Audience.private so it should have been straight forward okay to remove the functions. But HBaseTestingUtility, which is marked Audience.public had a public function returning its instance, which moved this decision into gray area. Discussing in the community, it was decided that it would be okay to do so in this particular case.</p></li>
</ul>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14906">HBASE-14906</a> | <em>Major</em> | <strong>Improvements on FlushLargeStoresPolicy</strong></li>
</ul>
<p>In HBASE-14906 we use &quot;hbase.hregion.memstore.flush.size/column_family_number&quot; as the default threshold for memstore flush instead of the fixed value through &quot;hbase.hregion.percolumnfamilyflush.size.lower.bound&quot; property, which makes the default threshold more flexible to various use case. We also introduce a new property in name of &quot;hbase.hregion.percolumnfamilyflush.size.lower.bound.min&quot; with 16M as the default value to avoid small flush in cases like hundreds of column families.</p>
<p>After this change setting &quot;hbase.hregion.percolumnfamilyflush.size.lower.bound&quot; in hbase-site.xml won't take effect anymore, but expert users could still set this property in table descriptor to override the default value just as before</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14946">HBASE-14946</a> | <em>Critical</em> | <strong>Don't allow multi's to over run the max result size.</strong></li>
</ul>
<p>The HBase region server will now send a chunk of get responses to a client if the total response size is too large. This will only be done for clients 1.2.0 and beyond. Older clients by default will have the old behavior.</p>
<p>This patch is for the case where the basic flow is like this:</p>
<p>I want to get a single column from lots of rows. So I create a list of gets. Then I send them to table.get(List&lt;Get&gt;). If the regions for that table are spread out then those requests get chunked out to all the region servers. No one regionserver gets too many. However if one region server contains lots of regions for that table then a multi action can contain lots of gets. No single get is too onerous. However the regionserver won't return until every get is complete. So if there are thousands of gets that are sent in one multi then the regionserver can retain lots of data in one thread.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14745">HBASE-14745</a> | <em>Blocker</em> | <strong>Shade the last few dependencies in hbase-shaded-client</strong></li>
</ul>
<p>Previously some dependencies in hbase-shaded-client were still leaking into the un-shaded namespace. This should now be fixed.</p>
<p>Additionally the rat checking on generated intermediate files from shading should be skipped.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14960">HBASE-14960</a> | <em>Major</em> | <strong>Fallback to using default RPCControllerFactory if class cannot be loaded</strong></li>
</ul>
<p>If the configured RPC controller factory (via hbase.rpc.controllerfactory.class) cannot be found in the classpath or loaded, we fall back to using the default RPC controller factory in HBase.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14534">HBASE-14534</a> | <em>Minor</em> | <strong>Bump yammer/coda/dropwizard metrics dependency version</strong></li>
</ul>
<p>Updated yammer metrics to version 3.1.2 (now it's been renamed to dropwizard). API has changed quite a bit, consult https://dropwizard.github.io/metrics/3.1.0/manual/core/ for additional information.</p>
<p>Note that among other things, in yammer 2.2.0 histograms were by default created in non-biased mode (uniform sampling), while in 3.1.0 histograms created via MetricsRegistry.histogram(...) are by default exponentially decayed. This shouldn't affect end users, though.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14984">HBASE-14984</a> | <em>Major</em> | <strong>Allow memcached block cache to set optimze to false</strong></li>
</ul>
<p>Setting hbase.cache.memcached.spy.optimze to true will allow the spy memcached client to try and optimize for the number of requests outstanding. This can increase throughput but can also increase variance for request times.</p>
<p>Setting it to true will help when round trip times are longer. Setting it to false ( the default ) will help ensure a more even distribution of response times.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14951">HBASE-14951</a> | <em>Minor</em> | <strong>Make hbase.regionserver.maxlogs obsolete</strong></li>
</ul>
<p>Rolling WAL events across a cluster can be highly correlated, hence flushing memstores, hence triggering minor compactions, that can be promoted to major ones. These events are highly correlated in time if there is a balanced write-load on the regions in a table. Default value for maximum WAL files (* hbase.regionserver.maxlogs*), which controls WAL rolling events - 32 is too small for many modern deployments. Now we calculate this value dynamically (if not defined by user), using the following formula:</p>
<p>maxLogs = Math.max( 32, HBASE_HEAP_SIZE * memstoreRatio * 2/ LogRollSize), where</p>
<p>memstoreRatio is *hbase.regionserver.global.memstore.size* LogRollSize is maximum WAL file size (default 0.95 * HDFS block size)</p>
<p>We need to make sure that we avoid fully or minimize events when RS has to flush memstores prematurely only because it reached artificial limit of hbase.regionserver.maxlogs, this is why we put this 2 x multiplier in equation, this gives us maximum WAL capacity of 2 x RS memstore-size.</p>
<p>Runaway WAL files.</p>
<p>The default log rolling period (1h) allows to accumulate up to 2 X Memstore Size data in a WAL. For heap size - 32G and all other default setting, this gives ~ 26GB of data. Under heavy write load, the number of WAL files can increase dramatically. RegionServer LogRoller will be archiving old WALs periodically. User has three options, either override default hbase.regionserver.maxlogs or override default hbase.regionserver.logroll.period (decrease), or both to control runaway WALs.</p>
<p>For system with bursty write load, the hbase.regionserver.logroll.period can be decreased to lower value. In this case the maximum number of wal files will be defined by the total size of memstore (unflushed data), not by the hbase.regionserver.maxlogs. But for majority of applications there will be no issues with defaults. Data will be flushed periodically from memstore, the LogRoller will archive old wal files and the system will never reach the new defaults for hbase.regionserver.maxlogs, unless the system is under extreme load for prolonged period of time, but in this case, decreasing hbase.regionserver.logroll.period allows us to control runaway wal files.</p>
<p>The following table gives the new default maximum log files values for several different Region Server heap sizes:</p>
<p>heap memstore perc maxLogs 1G 40% 32 2G 40% 32 10G 40% 80 20G 40% 160 32G 40% 256</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14978">HBASE-14978</a> | <em>Blocker</em> | <strong>Don't allow Multi to retain too many blocks</strong></li>
</ul>
<p>Limiting the amount of memory resident for any one request allows the server to handle concurrent requests smoothly. To this end we added the ability to limit the size of responses to a multi request. That worked well however it correctly represent the amount of memory resident. So this issue adds on a an approximation of the number of blocks held for a request.</p>
<p>All clients before 1.2.0 will not get this multi request chunking based upon blocks kept. All clients 1.2.0 and after will.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14205">HBASE-14205</a> | <em>Critical</em> | <strong>RegionCoprocessorHost System.nanoTime() performance bottleneck</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14822">HBASE-14822</a> | <em>Major</em> | <strong>Renewing leases of scanners doesn't work</strong></li>
</ul>
<p>And 1.1, 1.0, and 0.98.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14849">HBASE-14849</a> | <em>Major</em> | <strong>Add option to set block cache to false on SparkSQL executions</strong></li>
</ul>
<p>For user configurable parameters for HBase datasources. Please refer to org.apache.hadoop.hbase.spark.datasources.HBaseSparkConf for details.</p>
<p>User can either set them in SparkConf, which will take effect globally, or configure it per table, which will overwrite the value set in SparkConf. If not set, the default value will take effect.</p>
<p>Currently three parameters are supported. 1. spark.hbase.blockcache.enable for blockcache enable/disable. Default is enable, but note that this potentially may slow down the system. 2. spark.hbase.cacheSize for cache size when performing HBase table scan. Default value is 1000 3. spark.hbase.batchNum for the batch number when performing HBase table scan. Default value is 1000.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14976">HBASE-14976</a> | <em>Minor</em> | <strong>Add RPC call queues to the web ui</strong></li>
</ul>
<p>Adds column displaying current aggregated call queues size in region server queues tab UI.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14796">HBASE-14796</a> | <em>Minor</em> | <strong>Enhance the Gets in the connector</strong></li>
</ul>
<p>spark.hbase.bulkGetSize in HBaseSparkConf is for grouping bulkGet, and default value is 1000.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15018">HBASE-15018</a> | <em>Major</em> | <strong>Inconsistent way of handling TimeoutException in the rpc client implementations</strong></li>
</ul>
<p>When using the new AsyncRpcClient introduced in HBase 1.1.0 (HBASE-12684), time outs now result in an IOException wrapped around a CallTimeoutException instead of a bare CallTimeoutException. This change makes the AsyncRpcClient behave the same as the default HBase 1.y RPC client implementation.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14888">HBASE-14888</a> | <em>Major</em> | <strong>ClusterSchema: Add Namespace Operations</strong></li>
</ul>
<p>This patch changes the semantic around namespace create/delete/modify when coprocessor asks that the invocation be by-passed. Previous the by-pass was done silently -- the method would just return with no indication as to whether by-pass route had been taken or not. This patch adds throwing of a BypassCoprocessorException which is thrown if we have been asked to bypass a call.</p>
<p>The bypass facility has been in place since hbase 1.0.0 when namespace creation/deletion, etc.., was originally added in HBASE-8408 (HBASE-15071 is about addressing bypass handling in a general way)</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14468">HBASE-14468</a> | <em>Major</em> | <strong>Compaction improvements: FIFO compaction policy</strong></li>
</ul>
<p>FIFO compaction policy selects only files which have all cells expired. The column family MUST have non-default TTL. Essentially, FIFO compactor does only one job: collects expired store files.</p>
<p>Because we do not do any real compaction, we do not use CPU and IO (disk and network), we do not evict hot data from a block cache. The result: improved throughput and latency both write and read. See: https://github.com/facebook/rocksdb/wiki/FIFO-compaction-style</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15027">HBASE-15027</a> | <em>Major</em> | <strong>Refactor the way the CompactedHFileDischarger threads are created</strong></li>
</ul>
<p>The property 'hbase.hfile.compactions.discharger.interval' has been renamed to 'hbase.hfile.compaction.discharger.interval' that describes the interval after which the compaction discharger chore service should run. The property 'hbase.hfile.compaction.discharger.thread.count' describes the thread count that does the compaction discharge work. The CompactedHFilesDischarger is a chore service now started as part of the RegionServer and this chore service iterates over all the onlineRegions in that RS and uses the RegionServer's executor service to launch a set of threads that does this job of compaction files clean up.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15111">HBASE-15111</a> | <em>Trivial</em> | <strong>&quot;hbase version&quot; should write to stdout</strong></li>
</ul>
<p>The <code>hbase version</code> command now outputs directly to stdout rather than to a logger. This change allows the version information to be output consistently regardless of logger configuration. Naturally, this also means the command output ignores all logger configuration. Furthermore, the move from loggers to direct output changes the output of the command to omit metadata commonly included in logger ouput such as a timestamp, log level, and logger name.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15098">HBASE-15098</a> | <em>Blocker</em> | <strong>Normalizer switch in configuration is not used</strong></li>
</ul>
<p>The config parameter, hbase.normalizer.enabled, has been dropped since it is not used in the code base.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14865">HBASE-14865</a> | <em>Major</em> | <strong>Support passing multiple QOPs to SaslClient/Server via hbase.rpc.protection</strong></li>
</ul>
<p>With this patch, hbase.rpc.protection can now take multiple comma-separate QOP values. Accepted QOP values remain unchanged and are 'authentication', 'integrity', and 'privacy'. Server or client can use this configuration to specify their preference (in decreasing order) while negotiating QOP. This feature can be used to upgrade or downgrade QOP in an online cluster without compromising availability (i.e. taking cluster offline). For e.g. to change qop from A to B, typical steps would be: &quot;A&quot; --&gt; &quot;B,A&quot; --&gt; rolling restart --&gt; &quot;B&quot; --&gt; rolling restart</p>
<p>Sidenote: Based on experimentation, server's choice is given higher preference than client's choice. i.e. if server's choices are &quot;A,B,C&quot; and client's choices are &quot;B,C,A&quot;, both A and B are acceptable, but A is chosen.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13082">HBASE-13082</a> | <em>Major</em> | <strong>Coarsen StoreScanner locks to RegionScanner</strong></li>
</ul>
<p>After this JIRA we will not be doing any scanner reset after compaction during a course of a scan. The files that were compacted will still be continued to be used in the scan process. The compacted files will be archived by a background thread that runs every 2 mins by default only when there are no active scanners on those comapcted files. The above duration can be controlled using the knob 'hbase.hfile.compactions.cleaner.interval'.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15125">HBASE-15125</a> | <em>Major</em> | <strong>HBaseFsck's adoptHdfsOrphan function creates region with wrong end key boundary</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15145">HBASE-15145</a> | <em>Major</em> | <strong>HBCK and Replication should authenticate to zookepeer using server principal</strong></li>
</ul>
<p>Added a new command line argument: --auth-as-server to enable authenticating to ZooKeeper as the HBase Server principal. This is required for secure clusters for doing replication operations like add_peer, list_peers, etc until HBASE-11392 is fixed. This advanced option can also be used for manually fixing secure znodes.</p>
<p>Commands can now be invoked like: hbase --auth-as-server shell hbase --auth-as-server zkcli</p>
<p>HBCK in secure setup also needs to authenticate to ZK using servers principals.This is turned on by default (no need to pass additional argument).</p>
<p>When authenticating as server, HBASE_SERVER_JAAS_OPTS is concatenated to HBASE_OPTS if defined in hbase-env.sh. Otherwise, HBASE_REGIONSERVER_OPTS is concatenated.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15129">HBASE-15129</a> | <em>Major</em> | <strong>Set default value for hbase.fs.tmp.dir rather than fully depend on hbase-default.xml</strong></li>
</ul>
<p>Before HBASE-15129, if somehow hbase-default.xml is not on classpath, default values for hbase.fs.tmp.dir and hbase.bulkload.staging.dir are left empty. After HBASE-15129, default values of both properties are set to &quot;/user/&lt;user.name&gt;/hbase-staging&quot;.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15218">HBASE-15218</a> | <em>Blocker</em> | <strong>On RS crash and replay of WAL, loosing all Tags in Cells</strong></li>
</ul>
<p>This issue fixes - In case of normal WAL (Not encrypted) we were loosing all cell tags on WAL replay after an RS crash - In case of encrypted WAL we were not even persisting Cell tags in WAL. Tags from all unflushed (to HFile) Cells will get lost even after WAL replay recovery is done.</p>
<p>As we use tags for Cell level security, this fixes 2 security issues - Cell level visibility labels security breach . Making a visibility restricted cell global readable - Cell level ACL availability issue. A user who is cell level authorized to read this cell can not read it. It is a data loss for him.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15157">HBASE-15157</a> | <em>Major</em> | <strong>Add *PerformanceTest for Append, CheckAnd*</strong></li>
</ul>
<p>Add append, increment, checkAndMutate, checkAndPut, and checkAndDelete tests to PerformanceEvaluation tool. Below are excerpts from new usage from PE:</p>
<p>.... Command: append Append on each row; clients overlap on keyspace so some concurrent operations checkAndDelete CheckAndDelete on each row; clients overlap on keyspace so some concurrent operations checkAndMutate CheckAndMutate on each row; clients overlap on keyspace so some concurrent operations checkAndPut CheckAndPut on each row; clients overlap on keyspace so some concurrent operations filterScan Run scan test using a filter to find a specific row based on it's value (make sure to use --rows=20) increment Increment on each row; clients overlap on keyspace so some concurrent operations randomRead Run random read test .... Examples: ... To run 10 clients doing increments over ten rows: $ bin/hbase org.apache.hadoop.hbase.PerformanceEvaluation --rows=10 --nomapred increment 10</p>
<p>Removed IncrementPerformanceTest. It is not as configurable as the additions made here.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15158">HBASE-15158</a> | <em>Major</em> | <strong>Change order in which we do write pipeline operations; do all under row locks!</strong></li>
</ul>
<p>Changed the write pipeline order; made it more rational, easier-to-reason-about doing all updates to WA, MemStore, and mvcc while read/write rowlock is held where before we'd release after WAL append and then do sync and mvcc.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15046">HBASE-15046</a> | <em>Major</em> | <strong>Perf test doing all mutation steps under row lock</strong></li>
</ul>
<p>In here we perf tested a realignment of the write pipeline and mvcc handling. Thought was that this work was a predicate for a general fix of HBASE-14460 (turns out, realignment of write path was not needed to fix the increment perf regression). The perf testing here made it so we were able to simplify writing. HBASE-15158 was just committed. This work is done.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14460">HBASE-14460</a> | <em>Critical</em> | <strong>[Perf Regression] Merge of MVCC and SequenceId (HBASE-8763) slowed Increments, CheckAndPuts, batch operations</strong></li>
</ul>
<p>This release note tries to tell the general story. Dive into sub-tasks for more specific release noting.</p>
<p>Increments, appends, checkAnd* have been slow since hbase-.1.0.0. The unification of mvcc and sequence id done by HBASE-8763 was responsible.</p>
<p>A fast-path workaround was added by HBASE-15031 Fix merge of MVCC and SequenceID performance regression in branch-1.0 for Increments. It became available in 1.0.3 and 1.1.3. To enable the fast path, set &quot;hbase.increment.fast.but.narrow.consistency&quot; and then rolling restart. The workaround was for increments only (appends, checkAndPut, etc., were not addressed. See HBASE-15031 release note for more detail).</p>
<p>Subsequently, the regression was properly identified and fixed in HBASE-15213 and the fix applied to branch-1.0 and branch-1.1. As it happens, hbase-1.2.0 does not suffer from the performance regression (though the thought was that it did -- and so it got the fast-path patch too via HBASE-15092) nor does the master branch. HBASE-15213 identified that HBASE-12751 (as a side effect) had cured the regression.</p>
<p>hbase-1.0.4 (if it is ever released -- 1.0 has been end-of-lifed) and hbase-1.1.4 will have the HBASE-15213 fix. If you are suffering from the increment regression and you are on 1.0.3 or 1.1.3, you can enable the work around to get back your increment performance but you should upgrade.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14355">HBASE-14355</a> | <em>Major</em> | <strong>Scan different TimeRange for each column family</strong></li>
</ul>
<p>Adds being able to Scan each column family with a different time range. Adds new methods setColumnFamilyTimeRange and getColumnFamilyTimeRange to Scan.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15100">HBASE-15100</a> | <em>Blocker</em> | <strong>Master WALProcs still never clean up</strong></li>
</ul>
<p>The constructor for o.a.h.hbase.ProcedureInfo was mistakenly labeled IA.Public in previous releases and has now changed to IA.Private. Downstream users are safe to consume ProcedureInfo objects returned from HBase public interfaces, but should not expect to be able to reliably create new instances themselves.</p>
<p>The method ProcedureInfo.setNonceKey has been removed, because it should not have been exposed to clients.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14949">HBASE-14949</a> | <em>Major</em> | <strong>Resolve name conflict when splitting if there are duplicated WAL entries</strong></li>
</ul>
<p>Now we can write duplicated WAL entries into different WAL files. This feature is required by the replication consistency fix and new implementation of WAL writer.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15219">HBASE-15219</a> | <em>Critical</em> | <strong>Canary tool does not return non-zero exit code when one of regions is in stuck state</strong></li>
</ul>
<p>A new flag is added for Canary tool: -treatFailureAsError When this flag is specified, read / write failure would result in Canary tool exit code of 5.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-11927">HBASE-11927</a> | <em>Major</em> | <strong>Use Native Hadoop Library for HFile checksum (And flip default from CRC32 to CRC32C)</strong></li>
</ul>
<p>Checksumming is cpu intensive. HBase computes additional checksums for HFiles (hdfs does checksums too) and stores them inline with file data. During reading, these checksums are verified to ensure data is not corrupted. This patch tries to use Hadoop Native Library for checksum computation, if its available, otherwise falls back to standard Java libraries. Instructions to load NHL in HBase can be found here (http://hbase.apache.org/book.html#hadoop.native.lib).</p>
<p>Default checksum algorithm has been changed from CRC32 to CRC32C primarily because of two reasons: 1) CRC32C has better error detection properties, and 2) New Intel processors have a dedicated instruction for crc32c computation (SSE4.2 instruction set)*. This change is fully backward compatible. Also, users should not see any differences except decrease in cpu usage. To keep old settings, set configuration hbase.hstore.checksum.algorithm to CRC32.</p>
<p>* On linux, run 'cat /proc/cpuinfo and look for sse4_2 in list of flags to see if your processor supports SSE4.2.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13259">HBASE-13259</a> | <em>Critical</em> | <strong>mmap() based BucketCache IOEngine</strong></li>
</ul>
<p>mmap() based bucket cache can be configured by specifying the property {code} &lt;property&gt; &lt;name&gt;hbase.bucketcache.ioengine&lt;/name&gt; &lt;value&gt; mmap://filepath &lt;/value&gt; &lt;/property&gt; {code} This mode of bucket cache is ideal when your file based bucket cache size is lesser than then available RAM. When the cache is bigger than the available RAM then the kernel page faults will make this cache perform lesser particularly in case of scans.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15184">HBASE-15184</a> | <em>Critical</em> | <strong>SparkSQL Scan operation doesn't work on kerberos cluster</strong></li>
</ul>
<p>Before this patch, users of the spark HBaseContext would fail due to lack of privilege exceptions.</p>
<p>Note: * It is preferred to have spark in spark-on-yarn mode if Kerberos is used. * This is orthogonal to issues with a kerberized spark cluster via InputFormats.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15264">HBASE-15264</a> | <em>Major</em> | <strong>Implement a fan out HDFS OutputStream</strong></li>
</ul>
<p>Implement a fan-out asynchronous DFSOutputStream for implementing new WAL writer.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15290">HBASE-15290</a> | <em>Major</em> | <strong>Hbase Rest CheckAndAPI should save other cells along with compared cell</strong></li>
</ul>
<p>Fixed an issue in REST server checkAndPut operation where the remaining cells other than the to-be-checked column are also applied in the put operation .</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15181">HBASE-15181</a> | <em>Major</em> | <strong>A simple implementation of date based tiered compaction</strong></li>
</ul>
<p>Date tiered compaction policy is a date-aware store file layout that is beneficial for time-range scans for time-series data.</p>
<p>When it performs well:</p>
<pre><code>reads for limited time ranges, especially scans of recent data</code></pre>
<p>When it doesn't perform as well:</p>
<pre><code>random gets without a time range
frequent deletes and updates
out of order data writes, especially writes with timestamps in the future
bulk loads of historical data</code></pre>
<p>Recommended configuration: To turn on Date Tiered Compaction (It is not recommended to turn on for the whole cluster because that will put meta table on it too and random get on meta table will be impacted): hbase.hstore.compaction.compaction.policy: org.apache.hadoop.hbase.regionserver.compactions.DateTieredCompactionPolicy</p>
<p>Parameters for Date Tiered Compaction: hbase.hstore.compaction.date.tiered.max.storefile.age.millis: Files with max-timestamp smaller than this will no longer be compacted.Default at Long.MAX_VALUE. hbase.hstore.compaction.date.tiered.base.window.millis: base window size in milliseconds. Default at 6 hours. hbase.hstore.compaction.date.tiered.windows.per.tier: number of windows per tier. Default at 4. hbase.hstore.compaction.date.tiered.incoming.window.min: minimal number of files to compact in the incoming window. Set it to expected number of files in the window to avoid wasteful compaction. Default at 6. hbase.hstore.compaction.date.tiered.window.policy.class: the policy to select store files within the same time window. It doesnt apply to the incoming window. Default at exploring compaction. This is to avoid wasteful compaction.</p>
<p>With tiered compaction all servers in the cluster will promote windows to higher tier at the same time, so using a compaction throttle is recommended: hbase.regionserver.throughput.controller:org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController</p>
<p>Because there will most likely be more store files around, we need to adjust the configuration so that flush won't be blocked and compaction will be properly throttled: hbase.hstore.blockingStoreFiles: change to 50 if using all default parameters when turning on date tiered compaction. Use 1.5~2 x projected file count if changing the parameters, Projected file count = windows per tier x tier count + incoming window min + files older than max age</p>
<p>For more details, please refer to the design spec at https://docs.google.com/document/d/1_AmlNb2N8Us1xICsTeGDLKIqL6T-oHoRLZ323MG_uy8/edit#</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15136">HBASE-15136</a> | <em>Critical</em> | <strong>Explore different queuing behaviors while busy</strong></li>
</ul>
<p>Previously RPC request scheduler in HBase had 2 modes in could operate in:</p>
<ul>
<li>simple FIFO</li>
<li>&quot;partial&quot; deadline, where deadline constraints are only imposed on long-running scan requests.</li>
</ul>
<p>This patch adds new type of scheduler to HBase, based on the research around controlled delay (CoDel) algorithm [1], used in networking to combat bufferbloat, as well as some analysis on generalizing it to generic request queues [2]. The purpose of that work is to prevent long standing call queues caused by discrepancy between request rate and available throughput, caused by kernel/disk IO/networking stalls.</p>
<p>New RPC scheduler could be enabled by setting hbase.ipc.server.callqueue.type=codel in configuration. Several additional params allow to configure algorithm behavior -</p>
<p>hbase.ipc.server.callqueue.codel.target.delay hbase.ipc.server.callqueue.codel.interval hbase.ipc.server.callqueue.codel.lifo.threshold</p>
<p>[1] Controlling Queue Delay / A modern AQM is just one piece of the solution to bufferbloat. http://queue.acm.org/detail.cfm?id=2209336 [2] Fail at Scale / Reliability in the face of rapid change. http://queue.acm.org/detail.cfm?id=2839461</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15338">HBASE-15338</a> | <em>Minor</em> | <strong>Add a option to disable the data block cache for testing the performance of underlying file system</strong></li>
</ul>
<p>Add a new config: hbase.block.data.cacheonread, which is a global switch for caching data blocks on read. The default value of this switch is true, and data blocks will be cached on read if the block cache is enabled for the family and cacheBlocks flag is set to be true for get and scan operations. If this global switch is set to false, data blocks won't be cached even if the block cache is enabled for the family and the cacheBlocks flag of Gets or Scans are sets as true. Bloom blocks and index blocks are always be cached if the block cache of the regionserver is enabled. One usage of this switch is for the performance tests for the extreme case that the cache for data blocks all missed and all data blocks are read from underlying file system.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15376">HBASE-15376</a> | <em>Major</em> | <strong>ScanNext metric is size-based while every other per-operation metric is time based</strong></li>
</ul>
<p>Removed ScanNext histogram metrics as regionserver level and per-region level metrics since the semantics is not compatible with other similar metrics (size histogram vs latency histogram).</p>
<p>Instead, this patch adds ScanTime and ScanSize histogram metrics at the regionserver and per-region level.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15243">HBASE-15243</a> | <em>Major</em> | <strong>Utilize the lowest seek value when all Filters in MUST_PASS_ONE FilterList return SEEK_NEXT_USING_HINT</strong></li>
</ul>
<p>When all filters in a MUST_PASS_ONE FilterList return a SEEK_USING_NEXT_HINT code, we return SEEK_NEXT_USING_HINT from the FilterList#filterKeyValue() to utilize the lowest seek value.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15364">HBASE-15364</a> | <em>Major</em> | <strong>Fix unescaped &lt; characters in Javadoc</strong></li>
</ul>
<p>HBASE-15364 Fix unescaped &lt; and &gt; characters in Javadoc</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15271">HBASE-15271</a> | <em>Major</em> | <strong>Spark Bulk Load: Need to write HFiles to tmp location then rename to protect from Spark Executor Failures</strong></li>
</ul>
<p>When using the bulk load helper provided by the hbase-spark module, output files will now be written into temporary files and only made available when the executor has successfully completed.</p>
<p>Previously, failed executors would leave their files in place in a way that would be picked up by a bulk load command. This caused retried failures to include spurious copies of some cells.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13963">HBASE-13963</a> | <em>Critical</em> | <strong>avoid leaking jdk.tools</strong></li>
</ul>
<p>HBase now ensures that the JDK tools jar used during the build process is not exposed to downstream clients as a transitive dependency of hbase-annotations.</p>
<p>If you need to have the JDK tools jar in your classpath, you should add a system dependency on it. See the hbase-annotations pom for an example of the necessary pom additions.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15435">HBASE-15435</a> | <em>Major</em> | <strong>Add WAL (in bytes) written metric</strong></li>
</ul>
<p>Adds a new metric named &quot;writtenBytes&quot; as a per-regionserver metric. Metric Description: Size (in bytes) of the data written to the WAL.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-6721">HBASE-6721</a> | <em>Major</em> | <strong>RegionServer Group based Assignment</strong></li>
</ul>
<p>[ADVANCED USERS ONLY] This patch adds a new experimental module hbase-rsgroup. It is an advanced feature for partitioning regionservers into distinctive groups for strict isolation, and should only be used by users who are sophisticated enough to understand the full implications and have a sufficient background in managing HBase clusters.</p>
<p>RSGroups can be defined and managed with shell commands or corresponding Java APIs. A server can be added to a group with hostname and port pair, and tables can be moved to this group so that only regionservers in the same rsgroup can host the regions of the table. RegionServers and tables can only belong to 1 group at a time. By default, all tables and regionservers belong to the &quot;default&quot; group. System tables can also be put into a group using the regular APIs. A custom balancer implementation tracks assignments per rsgroup and makes sure to move regions to the relevant regionservers in that group. The group information is stored in a regular HBase table, and a zookeeper-based read-only cache is used at the cluster bootstrap time.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15377">HBASE-15377</a> | <em>Major</em> | <strong>Per-RS Get metric is time based, per-region metric is size-based</strong></li>
</ul>
<p>Per-region metrics related to Get histograms are changed from being response size based into being latency based similar to the per-regionserver metrics of the same name.</p>
<p>Added GetSize histogram metrics at the per-regionserver and per-region level for the response sizes.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15323">HBASE-15323</a> | <em>Major</em> | <strong>Hbase Rest CheckAndDeleteAPi should be able to delete more cells</strong></li>
</ul>
<p>Fixed an issue in REST server checkAndDelete operation where the remaining cells other than the to-be-checked column are also applied in the Delete operation. Also fixed an issue in RemoteHTable where the Delete object was not passed correctly to the REST server side.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15430">HBASE-15430</a> | <em>Critical</em> | <strong>Failed taking snapshot - Manifest proto-message too large</strong></li>
</ul>
<p>Failed taking snapshot - Manifest proto-message too large. add property (&quot;snapshot.manifest.size.limit&quot;) to change max size of proto-message</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12940">HBASE-12940</a> | <em>Major</em> | <strong>Expose listPeerConfigs and getPeerConfig to the HBase shell</strong></li>
</ul>
<p>Adds get_peer_config and list_peer_configs to the hbase shell.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15322">HBASE-15322</a> | <em>Critical</em> | <strong>Operations using Unsafe path broken for platforms not having sun.misc.Unsafe</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15479">HBASE-15479</a> | <em>Major</em> | <strong>No more garbage or beware of autoboxing</strong></li>
</ul>
<p>This fix decreases client's memory allocation during writes by more than 50%.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15412">HBASE-15412</a> | <em>Major</em> | <strong>Add average region size metric</strong></li>
</ul>
<p>Adds a new metric for called &quot;averageRegionSize&quot; that is emitted as a regionserver metric. Metric description: Average region size over the region server including memstore and storefile sizes</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15212">HBASE-15212</a> | <em>Major</em> | <strong>RRCServer should enforce max request size</strong></li>
</ul>
<p>Adds a configuration parameter &quot;hbase.ipc.max.request.size&quot; which defaults to 256MB to protect the server against very large incoming RPC requests. All requests larger than this size will be immediately rejected before allocating any resources (memory allocation, etc).</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14256">HBASE-14256</a> | <em>Major</em> | <strong>Flush task message may be confusing when region is recovered</strong></li>
</ul>
<p>HBASE-14256 Correct confusing flush task message</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15265">HBASE-15265</a> | <em>Major</em> | <strong>Implement an asynchronous FSHLog</strong></li>
</ul>
<p>To enable, set the WALProvider as follows:</p>
<p>{code} &lt;property&gt; &lt;name&gt;hbase.wal.provider&lt;/name&gt; &lt;value&gt;asyncfs&lt;/value&gt; &lt;/property&gt; &lt;property&gt; {code}</p>
<p>To check which provider is active, look for the log line:</p>
<p>LOG.info(&quot;Instantiating WALProvider of type &quot; + clazz);</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15538">HBASE-15538</a> | <em>Major</em> | <strong>Implement secure async protobuf wal writer</strong></li>
</ul>
<p>Add the following config in hbase-site.xml if you want to use secure protobuf wal writer together with AsyncFSWAL {code} &lt;property&gt; &lt;name&gt;hbase.regionserver.hlog.async.writer.impl&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.regionserver.wal.SecureAsyncProtobufLogWriter&lt;/value&gt; &lt;/property&gt; &lt;property&gt; {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15521">HBASE-15521</a> | <em>Major</em> | <strong>Procedure V2 - RestoreSnapshot and CloneSnapshot</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15568">HBASE-15568</a> | <em>Major</em> | <strong>Procedure V2 - Remove CreateTableHandler in HBase Apache 2.0 release</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15396">HBASE-15396</a> | <em>Minor</em> | <strong>Enhance mapreduce.TableSplit to add encoded region name</strong></li>
</ul>
<p>To aid troubleshooting of MapReduce job that rely on the HBase provided input format, splits now include the encoded region name they cover.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15592">HBASE-15592</a> | <em>Major</em> | <strong>Print Procedure WAL content</strong></li>
</ul>
<p>Use hbase org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALPrettyPrinter to print the content of a Procedure WAL. e.g. hbase org.apache.hadoop.hbase.procedure2.store.wal.ProcedureWALPrettyPrinter -f /hbase/MasterProcWALs/state-00000000000000002571.log</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15400">HBASE-15400</a> | <em>Major</em> | <strong>Use DateTieredCompactor for Date Tiered Compaction</strong></li>
</ul>
<p>With this patch combined with HBASE-15389, when we compact, we can output multiple files along the current window boundaries. There are two use cases: 1. Major compaction: We want to output date tiered store files with data older than max age archived in trunks of the window size on the higher tier. Once a window is old enough, we don't combine the windows to promote to the next tier any further. So files in these windows retain the same timespan as they were minor-compacted last time, which is the window size of the highest tier. Major compaction will touch these files and we want to maintain the same layout. This way, TTL and archiving will be simpler and more efficient. 2. Bulk load files and the old file generated by major compaction before upgrading to DTCP.</p>
<p>This will change the way to enable date tiered compaction. To turn it on: hbase.hstore.engine.class: org.apache.hadoop.hbase.regionserver.DateTieredStoreEngine</p>
<p>With tiered compaction all servers in the cluster will promote windows to higher tier at the same time, so using a compaction throttle is recommended: hbase.regionserver.throughput.controller:org.apache.hadoop.hbase.regionserver.compactions.PressureAwareCompactionThroughputController hbase.hstore.compaction.throughput.higher.bound and hbase.hstore.compaction.throughput.lower.bound need to be set for desired throughput range as uncompressed rates.</p>
<p>Because there will most likely be more store files around, we need to adjust the configuration so that flush won't be blocked and compaction will be properly throttled: hbase.hstore.blockingStoreFiles: change to 50 if using all default parameters when turning on date tiered compaction. Use 1.5~2 x projected file count if changing the parameters, Projected file count = windows per tier x tier count + incoming window min + files older than max age</p>
<p>Because major compaction is turned on now, we also need to adjust the configuration for max file to compact according to the larger file count: hbase.hstore.compaction.max: set to the same number as hbase.hstore.blockingStoreFiles.</p>
<p>For more details, please refer to the design spec at https://docs.google.com/document/d/1_AmlNb2N8Us1xICsTeGDLKIqL6T-oHoRLZ323MG_uy8/edit#</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15537">HBASE-15537</a> | <em>Major</em> | <strong>Make multi WAL work with WALs other than FSHLog</strong></li>
</ul>
<p>Add the delegate config for multiwal back. Now you can use 'hbase.wal.regiongrouping.delegate.provider' to specify the wal provider you want to use for multiwal. For example: {code} &lt;property&gt; &lt;name&gt;hbase.wal.regiongrouping.delegate.provider&lt;/name&gt; &lt;value&gt;asyncfs&lt;/value&gt; &lt;/property&gt; {code} And the default value is filesystem which is the alias of DefaultWALProvider, i.e., the FSHLog.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15507">HBASE-15507</a> | <em>Major</em> | <strong>Online modification of enabled ReplicationPeerConfig</strong></li>
</ul>
<p>Added update_peer_config to the HBase shell and ReplicationAdmin, and provided a callback for custom replication endpoints to be notified of changes to their configuration and peer data</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15481">HBASE-15481</a> | <em>Trivial</em> | <strong>Add pre/post roll to WALObserver</strong></li>
</ul>
<!-- markdown -->
<p>WALObserver coprocessors now can receive notifications of WAL rolling via the new methods <code>preWALRoll</code> and <code>postWALRoll</code>.</p>
<p>This change is incompatible due to the addition of these methods to the <code>WALObserver</code> interface. Downstream users are encouraged to instead extend the <code>BaseWALObserver</code> class, which remains compatible through this change.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15572">HBASE-15572</a> | <em>Major</em> | <strong>Adding optional timestamp semantics to HBase-Spark</strong></li>
</ul>
<p>Right now the timestamp is always latest. With this patch, users can select timestamps they want. In this patch, 4 parameters, &quot;timestamp&quot;, &quot;minTimestamp&quot;, &quot;maxiTimestamp&quot; and &quot;maxVersions&quot; are added to HBaseSparkConf. Users can select a timestamp, they can also select a time range with minimum timestamp and maximum timestamp.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15187">HBASE-15187</a> | <em>Major</em> | <strong>Integrate CSRF prevention filter to REST gateway</strong></li>
</ul>
<p>Protection against CSRF attack can be turned on with config parameter, hbase.rest.csrf.enabled - default value is false.</p>
<p>The custom header to be sent can be changed via config parameter, hbase.rest.csrf.custom.header whose default value is &quot;X-XSRF-HEADER&quot;.</p>
<p>Config parameter, hbase.rest.csrf.methods.to.ignore , controls which HTTP methods are not associated with customer header check.</p>
<p>Config parameter, hbase.rest-csrf.browser-useragents-regex , is a comma-separated list of regular expressions used to match against an HTTP request's User-Agent header when protection against cross-site request forgery (CSRF) is enabled for REST server by setting hbase.rest.csrf.enabled to true.</p>
<p>The implementation came from hadoop/hadoop-common-project/hadoop-common/src/main/java/org/apache/hadoop/security/http/RestCsrfPreventionFilter.java</p>
<p>We should periodically update the RestCsrfPreventionFilter.java in hbase codebase to include fixes to the hadoop implementation.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13372">HBASE-13372</a> | <em>Major</em> | <strong>Unit tests for SplitTransaction and RegionMergeTransaction listeners</strong></li>
</ul>
<p>HBASE-13372 Add unit tests for SplitTransaction and RegionMergeTransaction listeners</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15386">HBASE-15386</a> | <em>Major</em> | <strong>PREFETCH_BLOCKS_ON_OPEN in HColumnDescriptor is ignored</strong></li>
</ul>
<p>This was a non-issue. The PREFETCH_... flag actually works. While here though made the following additions.</p>
<p>Changes the prefetch TRACE-level loggings to include the word 'Prefetch' in them so you know what they are about.</p>
<p>Changes the cryptic logging of the CacheConfig#toString to have some preamble saying why and what column family is responsible (helps figure what is going on)</p>
<p>Add test that verifies setting flag on HColumnDescriptor actually works.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15640">HBASE-15640</a> | <em>Major</em> | <strong>L1 cache doesn't give fair warning that it is showing partial stats only when it hits limit</strong></li>
</ul>
<p>The blockcache UI tab would stop refreshing at 100k blocks (configurable, see &quot;hbase.ui.blockcache.by.file.max&quot;), which isn't very many blocks when doing a big cache, giving a misleading picture of the content of L1 and/or L2 cache. Up the default limit to 1M blocks (UI takes a while but just a few seconds counting over 1M blocks).</p>
<p>Also, when beyond the limit give the user a noticeable WARNING in the UI.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15518">HBASE-15518</a> | <em>Major</em> | <strong>Add Per-Table metrics back</strong></li>
</ul>
<p>Adds per-table metrics aggregated from per-region metrics in region server metrics. New metrics are available under JMX section &quot;Hadoop:service=HBase,name=RegionServer,sub=Tables&quot; and they are available via hadoop metrics2 collectors.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15368">HBASE-15368</a> | <em>Major</em> | <strong>Add pluggable window support</strong></li>
</ul>
<p>Use 'hbase.hstore.compaction.date.tiered.window.factory.class' to specify the window implementation you like for date tiered compaction. Now the only and default implementation is org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory.</p>
<p>{code} &lt;property&gt; &lt;name&gt;hbase.hstore.compaction.date.tiered.window.factory.class&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hbase.regionserver.compactions.ExponentialCompactionWindowFactory&lt;/value&gt; &lt;/property&gt; &lt;property&gt; {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15366">HBASE-15366</a> | <em>Major</em> | <strong>Add doc, trace-level logging, and test around hfileblock</strong></li>
</ul>
<p>No functional change. Added javadoc, comments, and extra trace-level logging to make clear what is happening around the reading and caching of hfile blocks.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15671">HBASE-15671</a> | <em>Major</em> | <strong>Add per-table metrics on memstore, storefile and regionsize</strong></li>
</ul>
<p>Adds storeFileSize, memstoreSize and tableSize to the per-table metrics.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15392">HBASE-15392</a> | <em>Major</em> | <strong>Single Cell Get reads two HFileBlocks</strong></li>
</ul>
<p>When an explicit Get with a one or more columns specified, we at a minimum, were overseeking, reading until we tripped over the next row, regardless, and only then returning. If the next row was in-block, we'd just do too much seeking but if the next row was in the next (or in the next block beyond that), we would keep seeking and loading blocks until we found the next row before we'd return.</p>
<p>There remains one case where we will still 'overread'. It is when the row end aligns with the end of the block. In this case we will load the next block just to find that there are no more cells in the current row. See HBASE-15457.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15628">HBASE-15628</a> | <em>Major</em> | <strong>Implement an AsyncOutputStream which can work with any FileSystem implementation</strong></li>
</ul>
<p>Introduce an AsyncFSOutput interface which is an abstraction of the original FanOutOneBlockAsyncDFSOutput. Now you can create AsyncFSOutput on any FileSystem using the method AsyncFSOutputHelper.createOutput. The returned AsyncFSOutput will be FanOutOneBlockAsyncDFSOutput if the given FileSystem is a DistributedFileSystem.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15477">HBASE-15477</a> | <em>Major</em> | <strong>Do not save 'next block header' when we cache hfileblocks</strong></li>
</ul>
<p>Fix over-persisting in blockcache; no longer save the block PLUS the header of the next block (3 bytes) when writing the cache.</p>
<p>Also removes support for hfileblock v1; hfile block v1 was used writing hfile v1. hfile v1 was the default in hbase before hbase-0.92. hbase.96 would not start unless all v1 hfiles had been compacted out of the cluster.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15645">HBASE-15645</a> | <em>Critical</em> | <strong>hbase.rpc.timeout is not used in operations of HTable</strong></li>
</ul>
<p>Fixes regression where hbase.rpc.timeout configuration was ignored in branch-1.0+</p>
<p>Adds new methods setOperationTimeout, getOperationTimeout, setRpcTimeout, and getRpcTimeout to Table. In branch-1.3+ they are public interfaces and in 1.0-1.2 they are labeled as @InterfaceAudience.Private.</p>
<p>Adds hbase.client.operation.timeout to hbase-default.xml with default of 1200000</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15686">HBASE-15686</a> | <em>Major</em> | <strong>Add override mechanism for the exempt classes when dynamically loading table coprocessor</strong></li>
</ul>
<p>New coprocessor table descriptor attribute, hbase.coprocessor.classloader.included.classes, is added. User can specify class name prefixes (semicolon separated) which should be loaded by CoprocessorClassLoader through this attribute using the following syntax: {code} hbase&gt; alter 't1', 'coprocessor'=&gt;'hdfs:///foo.jar|com.foo.FooRegionObserver|1001|arg1=1,arg2=2' {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15711">HBASE-15711</a> | <em>Major</em> | <strong>Add client side property to allow logging details for batch errors</strong></li>
</ul>
<p>In HBASE-15711 a new client side property hbase.client.log.batcherrors.details is introduced to allow logging full stacktrace of exceptions for batch error. It's disabled by default and set the property to true will enable it.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15281">HBASE-15281</a> | <em>Major</em> | <strong>Allow the FileSystem inside HFileSystem to be wrapped</strong></li>
</ul>
<p>This patch adds new configuration property - hbase.fs.wrapper. If provided, it should be fully qualified class name of the class used as a pluggable wrapper for HFileSystem. This may be useful for specific debugging/tracing needs.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15720">HBASE-15720</a> | <em>Major</em> | <strong>Print row locks at the debug dump page</strong></li>
</ul>
<p>Adds a section to the debug dump page listing current row locks held.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15575">HBASE-15575</a> | <em>Minor</em> | <strong>Rename table DDL *Handler methods in MasterObserver to more meaningful names</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15767">HBASE-15767</a> | <em>Major</em> | <strong>Upgrade httpclient dependency</strong></li>
</ul>
<p>HBase now relies on version 4.3.6 of the Apache Commons HTTPClient library. Downstream users who are exposed to it via the HBase classpath will have to similarly update their dependency.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15743">HBASE-15743</a> | <em>Major</em> | <strong>Add Transparent Data Encryption support for FanOutOneBlockAsyncDFSOutput</strong></li>
</ul>
<p>Now the AsyncFSWAL can write data to a encryption zone on HDFS.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15759">HBASE-15759</a> | <em>Minor</em> | <strong>RegionObserver.preStoreScannerOpen() doesn't have acces to current readpoint</strong></li>
</ul>
<p>The following RegionObserver method is deprecated and would no longer be called in hbase 2.0:</p>
<p>public KeyValueScanner preStoreScannerOpen(final ObserverContext&lt;RegionCoprocessorEnvironment&gt; c, final Store store, final Scan scan, final NavigableSet&lt;byte[]&gt; targetCols, final KeyValueScanner s) throws IOException {</p>
<p>Instead, override this method:</p>
<p>public KeyValueScanner preStoreScannerOpen(final ObserverContext&lt;RegionCoprocessorEnvironment&gt; c, final Store store, final Scan scan, final NavigableSet&lt;byte[]&gt; targetCols, final KeyValueScanner s, final long readPt) throws IOException {</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15740">HBASE-15740</a> | <em>Major</em> | <strong>Replication source.shippedKBs metric is undercounting because it is in KB</strong></li>
</ul>
<p>Deprecated Replication source.shippedKBs metric in favor of source.shippedBytes</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15236">HBASE-15236</a> | <em>Major</em> | <strong>Inconsistent cell reads over multiple bulk-loaded HFiles</strong></li>
</ul>
<p>This jira fixes that following bug: During bulkloading, if there are multiple hfiles corresponding to same region, and if they have same timestamps (which may have been set using importtsv.timestamp) and duplicate keys across them, then get and scan may return values coming from different hfiles.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15801">HBASE-15801</a> | <em>Major</em> | <strong>Upgrade checkstyle for all branches</strong></li>
</ul>
<p>All active branches now use maven-checkstyle-plugin 2.17 and checkstyle 6.18.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15780">HBASE-15780</a> | <em>Critical</em> | <strong>Expose AuthUtil as IA.Public</strong></li>
</ul>
<p>Downstream users with long lived applications that need to communicate with secure HBase instances can now rely on the AuthUtil class to handle authenticating via keytab.</p>
<p>For more information, see the javadoc for the org.apache.hadoop.hbase.AuthUtil class.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15651">HBASE-15651</a> | <em>Major</em> | <strong>Add report-flakies.py to use jenkins api to get failing tests</strong></li>
</ul>
<p>To find recent set of flakies, run the script added by this patch. Run it to get usage information passing -h:</p>
<p>{code} $ ./dev-support/report-flakies.py -h {code}</p>
<p>If you get the below:</p>
<p>{code} $ python ./dev-support/report-flakies.py Traceback (most recent call last): File &quot;./dev-support/report-flakies.py&quot;, line 25, in &lt;module&gt; import requests ImportError: No module named requests {code}</p>
<p>... install the requests module:</p>
<p>{code} $ sudo pip install requests {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15784">HBASE-15784</a> | <em>Major</em> | <strong>Misuse core/maxPoolSize of LinkedBlockingQueue in ThreadPoolExecutor</strong></li>
</ul>
<p>The core pool size and max pool size of ThreadPoolExecutor should be the same when LinkedBlockingQueue is used. Thus the configurations hbase.hconnection.threads.max, hbase.hconnection.meta.lookup.threads.max, hbase.region.replica.replication.threads.max and hbase.multihconnection.threads.max are used as the number of the core threads, and the related configurations *.thread.core are not used any more.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15593">HBASE-15593</a> | <em>Major</em> | <strong>Time limit of scanning should be offered by client</strong></li>
</ul>
<p>Add a new configuration: hbase.ipc.min.client.request.timeout Minimum allowable timeout (in milliseconds) in rpc request's header. This configuration exists to prevent the rpc service regarding this request as timeout immediately.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15856">HBASE-15856</a> | <em>Critical</em> | <strong>Cached Connection instances can wind up with addresses never resolved</strong></li>
</ul>
<p>During periods where DNS resolution was not available or not working correctly, we could previously cache unresolved hostnames forever, in some cases preventing further connections to these hosts even when DNS service was restored. With this change, unresolved hostnames will no longer be cached, and will instead throw an UnknownHostException during connection setup.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15228">HBASE-15228</a> | <em>Major</em> | <strong>Add the methods to RegionObserver to trigger start/complete restoring WALs</strong></li>
</ul>
<p>Added two hooks around WAL restore. preReplayWALs(final ObserverContext&lt;? extends RegionCoprocessorEnvironment&gt; ctx, HRegionInfo info, Path edits) and postReplayWALs(final ObserverContext&lt;? extends RegionCoprocessorEnvironment&gt; ctx, HRegionInfo info, Path edits)</p>
<p>Will be called at start and end of restore of a WAL file. The other hook around WAL restore (preWALRestore ) will be called before restore of every entry within the WAL file.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15876">HBASE-15876</a> | <em>Blocker</em> | <strong>Remove doBulkLoad(Path hfofDir, final HTable table) though it has not been through a full deprecation cycle</strong></li>
</ul>
<p>Removes a doBulkLoad method though it has not been through a full deprecation cycle (but it is 'damaged' because it has a parameter that has been properly deprecated). Use the alternative {code}public void doBulkLoad(Path hfofDir, final Admin admin, Table table, RegionLocator regionLocator){code}</p>
<p>See http://mail-archives.apache.org/mod_mbox/hbase-dev/201605.mbox/%3CCAMUu0w-ZiLoLBLO3D76=n3AjUr=VMtTUeYA28weLHYeq8+e3bQ@mail.gmail.com%3E for NOTICE on this 'premature' removal.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14030">HBASE-14030</a> | <em>Major</em> | <strong>HBase Backup/Restore Phase 1</strong></li>
</ul>
<p>This experimental feature allows to perform backup/restore operations, including incremental ones, on a set of HBase tables.</p>
<p>Key features and Use Cases</p>
<p>A common practice of backup and restore in database is to first take full baseline backup, and then periodically take incremental backup that capture the changes since the full baseline backup. HBase cluster can store massive amount data. Therefore we want use full backup in combination with incremental backups for HBase as well. The following is a typical use case scenario for full and incremental backup:</p>
<p> The user takes a full backup of a table or a set of tables in HBase.  The user schedules periodical incremental backups to capture the changes from the full backup, or from last incremental backup.  The user needs to restore table data to a past point in time.  The full backup is restored to the table(s) or to different table name(s). Then the incremental backups that are up to the desired point in time are applied on top of the full backup. We would support the following key features and capabilities.  Backup to DFS FileSystem across clusters and possibly to other storage media or servers.  Support single table or a set of tables backup and restore (full and incremental).  Restore to different table names and to different clusters.  Support adding and removing tables to and from backup set without interruption of incremental backup schedule.  Support merge of incremental backups into longer period and bigger incremental backups for easy storage and restore.  Support scheduled backups.  Unified command line interface for all the above.</p>
<p>To illustrate these key capabilities, the following are two more detailed use case examples.</p>
<p>Use case example 1:</p>
<ol>
<li>User takes a full backup of a set of tables (i.e. table1 and table2) in HBase.</li>
<li>User takes incremental backups. The incremental backup will only track table1 and table2.</li>
<li>User adds other tables (i.e. table3 and table4) in HBase, and an implicit full backup is executed during the add process</li>
<li>User continues to take incremental backups. The incremental backup data would cover table1, table2, table3 and table4.</li>
<li>User wants to restore table3 and table4 to a past PIT (point-in-time).</li>
<li>Full backup in 3. is restored onto HBase cluster. Then the incremental backups after that full backup are applied on top of the full restore until the PIT.</li>
</ol>
<p>Use case example 2:</p>
<ol>
<li>User takes a full backup of a set of tables in HBase.</li>
<li>User takes daily incremental backups.</li>
<li>User merges the daily incremental backups into weekly incremental backups.</li>
<li>User combines/rolls up the weekly incremental backup into monthly incremental backups.</li>
<li>User wants to restore the tables to a past PIT.</li>
<li>Full backup is restored onto HBase cluster.</li>
<li>Monthly incremental backups before the desired PIT are applied.</li>
<li>Closest daily backups up to the PIT are applied.</li>
</ol>
<p>To create full backup:</p>
<p>HBASE_DIR/bin/hbase backup create full &lt;backup_root_path&gt; [tables]</p>
<p>backup_root_path - path to backup root directory (file://, hdfs:// or any other Hadoop-compatible path) tables - list of tables, comma-separated. If no tables specified then all tables will be saved.</p>
<p>To create full backup:</p>
<p>HBASE_DIR/bin/hbase backup create incremental &lt;backup_root_path&gt; [tables]</p>
<p>backup_root_path - path to backup root directory (file://, hdfs:// or any other Hadoop-compatible path) tables - list of tables, comma-separated. If no tables specified then all tables will be saved.</p>
<p>To restore table(s):</p>
<p>HBASE_DIR/bin/hbase backup restore &lt;backup_root_path&gt; &lt;backup_id&gt; [tables]</p>
<p>backup_root_path - path to backup root directory (file://, hdfs:// or any other Hadoop-compatible path) backup_id - The id identifying the backup image. tables - list of tables, comma-separated.</p>
<p>FOR EXPERIENCED USERS only:</p>
<p>To get list of backup ids you will need to scan hbase:backup table using hbase shell or other means.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15890">HBASE-15890</a> | <em>Major</em> | <strong>Allow thrift to set/unset &quot;cacheBlocks&quot; for Scans</strong></li>
</ul>
<p>Adds cacheBlocks to Scan</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15610">HBASE-15610</a> | <em>Blocker</em> | <strong>Remove deprecated HConnection for 2.0 thus removing all PB references for 2.0</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15875">HBASE-15875</a> | <em>Major</em> | <strong>Remove HTable references and HTableInterface</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15915">HBASE-15915</a> | <em>Major</em> | <strong>Set timeouts on hanging tests</strong></li>
</ul>
<p>Use @ClassRule to set timeout on test case level (instead of @Rule which sets timeout for the test methods). CategoryBasedTimeout.forClass(..) determines the timeout value based on category annotation (small/medium/large) on the test case.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15907">HBASE-15907</a> | <em>Major</em> | <strong>Missing documentation of create table split options</strong></li>
</ul>
<p>documentation changes only - added section to Shell tricks and cross reference from region splitting section</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15931">HBASE-15931</a> | <em>Critical</em> | <strong>Add log for long-running tasks in AsyncProcess</strong></li>
</ul>
<p>After HBASE-15931, we will log more details for long-running tasks in AsyncProcess#waitForMaximumCurrentTasks every 10 seconds, including: 1. Table name will be included in the tasks status log 2. On which regionserver(s) the tasks are runnning will be logged when less than hbase.client.threshold.log.details tasks left, by default 10. 3. Against which regions the tasks are running will be logged when less than 2 tasks left.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15981">HBASE-15981</a> | <em>Minor</em> | <strong>Stripe and Date-tiered compactions inaccurately suggest disabling table in docs</strong></li>
</ul>
<p>Removes reference to disabling table in docs for stripe and date-tiered compactions</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15989">HBASE-15989</a> | <em>Major</em> | <strong>Remove hbase.online.schema.update.enable</strong></li>
</ul>
<p>Removes the &quot;hbase.online.schema.update.enable&quot; property. from now, every operation that alter the schema (e.g. modifyTable, addFamily, removeFamily, ...) will use the online schema update. there is no need to disable/enable the table.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15994">HBASE-15994</a> | <em>Major</em> | <strong>Allow selection of RpcSchedulers</strong></li>
</ul>
<p>Adds a FifoRpcSchedulerFactory so you can try the FifoRpcScheduler by setting &quot;hbase.region.server.rpc.scheduler.factory.class&quot;</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15525">HBASE-15525</a> | <em>Critical</em> | <strong>OutOfMemory could occur when using BoundedByteBufferPool during RPC bursts</strong></li>
</ul>
<p>Added a new ByteBufferPool which pools N ByteBuffers. By default it makes off heap ByteBuffers when getBuffer() is called. The size of each buffer defaults to 64KB. This can be configured using 'hbase.ipc.server.reservoir.initial.buffer.size'. The max number of buffers which can be pooled defaults to twice the number of handler threads in RS. This can be configured with key 'hbase.ipc.server.reservoir.initial.max'. While responding to read requests and client support Codec, we will create CellBlocks and directly return it as PB payload. For making this block, we will use N ByteBuffers from pool as per the total size of the response cells. The default size of 64 KB for the buffer is inline with the number of bytes written to RPC layer in one short.(That is also 64KB). When at point of time, the calle not able to get a free buffer from the pool (it returns null then), it will make on heap Buffer of same size (as that of Buffers in pool) and use that to create cell block.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15971">HBASE-15971</a> | <em>Critical</em> | <strong>Regression: Random Read/WorkloadC slower in 1.x than 0.98</strong></li>
</ul>
<p>Change the default rpc scheduler from 'deadline' to 'fifo' instead so it is the same as in branch 0.98. 'deadline' was of questionable benefit but with a high cost scheduling. To re-enable 'deadline', set hbase.ipc.server.callqueue.type to 'deadline' in your hbase-site.xml.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16023">HBASE-16023</a> | <em>Major</em> | <strong>Fastpath for the FIFO rpcscheduler</strong></li>
</ul>
<p>Adds a 'fastpath' when using the default FIFO rpc scheduler ('fifo'). Does direct handoff from Reader thread to Handler if there is one ready and willing. Will shine best when high random read workload (YCSB workloadc for instance)</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15950">HBASE-15950</a> | <em>Major</em> | <strong>Fix memstore size estimates to be more tighter</strong></li>
</ul>
<p>The estimates of heap usage by the memstore objects (KeyValue, object and array header sizes, etc) have been made more accurate for heap sizes up to 32G (using CompressedOops), resulting in them dropping by 10-50% in practice. This also results in less number of flushes and compactions due to &quot;fatter&quot; flushes. YMMV. As a result, the actual heap usage of the memstore before being flushed may increase by up to 100%. If configured memory limits for the region server had been tuned based on observed usage, this change could result in worse GC behavior or even OutOfMemory errors. Set the environment property (not hbase-site.xml) &quot;hbase.memorylayout.use.unsafe&quot; to false to disable.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-5291">HBASE-5291</a> | <em>Major</em> | <strong>Add Kerberos HTTP SPNEGO authentication support to HBase web consoles</strong></li>
</ul>
<p>HBase Web UIs can be secured from general public access using SPNEGO to require a valid Kerberos ticket.</p>
<p>Setting 'hbase.security.authentication.ui' to 'kerberos' in hbase-site.xml is a global switch to have all Web UIs allow only authenticated clients via Kerberos. 'hbase.security.authentication.spnego.kerberos.principal' and 'hbase.security.authentication.spnego.kerberos.keytab' are two other required properties in hbase-site.xml, the Kerberos principal and keytab to use for the server to use to log in. The primary in the Kerberos principal must be 'HTTP' as required by the SPNEGO mechanism, e.g. 'HTTP/host.domain.com@DOMAIN.COM'.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15977">HBASE-15977</a> | <em>Major</em> | <strong>Failed variable substitution on home page</strong></li>
</ul>
<p>Done. Thanks, Dima, Andrew!</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14877">HBASE-14877</a> | <em>Major</em> | <strong>maven archetype: client application</strong></li>
</ul>
<p>This patch introduces a new infrastructure for creation and maintenance of Maven archetypes in the context of the hbase project, and it also introduces the first archetype, which end-users may utilize to generate a simple hbase-client dependent project.</p>
<p>NOTE that this patch should introduce two new WARNINGs (&quot;Using platform encoding ... to copy filtered resources&quot;) into the hbase install process. These warnings are hard-wired into the maven-archetype-plugin:create-from-project goal. See hbase/hbase-archetypes/README.md, footnote [6] for details.</p>
<p>After applying the patch, see hbase/hbase-archetypes/README.md for details regarding the new archetype infrastructure introduced by this patch. (The README text is also conveniently positioned at the top of the patch itself.)</p>
<h1 id="here-is-the-opening-paragraph-of-the-readme.md-file">Here is the opening paragraph of the README.md file:</h1>
<h1 id="the-hbase-archetypes-subproject-of-hbase-provides-an-infrastructure-for-creation-and-maintenance-of-maven-archetypes-pertinent-to-hbase.-upon-deployment-to-the-archetype-catalog-of-the-central-maven-repository-these-archetypes-may-be-used-by-end-user-developers-to-autogenerate-completely-configured-maven-projects-including-fully-functioning-sample-code-through-invocation-of-the-archetypegenerate-goal-of-the-maven-archetype-plugin.">The hbase-archetypes subproject of hbase provides an infrastructure for creation and maintenance of Maven archetypes pertinent to HBase. Upon deployment to the archetype catalog of the central Maven repository, these archetypes may be used by end-user developers to autogenerate completely configured Maven projects (including fully-functioning sample code) through invocation of the archetype:generate goal of the maven-archetype-plugin.</h1>
<p>The README.md file also contains several paragraphs under the heading, &quot;Notes for contributors and committers to the HBase project&quot;, which explains the layout of 'hbase-archetypes', and how archetypes are created and installed into the local Maven repository, ready for deployment to the central Maven repository. It also outlines how new archetypes may be developed and added to the collection in the future.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14878">HBASE-14878</a> | <em>Major</em> | <strong>maven archetype: client application with shaded jars</strong></li>
</ul>
<p>Adds new hbase-shaded-client archetype; also corrects an omission found in hbase-archetypes/README.md in the section headed &quot;How to add a new archetype&quot;.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16153">HBASE-16153</a> | <em>Trivial</em> | <strong>Correct the config name 'hbase.memestore.inmemoryflush.threshold.factor'</strong></li>
</ul>
<p><strong>WARNING: No release note provided for this change.</strong></p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16147">HBASE-16147</a> | <em>Major</em> | <strong>Add ruby wrapper for getting compaction state</strong></li>
</ul>
<p>compaction_state shell command would return compaction state in String form: NONE, MINOR, MAJOR, MAJOR_AND_MINOR</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16140">HBASE-16140</a> | <em>Major</em> | <strong>bump owasp.esapi from 2.1.0 to 2.1.0.1</strong></li>
</ul>
<p>The dependency owasp.esapi had a compatible change from 2.1.0 to 2.1.0.1. As a result, the transitive dependency commons-fileupload had a change from 1.2 to 1.3.1, which has some minor class changes that impact binary compatibility. Interested users should check the release notes of commons-fileupload to see if any of the incompatible changes impact them.</p>
<p>http://commons.apache.org/proper/commons-fileupload/changes-report.html</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-15925">HBASE-15925</a> | <em>Blocker</em> | <strong>compat-module maven variable not evaluated</strong></li>
</ul>
<p>Downstream users of HBase dependencies that do not properly activate Maven profiles should now see a correct transitive dependency on the default hadoop-compatibility-module.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14548">HBASE-14548</a> | <em>Major</em> | <strong>Expand how table coprocessor jar and dependency path can be specified</strong></li>
</ul>
<p>Allow a directory containing the jars or some wildcards to be specified, such as: hdfs://namenode:port/user/hadoop-user/ or hdfs://namenode:port/user/hadoop-user/*.jar</p>
<p>Please note that if a directory is specified, all jar files(.jar) directly in the directory are added, but it does not search files in the subtree rooted in the directory. Do not contain any wildcard if you would like to specify a directory.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16087">HBASE-16087</a> | <em>Major</em> | <strong>Replication shouldn't start on a master if if only hosts system tables</strong></li>
</ul>
<p>Masters will no longer start any replication threads if they are hosting only system tables.</p>
<p>In order to change this add something to the config for tables on master that doesn't start with &quot;hbase:&quot; ( Replicating system tables is something that's currently unsupported and can open up security holes, so do this at your own peril)</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16081">HBASE-16081</a> | <em>Blocker</em> | <strong>Replication remove_peer gets stuck and blocks WAL rolling</strong></li>
</ul>
<p>When a replication endpoint is sent a shutdown request by the replication source in situations like removing a peer, we now try to gracefully shut it down by draining the items already sent for replication to the peer cluster. If the drain does not complete in the specified time (hbase.rpc.timeout * replication.source.maxterminationmultiplier), the regionserver is aborted to avoid blocking the WAL roll.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16095">HBASE-16095</a> | <em>Major</em> | <strong>Add priority to TableDescriptor and priority region open thread pool</strong></li>
</ul>
<p>Adds a PRIORITY property to the HTableDescriptor. PRIORITY should be in the same range as the RpcScheduler defines it (HConstants.XXX_QOS).</p>
<p>Table priorities are only used for region opening for now. There can be other uses later (like RpcScheduling).</p>
<p>Regions of high priority tables (priority &gt;= than HIGH_QOS) are opened from a different thread pool than the regular region open thread pool. However, table priorities are not used as a global order for region assigning or opening.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13823">HBASE-13823</a> | <em>Major</em> | <strong>Procedure V2: unnecessaery operaions on AssignmentManager#recoverTableInDisablingState() and recoverTableInEnablingState()</strong></li>
</ul>
<p>For cluster upgraded from 1.0.x or older releases, master startup would not continue the in-progress enable/disable table process. If orphaned znode with ENABLING/DISABLING state exists in the cluster, run hbck or manually fix the issue.</p>
<p>For new cluster or cluster upgraded from 1.1.x and newer release, there is no issue to worry about.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-3727">HBASE-3727</a> | <em>Minor</em> | <strong>MultiHFileOutputFormat</strong></li>
</ul>
<p>MultiHFileOutputFormat support output of HFiles from multiple tables. It will output directories and hfiles as follow, --table1 --family1 --family2 --Hfiles --table2 --family3 --hfiles --family4</p>
<p>family directory and its hfiles match the output of HFileOutputFormat2</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16144">HBASE-16144</a> | <em>Major</em> | <strong>Replication queue's lock will live forever if RS acquiring the lock has died prematurely</strong></li>
</ul>
<p>If zk based replication queue is used and useMulti is false, we will schedule a chore to clean up the orphan replication queue lock on zk.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16052">HBASE-16052</a> | <em>Major</em> | <strong>Improve HBaseFsck Scalability</strong></li>
</ul>
<p>HBASE-16052 improves the performance and scalability of HBaseFsck, especially for large clusters with a small number of large tables.</p>
<p>Searching for lingering reference files is now a multi-threaded operation. Loading HDFS region directory information is now multi-threaded at the region-level instead of the table-level to maximize concurrency. A performance bug in HBaseFsck that resulted in redundant I/O and RPCs was fixed by introducing a FileStatusFilter that filters FileStatus objects directly.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16244">HBASE-16244</a> | <em>Major</em> | <strong>LocalHBaseCluster start timeout should be configurable</strong></li>
</ul>
<p>When LocalHBaseCluster is started from the command line the Master would give up after 30 seconds due to a hardcoded timeout meant for unit tests. This change allows the timeout to be configured via hbase-site as well as sets it to 5 minutes when LocalHBaseCluster is started from the command line.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-13701">HBASE-13701</a> | <em>Major</em> | <strong>Consolidate SecureBulkLoadEndpoint into HBase core as default for bulk load</strong></li>
</ul>
<p>SecureBulkLoadEndpoint has been integrated into HBase core as default bulk load mechanism. It is no longer needed to install it as a coprocessor endpoint. The new server is backward compatible, accommodating non-secure old client and secure old client requesting SecureBulkLoadEndpointservice. SecureBulkLoadEndpointis deprecated. The backward compatibility support may be removed in future releases.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-14743">HBASE-14743</a> | <em>Minor</em> | <strong>Add metrics around HeapMemoryManager</strong></li>
</ul>
<p>A memory metrics reveals situations happened in both MemStores and BlockCache in RegionServer. Through this metrics, users/operators can know 1). Current size of MemStores and BlockCache in bytes. 2). Occurrence for Memstore minor and major flush. (named unblocked flush and blocked flush respectively, shown in histogram) 3). Dynamic changes in size between MemStores and BlockCache. (with Increase/Decrease as prefix, shown in histogram). And a counter for no changes, named DoNothingCounter. 4). Occurrence for memory usage alarm (used more than 95% by default) in RegionServer. (named AboveHeapOccupancyLowWatermarkCounter)</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16289">HBASE-16289</a> | <em>Critical</em> | <strong>AsyncProcess stuck messages need to print region/server</strong></li>
</ul>
<p>Adds logging of region and server. Helpful debugging. Logging now looks like this: {code} 2016-06-23 17:07:18,759 INFO [Thread-1] client.AsyncProcess$AsyncRequestFutureImpl(1601): #1, waiting for 1 actions to finish on table: DUMMY_TABLE 2016-06-23 17:07:18,759 INFO [Thread-1] client.AsyncProcess(1720): Left over 1 task(s) are processed on server(s): [s1:1,1,1] 2016-06-23 17:07:18,759 INFO [Thread-1] client.AsyncProcess(1728): Regions against which left over task(s) are processed: [DUMMY_TABLE,DUMMY_BYTES_1,1.3fd12ea80b4df621fb15497ba75f7368.,DUMMY_TABLE,DUMMY_BYTES_2,2.924207e242e313d2e5491c625e0a296e.] {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16186">HBASE-16186</a> | <em>Major</em> | <strong>Fix AssignmentManager MBean name</strong></li>
</ul>
<p>The AssignmentManager MBean was named AssignmentManger (note misspelling). This patch fixed the misspelling.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16288">HBASE-16288</a> | <em>Critical</em> | <strong>HFile intermediate block level indexes might recurse forever creating multi TB files</strong></li>
</ul>
<p>A new hfile configuration &quot;hfile.index.block.min.entries&quot; which defaults to 16 determines how many entries the hfile index block can have at least. The configuration which determines how large the index block can be at max (hfile.index.block.max.size) is ignored as long as we have fewer than hfile.index.block.min.entries entries. This ensures that multi-level index does not build up with too many levels.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16317">HBASE-16317</a> | <em>Blocker</em> | <strong>revert all ESAPI changes</strong></li>
</ul>
<p>This issue reverts fixes designed to prevent malicious content from rendering in HBase's UIs. Specifically, these changes shipped in 1.1.4+ and 1.2.0+. They were removed due to licensing issues discovered in the dependencies they introduced. Their implementation and those dependencies have been removed from HBase! Removal of these dependencies is against the strict definition of our version compatibility guidelines. However, inclusion of non-Apache approved licenses cannot be tolerated. Implementation of these fixes using an Apache-appropriate means is tracked in HBASE-16328.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16355">HBASE-16355</a> | <em>Major</em> | <strong>hbase-client dependency on hbase-common test-jar should be test scope</strong></li>
</ul>
<p>The HBase client artifact previously incorrectly included the hbase-common test jar as a runtime dependency. With this change, that dependency has been moved to test scope. Downstream users are not expected to be impacted, unless they relied on the transitive dependency for these HBase internal test classes.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16287">HBASE-16287</a> | <em>Major</em> | <strong>LruBlockCache size should not exceed acceptableSize too many</strong></li>
</ul>
<p>In order to avoid blockcache size exceed acceptable size too much, we add one configuration &quot;hbase.lru.blockcache.hard.capacity.limit.factor&quot; to decide whether the block could be put into LruBlockCache or not. This factor defaults to 1.2 If blockcache size &gt;= factor*acceptableSize, we will reject the block into cache.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-8386">HBASE-8386</a> | <em>Major</em> | <strong>deprecate TableMapReduce.addDependencyJars(Configuration, class&lt;?&gt; ...)</strong></li>
</ul>
<p>The MapReduce helper function <code>TableMapReduce.addDependencyJars(Configuration, class\&lt;?\&gt; ...)</code> has been deprecated since it is easy to use incorrectly. Most users should rely on addDependencyJars(Job) instead.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16321">HBASE-16321</a> | <em>Blocker</em> | <strong>Ensure findbugs jsr305 jar isn't present</strong></li>
</ul>
<p>HBase now ensures the jsr305 implementation from the findbugs project is not included in its binary artifacts or the compile / runtime dependencies of its user facing modules. Downstream users that rely on this jar will need to update their dependencies.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-9899">HBASE-9899</a> | <em>Major</em> | <strong>for idempotent operation dups, return the result instead of throwing conflict exception</strong></li>
</ul>
<p>Non-idempotent operations (increment/append/checkAndPut/...) may throw OperationConflictException even though the increment/append succeeded. For example (client rpc retries number set to 3):</p>
<ol>
<li>first increment rpc request success</li>
<li>client timeout and send second rpc request, but nonce is same and save in server. The server found that it has already succeed, so return a OperationConflictException to make sure that increment operation only be applied once in server.</li>
</ol>
<p>This patch will solve this problem by read the previous result when receive a duplicate rpc request. 1. Store the mvcc to OperationContext. When first rpc request succeed, store the mvcc for this operation nonce. 2. When there are duplicate rpc request, convert to read result by the mvcc.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-9465">HBASE-9465</a> | <em>Major</em> | <strong>Push entries to peer clusters serially</strong></li>
</ul>
<p>Now in replication we can make sure the order of pushing logs is same as the order of requests from client. Set REPLICATION_SCOPE=2 at one cf's configuration to enable this feature. This feature relies on zk-less assignment, and conflicts with distributed log replay. So users must set hbase.assignment.usezk and hbase.master.distributed.log.replay to false to support this feature.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16308">HBASE-16308</a> | <em>Major</em> | <strong>Contain protobuf references</strong></li>
</ul>
<p>Undo protobuf references through the codebase so protobuf references are contained rather than spread about the codebase. For example, moved protobuff-ing up into the various Callables rather than repeat on each method invocation cleaning up boilerplate around rpc calls. Having a few protobuf reference locations only simplifies the parent issue shading project.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16267">HBASE-16267</a> | <em>Critical</em> | <strong>Remove commons-httpclient dependency from hbase-rest module</strong></li>
</ul>
<p>This issue upgrades httpclient to 4.5.2 and httpcore to 4.4.4 which are the versions used by hadoop-2. This is to handle the following CVE's.</p>
<p>https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2015-5262 : http/conn/ssl/SSLConnectionSocketFactory.java in Apache HttpComponents HttpClient before 4.3.6 ignores the http.socket.timeout configuration setting during an SSL handshake, which allows remote attackers to cause a denial of service (HTTPS call hang) via unspecified vectors.</p>
<p>https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2012-6153 https://web.nvd.nist.gov/view/vuln/detail?vulnId=CVE-2012-5783 Apache Commons HttpClient 3.x, as used in Amazon Flexible Payments Service (FPS) merchant Java SDK and other products, does not verify that the server hostname matches a domain name in the subject's Common Name (CN) or subjectAltName field of the X.509 certificate, which allows man-in-the-middle attackers to spoof SSL servers via an arbitrary valid certificate.</p>
<p>Downstream users who are exposed to commons-httpclient via the HBase classpath will have to similarly update their dependency.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-12721">HBASE-12721</a> | <em>Major</em> | <strong>Create Docker container cluster infrastructure to enable better testing</strong></li>
</ul>
<p>Downstream users wishing to test HBase in a &quot;distributed&quot; fashion (multiple &quot;nodes&quot; running as separate containers on the same host) can now do so in an automated fashion while leveraging Docker for process isolation via the clusterdock project.</p>
<p>For details see the README.md in the dev-support/apache_hbase_topology folder.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-7621">HBASE-7621</a> | <em>Major</em> | <strong>REST client (RemoteHTable) doesn't support binary row keys</strong></li>
</ul>
<p>RemoteHTable now supports binary row keys with any character or byte by properly encoding request URLs. This is a both a behavioral change from earlier versions and an important fix for protocol correctness.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16422">HBASE-16422</a> | <em>Major</em> | <strong>Tighten our guarantees on compatibility across patch versions</strong></li>
</ul>
<p>Adds below change to our compat guarantees:</p>
<p>{code} -* Example: A user using a newly deprecated api does not need to modify application code with hbase api calls until the next major version. 10 +* New APIs introduced in a patch version will only be added in a source compatible way footnote:[See 'Source Compatibility' https://blogs.oracle.com/darcy/entry/kinds_of_compatibility]: i.e. code that implements public APIs will continue to compile. {code}</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16450">HBASE-16450</a> | <em>Major</em> | <strong>Shell tool to dump replication queues</strong></li>
</ul>
<p>New tool to dump existing replication peers, configurations and queues when using HBase Replication. The tool provides two flags:</p>
<p>--distributed This flag will poll each RS for information about the replication queues being processed on this RS. By default this is not enabled and the information about the replication queues and configuration will be obtained from ZooKeeper. --hdfs When --distributed is used, this flag will attempt to calculate the total size of the WAL files used by the replication queues. Since its possible that multiple peers can be configured this value can be overestimated.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16409">HBASE-16409</a> | <em>Minor</em> | <strong>Row key for bad row should be properly delimited in VerifyReplication</strong></li>
</ul>
<p>--delimiter= option is added to verifyrep. The delimiter would wrap bad rows in log output.</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16213">HBASE-16213</a> | <em>Major</em> | <strong>A new HFileBlock structure for fast random get</strong></li>
</ul>
<p>HBASE-16213 introduced a new DataBlockEncoding in name of ROW_INDEX_V1, which could improve random read (get) performance especially when the average record size (key-value size per row) is small. To use this feature, please set DATA_BLOCK_ENCODING to ROW_INDEX_V1 for CF of newly created table, or change existing CF with below command: alter 'table_name',{NAME =&gt; 'cf', DATA_BLOCK_ENCODING =&gt; 'ROW_INDEX_V1'}.</p>
<p>Please note that if we turn this DBE on, HFile block will be bigger than NONE encoding because it adds some meta infos for binary search: /** * Store cells following every row's start offset, so we can binary search to a row's cells. * * Format: * flat cells * integer: number of rows * integer: row0's offset * integer: row1's offset * .... * integer: dataSize * */</p>
<hr />
<ul>
<li><a href="https://issues.apache.org/jira/browse/HBASE-16340">HBASE-16340</a> | <em>Critical</em> | <strong>ensure no Xerces jars included</strong></li>
</ul>
<p>HBase no longer includes Xerces implementation jars that were previously included via transitive dependencies. Downstream users relying on HBase for these artifacts will need to update their dependencies.</p>
